{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install neccessary Library\n",
    "The libraries include:\n",
    "- langchain framework'\n",
    "- GPT4ALL, OpenAI and HuggingFace for various embedding methods and LLMs\n",
    "- Document loaders\n",
    "- Dependent libraries\n",
    "\n",
    "__Note__ : \n",
    "- It requires C++ builder for building a dependant library for Chroma. Check out https://github.com/bycloudai/InstallVSBuildToolsWindows for instruction. \n",
    "- Python version: 3.12.4\n",
    "- Pydantic version: 2.7.3. There is issue with pydantic version 1.10.8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U langchain-experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"unstructured[all-docs]\" pillow pydantic lxml pillow matplotlib chromadb tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install --upgrade nltk openpyxl matplotlib textblob spacy gensim scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx==1.16.1\n",
      "  Downloading onnx-1.16.1-cp312-cp312-win_amd64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from onnx==1.16.1) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from onnx==1.16.1) (4.25.5)\n",
      "Downloading onnx-1.16.1-cp312-cp312-win_amd64.whl (14.4 MB)\n",
      "   ---------------------------------------- 0.0/14.4 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 3.7/14.4 MB 21.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 8.7/14.4 MB 23.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 13.9/14.4 MB 24.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.4/14.4 MB 23.9 MB/s eta 0:00:00\n",
      "Installing collected packages: onnx\n",
      "  Attempting uninstall: onnx\n",
      "    Found existing installation: onnx 1.17.0\n",
      "    Uninstalling onnx-1.17.0:\n",
      "      Successfully uninstalled onnx-1.17.0\n",
      "Successfully installed onnx-1.16.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#!pip install onnx==1.16.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import textblob \n",
    "#!python -m textblob.download_corpora\n",
    "#import spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('universal_tagset')\n",
    "#nltk.download('averaged_perceptron_tagger_eng')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('punkt_tab')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Environment Parameters\n",
    "Prepare the list of parameter in .env file for later use. \n",
    "Parameters: \n",
    "- API keys for LLMs\n",
    "    - OPENAI_API_KEY \n",
    "    - HUGGINGFACEHUB_API_TOKEN \n",
    "- Directory / location for documents and vector databases\n",
    "    - DOC_ARVIX = \"./source/from_arvix/\"\n",
    "    - DOC_WIKI = \"./source/from_wiki/\"\n",
    "    - VECTORDB_OPENAI_EM = \"./vector_db/openai_embedding/\"\n",
    "    - VECTORDB_MINILM_EM = \"./vector_db/gpt4all_miniLM/\"\n",
    "    - TS_RAGAS = \"./evaluation/testset/by_RAGAS/\"\n",
    "    - TS_PROMPT = \"./evaluation/testset/by_direct_prompt/\"\n",
    "    - EVAL_DATASET = \"./evaluation/evaluation_data_set/\"\n",
    "    - EVAL_METRIC = \"./evaluation/evaluation_metric\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Simple RAG Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagrams/HL architecture.png\" alt=\"HL arc\" title= \"HL Architecture\" />\n",
    "\n",
    "The system comprises of 5 components: \n",
    "\n",
    "- Internal data, documents: The system starts with a collection of internal documents and / or structured databases. Documents can be in text, PDF, photo or video formats. These documents and data are sources for the specified knowledgebase.\n",
    "\n",
    "- Embedding processor: The documents and database entries are processed to create vector embeddings. Embeddings are numerical representations of the documents in a high-dimensional space that capture their semantic meaning. \n",
    "\n",
    "- Vector database: the vectorized chunk of documents and database entries are stored on vector database to be search and retrieved in a later stage. \n",
    "\n",
    "- Query processor: The query processor takes the user's query and performs semantic search against the vectorized database. This component ensures that the query is interpreted correctly and retrieves relevant document embeddings from the vectorized DB. It combines the user's original query with the retrieved document embeddings to form a context-rich query. This augmented query provides additional context that can help in generating a more accurate and relevant response.\n",
    "\n",
    "- LLM: pre-trained large language model where the augmented query is passed to for generating a response based on the query and the relevant documents.\n",
    "\n",
    "The system involves 2 main pipelines: the embedding pipeline and the retrieval pipeline. Each pipeline has specific stages and processes that contribute to the overall functionality of the system.\n",
    "\n",
    "In this experiment, we use Langchain as a framework to build a simple RAG as a chain of tasks, which interacts with surrounding services like parsing, embedding, vector database and LLMs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. MultiModal RAG Architecture\n",
    "<img src=\"diagrams/ISM6564-Project.png\" alt=\"HL arc\" title= \"MM HL Architecture\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the environment parameters\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we load data from various sources. Make them ready to ingest.\n",
    "We will download 5 articles from ARVIX with query \"RAG for Large Language Model\" and store them locally and ready for next steps of embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From ARXIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv \n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "  query = \"RAG for Large Language Model\",     # To get more of other topics and number of papers. \n",
    "  max_results = 5,\n",
    "#  sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "results = client.results(search)\n",
    "all_results = list(client.results(search)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks http://arxiv.org/abs/2407.21059v1\n",
      "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation http://arxiv.org/abs/2408.02545v1\n",
      "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries http://arxiv.org/abs/2401.15391v1\n",
      "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge Update http://arxiv.org/abs/2410.20299v1\n",
      "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models http://arxiv.org/abs/2410.07176v1\n"
     ]
    }
   ],
   "source": [
    "# Print out the articles' titles\n",
    "for r in all_results:\n",
    "    print(f\"{r.title} {r.entry_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: download articles and save them in pre-defined location for later use\n",
    "# Prepare: create the environment paramter DOC_ARVIX for the path to save articles. \n",
    "# Download and save articles in PDF format to the \"RAG_for_LLM\" folder under ARVIX_DOC path\n",
    "DOC_ARVIX = os.getenv(\"DOC_ARVIX\") \n",
    "directory_path = os.path.join(DOC_ARVIX) \n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "for r in all_results:\n",
    "    r.download_pdf(dirpath=directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Springer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Lexis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step and the previous one are usually processed together. I try to separate them to make attention that these are not always coupled.\n",
    "We use available library DirectoryLoader and PyMuPDFLoader from Langchain to load and parse all .pdf files in the directory.\n",
    "We can use corresponding loader for other data types such as excel, presentation, unstructured ... \n",
    "\n",
    "Refer to https://python.langchain.com/v0.1/docs/integrations/document_loaders/ for other available loaders. \n",
    "We also use the OCR library rapidocr to extract image as text. Certainly, the trade-off is processing time. It took 18 minutes to parse 5 pdf files with OCR compared to 0.1 second without. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Util functions for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.documents.elements import NarrativeText\n",
    "\n",
    "\n",
    "# Extract elements from PDF\n",
    "def extract_pdf_elements(path, fname,img_path=\"\"):\n",
    "    \"\"\"\n",
    "    Extract images, tables, and chunk text from a PDF file.\n",
    "    path: File path, which is used to dump images (.jpg)\n",
    "    fname: File name\n",
    "    \"\"\"\n",
    "    if img_path == \"\":\n",
    "        img_path = path\n",
    "    if not os.path.exists(img_path):\n",
    "        os.makedirs(img_path)\n",
    "    return partition_pdf(\n",
    "        filename=path + fname,\n",
    "        extract_images_in_pdf=True,\n",
    "        infer_table_structure=True,\n",
    "        chunking_strategy=\"by_title\",\n",
    "        max_characters=4000,\n",
    "        new_after_n_chars=3800,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        image_output_dir_path=img_path,\n",
    "        form_extraction_skip_tables=False,\n",
    "        extract_image_block_output_dir = img_path\n",
    "    )\n",
    "\n",
    "def extract_pdf_elements_v2(path, fname,img_path=\"\"):\n",
    "    \"\"\"\n",
    "    Extract images, tables, and chunk text from a PDF file.\n",
    "    path: File path, which is used to dump images (.jpg)\n",
    "    fname: File name\n",
    "    \"\"\"\n",
    "    if img_path == \"\":\n",
    "        img_path = path\n",
    "    if not os.path.exists(img_path):\n",
    "        os.makedirs(img_path)\n",
    "    return partition_pdf(\n",
    "        filename=path + fname,\n",
    "        extract_images_in_pdf=True,\n",
    "        infer_table_structure=True,\n",
    "        strategy=\"hi_res\",\n",
    "        max_characters=4000,\n",
    "        new_after_n_chars=3800,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        image_output_dir_path=img_path,\n",
    "        form_extraction_skip_tables=False,\n",
    "        extract_image_block_output_dir = img_path\n",
    "    )\n",
    "\n",
    "# Categorize elements by type\n",
    "def categorize_elements(raw_pdf_elements):\n",
    "    \"\"\"\n",
    "    Categorize extracted elements from a PDF into tables and texts.\n",
    "    raw_pdf_elements: List of unstructured.documents.elements\n",
    "    \"\"\"\n",
    "    tables = []\n",
    "    texts = []\n",
    "    for element in raw_pdf_elements:\n",
    "        if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            tables.append(element.to_dict()[\"metadata\"][\"text_as_html\"])\n",
    "        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "            texts.append(str(element))\n",
    "    return texts, tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Generate summaries of text elements\n",
    "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str\n",
    "    summarize_texts: Bool to summarize texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "    # Text summary chain\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "    table_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    if texts and summarize_texts:\n",
    "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "    elif texts:\n",
    "        text_summaries = texts\n",
    "\n",
    "    # Apply to tables if tables are provided\n",
    "    if tables:\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
    "\n",
    "    return text_summaries, table_summaries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    chat = ChatOpenAI(model=\"gpt-4o\", max_tokens=1024)\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            print(img_file)\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summaries.append(image_summarize(base64_image, prompt))\n",
    "\n",
    "    return img_base64_list, image_summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "CHROMA_OPENAI_RAG_FOR_LLM = \"CHROMA_OPENAI_RAG_FOR_LLM\"\n",
    "CHROMA_HF_RAG_FOR_LLM = \"CHROMA_HF_RAG_FOR_LLM\"\n",
    "CHROMA_MINILM_RAG_FOR_LLM = \"CHROMA_MINILM_RAG_FOR_LLM\"\n",
    "CHROMA_OLLAMA_RAG_FOR_LLM = \"CHROMA_OLLAMA_RAG_FOR_LLM\"\n",
    "\n",
    "#IMPORTANT: THE CHROMA INSTANCE CANNOT INITIATED WITHIN A .PY. IT WILL CRASH THE KERNEL. \n",
    "class VectorBD:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vectordb_name) -> None:\n",
    "        load_dotenv()\n",
    "#       OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "#       print(OPENAI_API_KEY)\n",
    "        if vectordb_name == CHROMA_OPENAI_RAG_FOR_LLM:\n",
    "            self.vectordb_directory = os.path.join(os.getenv(\"VECTORDB_OPENAI_EM\"),\"RAG_for_LLM\")\n",
    "            self.embeddings = OpenAIEmbeddings()\n",
    "            self.vectordb =  Chroma(persist_directory=self.vectordb_directory, embedding_function=self.embeddings)\n",
    "            self.retriever = self.vectordb.as_retriever()\n",
    "\n",
    "        if vectordb_name == CHROMA_MINILM_RAG_FOR_LLM:\n",
    "            self.vectordb_directory = os.path.join(os.getenv(\"VECTORDB_MINILM_EM\"),\"RAG_for_LLM\")\n",
    "            self.embeddings = GPT4AllEmbeddings(model_name=\"all-MiniLM-L6-v2.gguf2.f16.gguf\", gpt4all_kwargs={'allow_download': 'True'})\n",
    "            self.vectordb =  Chroma(persist_directory=self.vectordb_directory, embedding_function=self.embeddings)\n",
    "            self.retriever = self.vectordb.as_retriever()\n",
    "\n",
    "        if vectordb_name == CHROMA_OLLAMA_RAG_FOR_LLM:\n",
    "            self.vectordb_directory = os.path.join(os.getenv(\"VECTORDB_OLLAMA_EM\"),\"RAG_for_LLM\")\n",
    "            self.embeddings = OllamaEmbeddings(model=\"llama3.1\")\n",
    "            self.vectordb =  Chroma(persist_directory=self.vectordb_directory, embedding_function=self.embeddings)\n",
    "            self.retriever = self.vectordb.as_retriever()\n",
    "\n",
    "        if vectordb_name == CHROMA_HF_RAG_FOR_LLM:\n",
    "            self.vectordb_directory = os.path.join(os.getenv(\"VECTORDB_HF_EM\"),\"RAG_for_LLM\")\n",
    "            self.embeddings = HuggingFaceEmbeddings()\n",
    "            self.vectordb =  Chroma(persist_directory=self.vectordb_directory, embedding_function=self.embeddings)\n",
    "            self.retriever = self.vectordb.as_retriever()       \n",
    "\n",
    "    def vectorizing(self, documents):\n",
    "        self.vectordb = Chroma.from_documents(documents=documents,embedding=self.embeddings, persist_directory=self.vectordb_directory)\n",
    "        self.vectordb.persist()\n",
    "\n",
    "    def invoke(self,question):\n",
    "#       print(self.retriever.invoke(\"What is RAG?\"))\n",
    "        return self.retriever.invoke(question)\n",
    "\n",
    "def connect_km(km_name):\n",
    "    load_dotenv()\n",
    "#   OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "#   print(OPENAI_API_KEY)\n",
    "    if km_name == CHROMA_OPENAI_RAG_FOR_LLM:\n",
    "        km_dir = os.path.join(os.getenv(\"VECTORDB_OPENAI_EM\"),\"RAG_for_LLM\")\n",
    "        km_embeddings = OpenAIEmbeddings()\n",
    "        km_db =  Chroma(persist_directory=km_dir, embedding_function=km_embeddings)\n",
    "        return km_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Connect to LLM \n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_huggingface import HuggingFaceEndpoint \n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "llm_model = {\n",
    "    \"GPT_3_5_TURBO\" : \"gpt-3.5-turbo\",\n",
    "    \"GPT_4\" : \"gpt-4\",\n",
    "    \"GPT_4o\" : \"gpt-4o\",  #For vision\n",
    "    \"GPT_4_PREVIEW\" : \"gpt-4-1106-preview\",\n",
    "    \"LOCAL_GPT4ALL\" : \"\",\n",
    "    \"MISRALAI\" : \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"LLAMA3_70B\" : \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "    \"ZEPHYR_7B\" : \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    \"OLLAMA_GEMMA2\" : \"gemma2\",\n",
    "    \"OLLAMA_LLAMA3\" : \"llama3\",\n",
    "    \"OLLAMA_LLAMA3.1\" : \"llama3.1\"\n",
    "}\n",
    "\n",
    "def connectLLM(model, temperature = 0):\n",
    "    load_dotenv()\n",
    "\n",
    "    # Connect to Open AI chat model: Online, Token-base\n",
    "    if model == \"GPT_3_5_TURBO\" or model == \"GPT_4_PREVIEW\" or model == \"GPT_4\" or model == \"GPT_4o\":\n",
    "#       print(\"connect llm\")\n",
    "        return ChatOpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY\"), model=llm_model[model], temperature=temperature)\n",
    "    \n",
    "    # Connect to HuggingFace chat model: Online, Token-base\n",
    "    # Note: to use Llama3, we need to register on HuggingFace website\n",
    "    if model == \"LLAMA3_70B\" or model == \"MISRALAI\" or model == \"ZEPHYR_7B\":\n",
    "        repo_id = llm_model[model]\n",
    "        return HuggingFaceEndpoint(\n",
    "            repo_id=repo_id,\n",
    "            max_length=128,\n",
    "            temperature=temperature, # Should be 0.5 \n",
    "            huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "        )\n",
    "    \n",
    "    # Connect to Ollama for Llama3, Llama3.1 and Gemma2 chat models\n",
    "    # Need these models are working locally, they must have been downloaded. Check instruction for downloading Ollama and models\n",
    "    if model == \"OLLAMA_GEMMA2\" or model == \"OLLAMA_LLAMA3\" or model == \"OLLAMA_LLAMA3.1\":\n",
    "        return ChatOllama(model=llm_model[model], temperature=temperature)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "def create_multi_vector_retriever(\n",
    "    vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images,image_file = [],\n",
    "    document_meta ={} # is the meta data to be stored together with the vectorized content\n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents,type = \"\"):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, \n",
    "                     metadata={\n",
    "                         id_key: doc_ids[i],\n",
    "                         'source': document_meta.get(\"filename\",\"\"),\n",
    "                         'type': type,\n",
    "                         'paper_id': document_meta.get(\"docid\",\"\")\n",
    "                         }\n",
    "                     )\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        content_docs = [\n",
    "            Document(page_content=s, \n",
    "                     metadata={\n",
    "                         id_key: doc_ids[i],\n",
    "                         'source': document_meta.get(\"filename\",\"\"),\n",
    "                         'type': type,\n",
    "                         'paper_id': document_meta.get(\"docid\",\"\")\n",
    "                         }\n",
    "                     )\n",
    "            for i, s in enumerate(doc_contents)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, content_docs)))\n",
    "\n",
    "    # Add texts, tables, and images\n",
    "    # Check that text_summaries is not empty before adding\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts,\"text\")\n",
    "    # Check that table_summaries is not empty before adding\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables,\"table\")\n",
    "    # Check that image_summaries is not empty before adding\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images,\"image\")\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries.pdf',\n",
       " '2407.21059v1.Modular_RAG__Transforming_RAG_Systems_into_LEGO_like_Reconfigurable_Frameworks.pdf',\n",
       " '2408.02545v1.RAG_Foundry__A_Framework_for_Enhancing_LLMs_for_Retrieval_Augmented_Generation.pdf',\n",
       " '2410.20299v1.EACO_RAG__Edge_Assisted_and_Collaborative_RAG_with_Adaptive_Knowledge_Update.pdf']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOC_ARVIX = os.getenv(\"DOC_ARVIX\") \n",
    "directory_path = os.path.join(DOC_ARVIX) \n",
    "pdffiles = [f for f in os.listdir(directory_path) if f.endswith(\".pdf\")]\n",
    "pdffiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>filename</th>\n",
       "      <th>status</th>\n",
       "      <th>topic</th>\n",
       "      <th>summary</th>\n",
       "      <th>img_folder</th>\n",
       "      <th>imgs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a5cdaa51-39b4-42fe-bc76-e19fb729c37b</td>\n",
       "      <td>2401.15391v1.MultiHop_RAG__Benchmarking_Retrie...</td>\n",
       "      <td>new</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>./figure/document_0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>012e560a-9388-4f1f-9ae3-1b4afc2a0bcd</td>\n",
       "      <td>2407.21059v1.Modular_RAG__Transforming_RAG_Sys...</td>\n",
       "      <td>new</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>./figure/document_1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a4b74ce1-b399-4b18-93ac-0c620d1438c7</td>\n",
       "      <td>2408.02545v1.RAG_Foundry__A_Framework_for_Enha...</td>\n",
       "      <td>new</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>./figure/document_2</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>decc1461-6857-422c-b0d4-b6f6420e0d6a</td>\n",
       "      <td>2410.20299v1.EACO_RAG__Edge_Assisted_and_Colla...</td>\n",
       "      <td>new</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>./figure/document_3</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  docid  \\\n",
       "0  a5cdaa51-39b4-42fe-bc76-e19fb729c37b   \n",
       "1  012e560a-9388-4f1f-9ae3-1b4afc2a0bcd   \n",
       "2  a4b74ce1-b399-4b18-93ac-0c620d1438c7   \n",
       "3  decc1461-6857-422c-b0d4-b6f6420e0d6a   \n",
       "\n",
       "                                            filename status topic summary  \\\n",
       "0  2401.15391v1.MultiHop_RAG__Benchmarking_Retrie...    new                 \n",
       "1  2407.21059v1.Modular_RAG__Transforming_RAG_Sys...    new                 \n",
       "2  2408.02545v1.RAG_Foundry__A_Framework_for_Enha...    new                 \n",
       "3  2410.20299v1.EACO_RAG__Edge_Assisted_and_Colla...    new                 \n",
       "\n",
       "            img_folder imgs  \n",
       "0  ./figure/document_0   []  \n",
       "1  ./figure/document_1   []  \n",
       "2  ./figure/document_2   []  \n",
       "3  ./figure/document_3   []  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import uuid\n",
    "\n",
    "# Load document catalog from picker files\n",
    "if os.path.exists('document_catalog.pickle'):\n",
    "    with open('document_catalog.pickle', 'rb') as pkl_file:\n",
    "        df_documents = pickle.load(pkl_file) \n",
    "else:\n",
    "    df_documents = pd.DataFrame(columns=[\"docid\",\"filename\",\"status\",\"topic\",\"summary\",\"img_folder\",\"imgs\"])\n",
    "\n",
    "# Load new files for processing\n",
    "new_item = False\n",
    "existing_files = list(df_documents[\"filename\"])\n",
    "for fn in pdffiles:\n",
    "    if fn not in existing_files:\n",
    "        i = len(df_documents.index)\n",
    "        df_documents.loc[len(df_documents.index)]={\"docid\":str(uuid.uuid4()),\"filename\":fn,\"status\":\"new\",\"topic\":\"\",\"summary\":\"\",\"img_folder\":\"./figure/document_\"+str(i),\"imgs\":[]}\n",
    "        new_item = True\n",
    "if new_item:\n",
    "    with open('document_catalog.pickle', 'wb') as pkl_file:\n",
    "        pickle.dump(df_documents,pkl_file)\n",
    "df_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Text Parsing and Image Extraction\n",
    "\n",
    "From each of pdf, extracts images. Expected return a list of images for each PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example for a subset of dataframe aka the first document / paper\n",
    "df_subset = df_documents.loc[0]\n",
    "directory_path = os.path.join(os.getenv(\"DOC_ARVIX\"))\n",
    "doc_elements = extract_pdf_elements(directory_path,df_subset[\"filename\"],df_subset[\"img_folder\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, tables = categorize_elements(doc_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Text Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the data into smaller chunks for better handling, processing, and retrieving.\n",
    "There is a limitation on number of tokens which the embedding service can process at later stage which requires documents are chunked in smaller size.\n",
    "There are many of chunking methods from Langchain. In which, Recursive CharacterText and Semantic are most popular. \n",
    "\n",
    "Reference: https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=4000, chunk_overlap=0\n",
    ")\n",
    "joined_texts = \" \".join(texts)\n",
    "texts_4k_token = text_splitter.split_text(joined_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Table Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>News source</td><td>Fortune Magazine</td><td>The Sydney Morning Herald</td></tr><tr><td>Evidence</td><td>Back then, just like today, home prices had boomed for years before Fed officials were ultimately forced to hike interest rates aggressively in an attempt to fight inflation.</td><td>Postponements of such reports could complicate things for the Fed, which has insisted it will make upcoming decisions on interest rates based on what incoming data say about the economy.</td></tr><tr><td>Claim</td><td>Federal Reserve officials were forced to aggressively hike interest rates to combat inflation after years of booming home prices.</td><td>The Federal Reserve has insisted that it will base its upcoming decisions on interest rates on the incoming economic data.</td></tr><tr><td>Bridge-Topic Bridge-Entity</td><td>Interest rate hikes to combat inflation Federal Reserve</td><td>Interest rate decisions based on economic data Federal Reserve</td></tr><tr><td>Query</td><td>Does the article from Fortune suggest that the Federal Reserve’s interest rate hikes are a response to past conditions, such as booming home prices, while The Sydney Morning Herald article indicates that the Federal Reserve’s future interest rate decisions will be based on incoming economic data?</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Category</td><td>Avg. Tokens</td><td>Entry Count</td></tr><tr><td>technology 2262.3 172</td></tr><tr><td>entertainment 2084.3 114</td></tr><tr><td>sports 2030.6 211</td></tr><tr><td>science 1745.5 21</td></tr><tr><td>business</td><td>1723.8</td><td>81</td></tr><tr><td>health 1481.1 10</td></tr><tr><td>total</td><td>2046.5</td><td>609</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Query Category</td><td>Entry Count</td><td>Percentage</td></tr><tr><td>Inference Query</td><td>816</td><td>31.92%</td></tr><tr><td>Comparison Query</td><td>856</td><td>33.49%</td></tr><tr><td>Temporal Query</td><td>583</td><td>22.81%</td></tr><tr><td>Null Query 301 11.78%</td></tr><tr><td>Total</td><td>2,556</td><td>100.00 %</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Num. of Evidence Needed</td><td>Count</td><td>Percentage</td></tr><tr><td>0 (Null Query)</td><td>301</td><td>11.78%</td></tr><tr><td>2</td><td>1078</td><td>42.18%</td></tr><tr><td>3</td><td>719</td><td>30.48%</td></tr><tr><td>4</td><td>398</td><td>15.56%</td></tr><tr><td>Total</td><td>2,556</td><td>100.00 %</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Embedding</td><td/><td>Without</td><td>Reranker</td><td/><td>With bge-reranker-large</td></tr><tr><td>MRR@10</td><td>MAP@10</td><td>Hits@10</td><td>Hits@4</td><td>MRR@10</td><td>MAP@10</td><td>Hits@10</td><td>Hits@4</td></tr><tr><td>text-embedding-ada-002</td><td>0.4203</td><td>0.3431</td><td>0.6381</td><td>0.504</td><td>0.5477</td><td>0.4625</td><td>0.7059</td><td>0.6169</td></tr><tr><td>text-search-ada-query-001</td><td>0.4203</td><td>0.3431</td><td>0.6399</td><td>0.5031</td><td>0.5483</td><td>0.4625</td><td>0.7064</td><td>0.6174</td></tr><tr><td>Ilm-embedder</td><td>0.2558</td><td>0.1725</td><td>0.4499</td><td>0.3189</td><td>0.425</td><td>0.3059</td><td>0.5478</td><td>0.4756</td></tr><tr><td>bge-large-en-v1.5</td><td>0.4298</td><td>0.3423</td><td>0.6718</td><td>= 0.5221</td><td>0.563</td><td>0.4759</td><td>0.7183</td><td>0.6364</td></tr><tr><td>jina-embeddings-v2-base-en</td><td>0.0621</td><td>0.031</td><td>0.1479</td><td>0.0802</td><td>0.1412</td><td>0.0772</td><td>0.1909</td><td>0.1639</td></tr><tr><td>intfloat/e5-base-v2</td><td>0.1843</td><td>0.1161</td><td>0.3556</td><td>= 0.2334</td><td>0.3237</td><td>0.2165</td><td>0.4176</td><td>0.3716</td></tr><tr><td>voyage-02</td><td>0.3934</td><td>0.3143</td><td>0.6506</td><td>0.4619</td><td>0.586</td><td>0.4795</td><td>0.7467</td><td>0.6625</td></tr><tr><td>hkun!p/instructor-large</td><td>0.3458</td><td>0.265</td><td>0.5717</td><td>0.4229</td><td>0.5115</td><td>0.4118</td><td>0.659</td><td>0.5775</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Models</td><td>Accuracy</td></tr><tr><td>Retrieved Chunk</td><td>~Ground-truth Chunk</td></tr><tr><td>GPT-4</td><td>0.56</td><td>0.89</td></tr><tr><td>ChatGPT</td><td>0.44</td><td>0.57</td></tr><tr><td>Llama-2-70b-chat-hf</td><td>0.28</td><td>0.32</td></tr><tr><td>Mixtral-8x7B-Instruct</td><td>0.32</td><td>0.36</td></tr><tr><td>Claude-2.1</td><td>0.52</td><td>0.56</td></tr><tr><td>Google-PaLM</td><td>0.47</td><td>0.74</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "import html_to_json \n",
    "import html\n",
    "for element in tables:\n",
    "    display(HTML(element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Text, Table and Image Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LLM e.g. Llama3.1 or Gemini to provide summary for an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get text, table summaries\n",
    "text_summaries, table_summaries = generate_text_summaries(\n",
    "    texts_4k_token, tables, summarize_texts=False # Will use the original Text for getting vectorized\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "figure-1-1.jpg\n",
      "figure-4-2.jpg\n",
      "figure-7-3.jpg\n"
     ]
    }
   ],
   "source": [
    "## return string of summary for an input of image\n",
    "# Image summaries\n",
    "img_base64_list, image_summaries = generate_img_summaries(df_subset[\"img_folder\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Text, Table and Image Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors are semantic representation of texts. \n",
    "This is an important step to make documents searchable in the later pipeline. \n",
    "Embedding is an essential step in Transformer architecture, underlined to every modern LLMs. Therefore, many LLMs provide their embedding functions as services which are ready to use, e.g. OpenAI embedding API. However, it is important to consider privacy risk when exposing internal data to those services.\n",
    "\n",
    "IMPORTANT NOTE: \n",
    "1. the embedding method to perform similarity search in the retrieval pipeline must be the same to the one used to vectorize documents in this step. \n",
    "2. Public embedding method such as OpenAIEmbedding may cost a fraction of money and leak internal data.  \n",
    "\n",
    "Reference: https://python.langchain.com/v0.1/docs/modules/data_connection/text_embedding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings #To use other embeddings e.g. Llama or Gemini\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb_directory = os.path.join(os.getenv(\"VECTORDB_OPENAI_EM\"),\"\")\n",
    "vectorstore =  Chroma( collection_name=\"research_paper\",persist_directory=vectordb_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.reset_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever\n",
    "retriever_multi_vector_img = create_multi_vector_retriever(\n",
    "    vectorstore,\n",
    "    text_summaries,\n",
    "    texts_4k_token,\n",
    "    table_summaries,\n",
    "    tables,\n",
    "    image_summaries,\n",
    "    img_base64_list,\n",
    "    df_subset[\"imgs\"],\n",
    "    dict(df_subset)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Article Summary\n",
    "Using LLM to summarize the paper (as text or as image (convert pdf to image ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Store Article Summary + Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Store Vector DB (New version of Chroma persists data automatically after vectorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some vector databases of choices: Chroma, FAISS, Pinecone ... \n",
    "We will create Chroma vector database with openai embedding method. \n",
    "\n",
    "Note: different embedding methods will result different vector dimensions and cannot be stored together. \n",
    "The same embedding method to be used in retrieval pipeline\n",
    "\n",
    "Reference: https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval pipeline is to retrieve relevant chunk of knowledge from pre-prepared vectorized knowledge to enrich the LLM prompt with specified context. This pipeline is run to respond to each user’s query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to load from store if there is, here is Chroma vectordb we have just persisted. \n",
    "Perform a semantic search in the vectorized database to retrieve relevant embedded documents.\n",
    "\n",
    "NOTE: The embedding method used in this step must be same as which used to vectorize knowledges in the previous pipeline.\n",
    "\n",
    "There is opportunity to improve efficiency and quality of similarity search, especially when the knowledgebase gets larger and more complicated (type of sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Util functions for retrieval and response processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xff\\xd8\\xff\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            doc = resize_base64_image(doc, size=(1300, 600))\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    return {\"images\": b64_images, \"texts\": texts}\n",
    "\n",
    "\n",
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are financial analyst tasking with providing investment advice.\\n\"\n",
    "            \"You will be given a mixed of text, tables, and image(s) usually of charts or graphs.\\n\"\n",
    "            \"Use this information to provide investment advice related to the user question. \\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "\n",
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Multi-modal LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o\", max_tokens=1024)\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Process Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is the pperformance of GPT-4 vs Mixtral?\"\n",
    "#user_query = \"Describe the RAG-Sequence Model?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Retrieve Relevant Docs - Text, Table, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = retriever_multi_vector_img.invoke(user_query, limit=3) # Top k relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'doc_id': '7b933c86-50a5-44f1-905b-17ea7464eca9', 'source': '2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries.pdf', 'type': 'image', 'paper_id': 'a5cdaa51-39b4-42fe-bc76-e19fb729c37b'}, page_content='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'),\n",
       " Document(metadata={'doc_id': '9e4dd23d-9d21-4ee8-a6a1-4be3fb875f58', 'source': '2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries.pdf', 'type': 'table', 'paper_id': 'a5cdaa51-39b4-42fe-bc76-e19fb729c37b'}, page_content='<table><tr><td>Models</td><td>Accuracy</td></tr><tr><td>Retrieved Chunk</td><td>~Ground-truth Chunk</td></tr><tr><td>GPT-4</td><td>0.56</td><td>0.89</td></tr><tr><td>ChatGPT</td><td>0.44</td><td>0.57</td></tr><tr><td>Llama-2-70b-chat-hf</td><td>0.28</td><td>0.32</td></tr><tr><td>Mixtral-8x7B-Instruct</td><td>0.32</td><td>0.36</td></tr><tr><td>Claude-2.1</td><td>0.52</td><td>0.56</td></tr><tr><td>Google-PaLM</td><td>0.47</td><td>0.74</td></tr></table>'),\n",
       " Document(metadata={'doc_id': '1f131d66-bc00-4202-888e-ee02bf909761', 'source': '2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries.pdf', 'type': 'table', 'paper_id': 'a5cdaa51-39b4-42fe-bc76-e19fb729c37b'}, page_content='<table><tr><td>Embedding</td><td/><td>Without</td><td>Reranker</td><td/><td>With bge-reranker-large</td></tr><tr><td>MRR@10</td><td>MAP@10</td><td>Hits@10</td><td>Hits@4</td><td>MRR@10</td><td>MAP@10</td><td>Hits@10</td><td>Hits@4</td></tr><tr><td>text-embedding-ada-002</td><td>0.4203</td><td>0.3431</td><td>0.6381</td><td>0.504</td><td>0.5477</td><td>0.4625</td><td>0.7059</td><td>0.6169</td></tr><tr><td>text-search-ada-query-001</td><td>0.4203</td><td>0.3431</td><td>0.6399</td><td>0.5031</td><td>0.5483</td><td>0.4625</td><td>0.7064</td><td>0.6174</td></tr><tr><td>Ilm-embedder</td><td>0.2558</td><td>0.1725</td><td>0.4499</td><td>0.3189</td><td>0.425</td><td>0.3059</td><td>0.5478</td><td>0.4756</td></tr><tr><td>bge-large-en-v1.5</td><td>0.4298</td><td>0.3423</td><td>0.6718</td><td>= 0.5221</td><td>0.563</td><td>0.4759</td><td>0.7183</td><td>0.6364</td></tr><tr><td>jina-embeddings-v2-base-en</td><td>0.0621</td><td>0.031</td><td>0.1479</td><td>0.0802</td><td>0.1412</td><td>0.0772</td><td>0.1909</td><td>0.1639</td></tr><tr><td>intfloat/e5-base-v2</td><td>0.1843</td><td>0.1161</td><td>0.3556</td><td>= 0.2334</td><td>0.3237</td><td>0.2165</td><td>0.4176</td><td>0.3716</td></tr><tr><td>voyage-02</td><td>0.3934</td><td>0.3143</td><td>0.6506</td><td>0.4619</td><td>0.586</td><td>0.4795</td><td>0.7467</td><td>0.6625</td></tr><tr><td>hkun!p/instructor-large</td><td>0.3458</td><td>0.265</td><td>0.5717</td><td>0.4229</td><td>0.5115</td><td>0.4118</td><td>0.659</td><td>0.5775</td></tr></table>'),\n",
       " Document(metadata={'doc_id': '58e493de-a66f-4e2b-bc38-1014fbf33436', 'source': '2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries.pdf', 'type': 'text', 'paper_id': 'a5cdaa51-39b4-42fe-bc76-e19fb729c37b'}, page_content='4\\n\\n2024\\n\\n2\\n\\n0\\n\\n2\\n\\nn a J 7 2 ] L C . s c [ 1 v 1 9 3 5 1 . 1 0 4 2\\n\\n:\\n\\nv\\n\\ni\\n\\nX\\n\\nr\\n\\na\\n\\nMultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries\\n\\nYixuan Tang and Yi Yang Hong Kong University of Science and Technology {yixuantang,imyiyang}@ust.hk\\n\\nAbstract\\n\\nRetrieval-augmented generation (RAG) aug-\\n\\nments large language models (LLM) by re- trieving relevant knowledge, showing promis- ing potential in mitigating LLM hallucinations and enhancing response quality, thereby facil- itating the great adoption of LLMs in prac- tice. However, we find that existing RAG sys- tems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence. Furthermore, to our knowledge, no existing RAG benchmarking dataset focuses on multi- hop queries. In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a knowledge base, a large collection of multi- hop queries, their ground-truth answers, and the associated supporting evidence. We detail the procedure of building the dataset, utiliz- ing an English news article dataset as the un- derlying RAG knowledge base. We demon- strate the benchmarking utility of MultiHop- RAG in two experiments. The first experiment compares different embedding models for re- trieving evidence for multi-hop queries. In the second experiment, we examine the capabili- ties of various state-of-the-art LLMs, includ- ing GPT-4, PaLM, and Llama2-70B, in rea- soning and answering multi-hop queries given the evidence. Both experiments reveal that ex- isting RAG methods perform unsatisfactorily in retrieving and answering multi-hop queries. We hope MultiHop-RAG will be a valuable re- source for the community in developing effec- tive RAG systems, thereby facilitating greater adoption of LLMs in practice. The MultiHop- RAG and implemented RAG system is publicly available at https://github.com/yixuantt/ MultiHop-RAG/.\\n\\nIntroduction\\n\\nThe emergence of large language models (LLMs), such as ChatGPT, has fostered a wide range of inno- vations, powering intelligent chatbots and other nat- ural language processing (NLP) applications (Ope-\\n\\nMulti-Documents Which company among \\' | Google, Apple, and Nvidia Google Chunk]| ! | reported the largest profit 4 \\' | margins in their third- nae Chunk]) (1 1 | quarter reports for 2023? q Database” || Nvidia -—[Chunk]} ) |\\n\\nFigure 1: RAG with multi-hop query.\\n\\nnAI, 2023). One promising use case is Retrieval- Augmented Generation (RAG) (Asai et al., 2023), which optimizes the output of a large language model by referencing an external knowledge base outside of the LLM training data sources before generating a response. RAG improves LLM’s re- sponse (Borgeaud et al., 2022) and also mitigates the occurrence of hallucinations, thereby enhancing the models’ credibility (Gao et al., 2023). LLM- based frameworks, such as LlamaIndex (Liu, 2022) and LangChain (Chase, 2022), specialize in sup- porting RAG pipelines.\\n\\nIn real-world Retrieval-Augmented Generation (RAG) applications, a user’s query often necessi- tates retrieving and reasoning over evidence from multiple documents, a process known as multi-hop query. For instance, consider financial analysis us- ing a database of financial reports. A financial ana- lyst might query, Which company among Google, Apple, and Nvidia reported the largest profit mar- gins in their third-quarter reports for 2023? or inquire about a specific company’s performance over time, such as How does Apple’s sales trend look over the past three years? These queries re- quire evidence from multiple documents to formu- late an answer. Due to the multifaceted nature of such queries, involving information from various sources, traditional similarity matching methods like cosine similarity between query and financial Answer\\n\\nYes\\n\\nTable 1: An example of a multi-hop query, including supporting evidence from two news articles, the paraphrased claim, the bridge-topic and bridge-entity, and the corresponding answer.\\n\\nreport chunk embeddings might not yield optimal results. We demonstrate this multi-hop retrieval process in Figure 1.\\n\\nHowever, existing RAG benchmarks, such as RGB (Chen et al., 2023) and RECALL (Liu et al., 2023), mainly evaluate a simple case where the an- swer of a query can be retrieved and solved using one single piece of evidence. None of these bench- marks assess the retrieval and reasoning capability of LLMs for complex multi-hop queries. To ad- dress this gap and make RAG benchmarking more closely resemble real-world scenarios, in this paper, we introduce MultiHop-RAG. To our knowledge, MultiHop-RAG is one of the first RAG datasets focusing specifically on multi-hop queries.\\n\\nBased on the RAG queries commonly encoun- tered in real-world scenarios, we first categorize multi-hop queries into four types: Inference query, Comparison query, Temporal query, and Null query. The first three types — Inference, Com- parison, and Temporal — require the retrieval and analysis of evidence from multiple sources, encom- passing tasks like inferring relationships, compar- ing data points, and sequencing events over time. The Null query represents a scenario where the query cannot be derived from the knowledge base. This category is crucial for assessing whether an LLM might hallucinate an answer to a multi-hop query when the retrieved text lacks relevance.\\n\\nWe construct our RAG knowledge base using a collection of news articles. Using GPT-4 as a data generator, we then take an extensive procedure to construct a diverse set of multi-hop queries, each requiring the retrieval and reasoning over multiple documents. An example of query construction is shown in Table 1. First, we begin by extracting\\n\\nfactual sentences from each news article as evi-\\n\\ndence. For example, an extracted piece of evidence from an article may state: “Back then, just like today, home prices had boomed for years before Fed officials were ultimately forced to hike interest rates aggressively in an attempt to fight inflation.” Second, we input each evidence piece into GPT-4, prompting it to rephrase the evidence into a claim. This claim is clarified with a disambiguated topic and entity. For instance, GPT-4 might rephrase the aforementioned evidence into: “Federal Reserve officials were forced to aggressively hike interest rates to combat inflation after years of booming home prices”, identifying “Interest rate hikes to combat inflation” as the topic and “Federal Re- serve” as the entity. These topics and entities act as bridges for constructing multi-hop queries, known as bridge-topic or bridge-entity. Next, we use GPT- 4 to generate specific multi-hop queries related to the same bridge-topic or bridge-entity, accompa- nied by the correct answers. Lastly, we undertake a validation step to ensure the data quality.\\n\\nWe demonstrate the benchmarking capabilities of MultiHop-RAG using two experiments, utilizing a RAG system implemented with LlamaIndex (Liu, 2022). The first experiment involves a comparison of different embedding models for retrieving rele- vant evidence for multi-hop queries. In the second experiment, we assess the reasoning and answering abilities of various state-of-the-art LLMs, including GPT-4, GPT-3.5, PaLM, Claude-2, Llama2-70B, and Mixtral-8x7B, for multi-hop queries when re- trieved text is provided. The results from both ex- periments indicate that the current RAG implemen- tations are inadequate for effectively retrieving and answering multi-hop queries. We publicly release\\n\\nthis challenging MultiHop-RAG dataset and hope it will be a valuable resource for the community in de- veloping and benchmarking RAG systems, thereby unleashing the great potential of generative AI in practice. 2 RAG with multi-Hop queries\\n\\n2.1 Retrieval-augmented Generation (RAG)\\n\\nIn an RAG application, we utilize an external cor- pus, denoted as D, which comprises multiple docu- ments and serves as the knowledge base. Each doc- ument within this corpus, represented as di ∈ D, is segmented into a set of chunks.These chunks are then transformed into vector representations using an embedding model and stored in an embedding database. Given a user query q, the system typi- cally retrieves the top-K chunks that best match the query. These chunks constitute the retrieval set for query q, represented as Rq = {r1, r2, ..., rK}. The retrieved chunks, combined with the query and an optional prompt, are then fed into an LLM to generate a final answer, following the format: LLM(q, Rq, prompt) → answer.\\n\\n2.2 Multi-Hop Query\\n\\nWe define a multi-hop query as one that requires retrieving and reasoning over multiple pieces of supporting evidence to provide an answer. In other words, for a multi-hop query q, the chunks in the retrieval set Rq collectively provide an answer to q. For example, the query \"Which company among Google, Apple, and Nvidia reported the largest profit margins in their third-quarter reports for 2023?\" requires 1) retrieving relevant pieces of evidence related to profit margins from the reports of the three companies; 2) generating an answer by comparing and reasoning from the multiple pieces of retrieved evidence. This differs from a single- hop query such as \"What is Google’s profit margin in the third-quarter reports for 2023,\" where the answer can be directly derived from a single piece of evidence.\\n\\nBased on the queries commonly used in real- world RAG systems, we identify four types of multi-hop queries. For each type, we present a hypothetical query within the context of a financial RAG system, where the knowledge base consists of a collection of annual reports.\\n\\nInference query: For such a query q, the answer is deduced through reasoning from the retrieval set Rq. An example of an inference query might\\n\\nbe: Which report discusses the supply chain risk of Apple, the 2019 annual report or the 2020 annual report?\\n\\nComparison query: For such a query q, the an- swer requires a comparison of evidence within the retrieval set Rq. For instance, a comparison query might ask: Did Netflix or Google report higher revenue for the year 2023?\"\\n\\nTemporal query: For such a query q, the answer requires an analysis of the temporal information of the retrieved chunks. For example, a temporal query may ask: Did Apple introduce the AirTag tracking device before or after the launch of the 5th generation iPad Pro?\\n\\nNull query: For such as query q, the answer cannot be derived from the retrieved set Rq. We include the null query to assess the generation quality, es- pecially regarding the issue of hallucination. For a null query, even though a retrieved set is provided, an LLM should produce a null response instead of hallucinating an answer. For example, assum- ing ABCD is a non-existent company, a null query might ask: What are the sales of company ABCD as reported in its 2022 and 2023 annual reports? 2.3 Evaluation Metrics\\n\\nAn RAG system handling multi-hop queries can be assessed from two key aspects: retrieval evaluation and generation evaluation.\\n\\nRetrieval Evaluation: Evidently, the quality of the retrieval set Rq determines the final genera- tion quality. We compare the retrieved set with the ground truth evidence associated with each query, except for the null queries, as they have no evidence to derive from. Assuming the top- K chunks are retrieved, i.e., |Rq| = K, we use retrieval evaluation metrics including Mean Aver- age Precision at K (MAP@K), Mean Reciprocal Rank at K (MRR@K), and Hit Rate at K (Hit@K). MAP@K measures the average top-K retrieval pre- cision across all queries. MRR@K calculates the average of the reciprocal ranks of the first relevant chunk for each query, considering the top-K re- trieved set. Hit@K metric measures the fraction of evidence that appears in the top-K retrieved set.\\n\\nResponse Evaluation: Since the multi-hop query requires reasoning over multiple pieces of retrieved chunks, we can also evaluate the reason- ing capability of the LLM by comparing the LLM response with the ground truth answer of the query.\\n\\n| Download Dataset |! Dataset Collection | ---6 —————_ | | Select News | Extraction | Select Sentences | | Claim Generation . . ooo Claim Generation | --0| [pridge-Entity Generation | | Bridge-Topic Generation OO = Query and Answer all Inference || Comparison} | Generation ~ | Null Temporal ji | Manually Review | Quality Assurance +--Q = | | GPT-4 Review I\\n\\nFigure 2: MultiHop-RAG Construction Pipeline.\\n\\n3 A Benchmarking Dataset: MultiHop-RAG\\n\\nIn this section, we provide detailed information on the construction of the MultiHop-RAG dataset. Specifically, we describe the process of creating a set of multi-hop queries, along with the correspond- ing ground truth evidence sets and answers derived from a collection of news articles. 3.1 MultiHop-RAG Construction\\n\\nStep 1: Dataset Collection. We download a news dataset using the mediastack API 1, a REST API in- terface delivering worldwide news data. The news data source comprises various English-language websites covering a range of news categories: en- tertainment, business, sports, technology, health, and science. To mimic real-world RAG scenarios, where the knowledge base data, such as an enter- prise’s internal data, may differ from the LLMs’ training data, we select news articles published from September 26, 2023, to December 26, 2023. This timeframe extends beyond the knowledge cut- off of some widely-used LLMs, including Chat- GPT and LLaMA, as of the time of writing. This selection also helps in teasing out the possibility of the underlying LLM having been exposed to these news articles. We only keep articles with a token length greater than or equal to 1,024. Every\\n\\n1https://mediastack.com/\\n\\nnews article is paired with metadata, including the title, publish date, author, category, URL, and news source.\\n\\nStep 2: Evidence Extraction. For each article, we extract factual or opinion sentences using a trained language model 2. These factual sentences are later used as evidence for answering multi-hop queries. We retain only those news articles containing ev- idence that may have overlapping keywords with other news articles. This allows us to later create multi-hop queries where the answer’s evidences are drawn from multiple sources.\\n\\nStep 3: Claim, Bridge-Entity, Bridge-Topic Gen- eration. Our goal is to use GPT-4 to automatically generate high-quality multi-hop queries using the evidence set. However, the raw evidence obtained from Step 2 is not ideal for query generation due to inconsistency in linguistic structure. For exam- ple, some pieces of evidence use pronouns to refer to subjects and lack the actual entity in the text. To address this, we employ GPT-4 to paraphrase the evidence, which we refer to as claims, given the original evidence and its context. To ensure consistency between the generated claim and the evidence, we further perform fact-checking using the UniEval (Zhong et al., 2022) framework to ver- ify the alignment between the evidence and claim. Appendix A presents the prompt used for GPT-4 for claim generation.\\n\\nBridge-Entity and Bridge-Topic: The shared en- tity or topic across pieces of evidence is referred to as the bridge-entity or bridge-topic. These bridge- entities or bridge-topics can be used to link dif- ferent pieces of evidence from which a multi-hop query’s answer is derived. For example, in a claim such as “Google reports its third-quarter results for 2023, showcasing a detailed overview of its finan- cial performance, including revenue growth, profit margins”, the term profit margin can be viewed as a bridge-topic and the term Google can be viewed as a bridge-entity that links the different pieces of evidence. We prompt GPT-4 to identify the bridge- entity and bridge-topic for each claim. Appendix A also presents the prompt used for GPT-4 for bridge generation.\\n\\nStep 4: Query and Answer Generation. In this step, we leverage the bridge-entity or bridge-topic to generate multi-hop queries. Specifically, we first group the claims having the same bridge-entity or\\n\\n2https://huggingface.co/lighteternal/fact-or-opinion-xlmr- el\\n\\nbridge-topic into a claim set. We restrict the claim set to have at least two claims but no more than four claims. For each type of query, we feed the claim set to GPT-4 and prompt it with an instruction to generate a query with information from each claim. Below, we explain the specifications for different multi-hop query types. In the construction of each query, we also include the source of the news article where the supporting evidence is associated with to mimic real-world RAG scenarios. Appendix A presents the prompts used for GPT-4 for query generation.\\n\\nInference Query: These queries are formulated by synthesizing the various characterizations of the bridge-entity across multiple claims, with the final answer being the identification of the entity itself.\\n\\nComparison Query: These queries are struc- tured to compare the similarities and differences related to the bridge entity or topic. The resultant answer to such queries is typically a definitive “yes” or “no”, based on the comparison.\\n\\nTemporal Query: These queries explore the temporal ordering of events across different points in time. The answer to such queries is typically a “yes” or “no” or a single temporal indicator word like “before” or “after”.')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/jpeg;base64,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\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Models</td><td>Accuracy</td></tr><tr><td>Retrieved Chunk</td><td>~Ground-truth Chunk</td></tr><tr><td>GPT-4</td><td>0.56</td><td>0.89</td></tr><tr><td>ChatGPT</td><td>0.44</td><td>0.57</td></tr><tr><td>Llama-2-70b-chat-hf</td><td>0.28</td><td>0.32</td></tr><tr><td>Mixtral-8x7B-Instruct</td><td>0.32</td><td>0.36</td></tr><tr><td>Claude-2.1</td><td>0.52</td><td>0.56</td></tr><tr><td>Google-PaLM</td><td>0.47</td><td>0.74</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Embedding</td><td/><td>Without</td><td>Reranker</td><td/><td>With bge-reranker-large</td></tr><tr><td>MRR@10</td><td>MAP@10</td><td>Hits@10</td><td>Hits@4</td><td>MRR@10</td><td>MAP@10</td><td>Hits@10</td><td>Hits@4</td></tr><tr><td>text-embedding-ada-002</td><td>0.4203</td><td>0.3431</td><td>0.6381</td><td>0.504</td><td>0.5477</td><td>0.4625</td><td>0.7059</td><td>0.6169</td></tr><tr><td>text-search-ada-query-001</td><td>0.4203</td><td>0.3431</td><td>0.6399</td><td>0.5031</td><td>0.5483</td><td>0.4625</td><td>0.7064</td><td>0.6174</td></tr><tr><td>Ilm-embedder</td><td>0.2558</td><td>0.1725</td><td>0.4499</td><td>0.3189</td><td>0.425</td><td>0.3059</td><td>0.5478</td><td>0.4756</td></tr><tr><td>bge-large-en-v1.5</td><td>0.4298</td><td>0.3423</td><td>0.6718</td><td>= 0.5221</td><td>0.563</td><td>0.4759</td><td>0.7183</td><td>0.6364</td></tr><tr><td>jina-embeddings-v2-base-en</td><td>0.0621</td><td>0.031</td><td>0.1479</td><td>0.0802</td><td>0.1412</td><td>0.0772</td><td>0.1909</td><td>0.1639</td></tr><tr><td>intfloat/e5-base-v2</td><td>0.1843</td><td>0.1161</td><td>0.3556</td><td>= 0.2334</td><td>0.3237</td><td>0.2165</td><td>0.4176</td><td>0.3716</td></tr><tr><td>voyage-02</td><td>0.3934</td><td>0.3143</td><td>0.6506</td><td>0.4619</td><td>0.586</td><td>0.4795</td><td>0.7467</td><td>0.6625</td></tr><tr><td>hkun!p/instructor-large</td><td>0.3458</td><td>0.265</td><td>0.5717</td><td>0.4229</td><td>0.5115</td><td>0.4118</td><td>0.659</td><td>0.5775</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  4\n",
      "\n",
      "2024\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "n a J 7 2 ] L C . s c [ 1 v 1 9 3 5 1 . 1 0 4 2\n",
      "\n",
      ":\n",
      "\n",
      "v\n",
      "\n",
      "i\n",
      "\n",
      "X\n",
      "\n",
      "r\n",
      "\n",
      "a\n",
      "\n",
      "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries\n",
      "\n",
      "Yixuan Tang and Yi Yang Hong Kong University of Science and Technology {yixuantang,imyiyang}@ust.hk\n",
      "\n",
      "Abstract\n",
      "\n",
      "Retrieval-augmented generation (RAG) aug-\n",
      "\n",
      "ments large language models (LLM) by re- trieving relevant knowledge, showing promis- ing potential in mitigating LLM hallucinations and enhancing response quality, thereby facil- itating the great adoption of LLMs in prac- tice. However, we find that existing RAG sys- tems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence. Furthermore, to our knowledge, no existing RAG benchmarking dataset focuses on multi- hop queries. In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a knowledge base, a large collection of multi- hop queries, their ground-truth answers, and the associated supporting evidence. We detail the procedure of building the dataset, utiliz- ing an English news article dataset as the un- derlying RAG knowledge base. We demon- strate the benchmarking utility of MultiHop- RAG in two experiments. The first experiment compares different embedding models for re- trieving evidence for multi-hop queries. In the second experiment, we examine the capabili- ties of various state-of-the-art LLMs, includ- ing GPT-4, PaLM, and Llama2-70B, in rea- soning and answering multi-hop queries given the evidence. Both experiments reveal that ex- isting RAG methods perform unsatisfactorily in retrieving and answering multi-hop queries. We hope MultiHop-RAG will be a valuable re- source for the community in developing effec- tive RAG systems, thereby facilitating greater adoption of LLMs in practice. The MultiHop- RAG and implemented RAG system is publicly available at https://github.com/yixuantt/ MultiHop-RAG/.\n",
      "\n",
      "Introduction\n",
      "\n",
      "The emergence of large language models (LLMs), such as ChatGPT, has fostered a wide range of inno- vations, powering intelligent chatbots and other nat- ural language processing (NLP) applications (Ope-\n",
      "\n",
      "Multi-Documents Which company among ' | Google, Apple, and Nvidia Google Chunk]| ! | reported the largest profit 4 ' | margins in their third- nae Chunk]) (1 1 | quarter reports for 2023? q Database” || Nvidia -—[Chunk]} ) |\n",
      "\n",
      "Figure 1: RAG with multi-hop query.\n",
      "\n",
      "nAI, 2023). One promising use case is Retrieval- Augmented Generation (RAG) (Asai et al., 2023), which optimizes the output of a large language model by referencing an external knowledge base outside of the LLM training data sources before generating a response. RAG improves LLM’s re- sponse (Borgeaud et al., 2022) and also mitigates the occurrence of hallucinations, thereby enhancing the models’ credibility (Gao et al., 2023). LLM- based frameworks, such as LlamaIndex (Liu, 2022) and LangChain (Chase, 2022), specialize in sup- porting RAG pipelines.\n",
      "\n",
      "In real-world Retrieval-Augmented Generation (RAG) applications, a user’s query often necessi- tates retrieving and reasoning over evidence from multiple documents, a process known as multi-hop query. For instance, consider financial analysis us- ing a database of financial reports. A financial ana- lyst might query, Which company among Google, Apple, and Nvidia reported the largest profit mar- gins in their third-quarter reports for 2023? or inquire about a specific company’s performance over time, such as How does Apple’s sales trend look over the past three years? These queries re- quire evidence from multiple documents to formu- late an answer. Due to the multifaceted nature of such queries, involving information from various sources, traditional similarity matching methods like cosine similarity between query and financial Answer\n",
      "\n",
      "Yes\n",
      "\n",
      "Table 1: An example of a multi-hop query, including supporting evidence from two news articles, the paraphrased claim, the bridge-topic and bridge-entity, and the corresponding answer.\n",
      "\n",
      "report chunk embeddings might not yield optimal results. We demonstrate this multi-hop retrieval process in Figure 1.\n",
      "\n",
      "However, existing RAG benchmarks, such as RGB (Chen et al., 2023) and RECALL (Liu et al., 2023), mainly evaluate a simple case where the an- swer of a query can be retrieved and solved using one single piece of evidence. None of these bench- marks assess the retrieval and reasoning capability of LLMs for complex multi-hop queries. To ad- dress this gap and make RAG benchmarking more closely resemble real-world scenarios, in this paper, we introduce MultiHop-RAG. To our knowledge, MultiHop-RAG is one of the first RAG datasets focusing specifically on multi-hop queries.\n",
      "\n",
      "Based on the RAG queries commonly encoun- tered in real-world scenarios, we first categorize multi-hop queries into four types: Inference query, Comparison query, Temporal query, and Null query. The first three types — Inference, Com- parison, and Temporal — require the retrieval and analysis of evidence from multiple sources, encom- passing tasks like inferring relationships, compar- ing data points, and sequencing events over time. The Null query represents a scenario where the query cannot be derived from the knowledge base. This category is crucial for assessing whether an LLM might hallucinate an answer to a multi-hop query when the retrieved text lacks relevance.\n",
      "\n",
      "We construct our RAG knowledge base using a collection of news articles. Using GPT-4 as a data generator, we then take an extensive procedure to construct a diverse set of multi-hop queries, each requiring the retrieval and reasoning over multiple documents. An example of query construction is shown in Table 1. First, we begin by extracting\n",
      "\n",
      "factual sentences from each news article as evi-\n",
      "\n",
      "dence. For example, an extracted piece of evidence from an article may state: “Back then, just like today, home prices had boomed for years before Fed officials were ultimately forced to hike interest rates aggressively in an attempt to fight inflation.” Second, we input each evidence piece into GPT-4, prompting it to rephrase the evidence into a claim. This claim is clarified with a disambiguated topic and entity. For instance, GPT-4 might rephrase the aforementioned evidence into: “Federal Reserve officials were forced to aggressively hike interest rates to combat inflation after years of booming home prices”, identifying “Interest rate hikes to combat inflation” as the topic and “Federal Re- serve” as the entity. These topics and entities act as bridges for constructing multi-hop queries, known as bridge-topic or bridge-entity. Next, we use GPT- 4 to generate specific multi-hop queries related to the same bridge-topic or bridge-entity, accompa- nied by the correct answers. Lastly, we undertake a validation step to ensure the data quality.\n",
      "\n",
      "We demonstrate the benchmarking capabilities of MultiHop-RAG using two experiments, utilizing a RAG system implemented with LlamaIndex (Liu, 2022). The first experiment involves a comparison of different embedding models for retrieving rele- vant evidence for multi-hop queries. In the second experiment, we assess the reasoning and answering abilities of various state-of-the-art LLMs, including GPT-4, GPT-3.5, PaLM, Claude-2, Llama2-70B, and Mixtral-8x7B, for multi-hop queries when re- trieved text is provided. The results from both ex- periments indicate that the current RAG implemen- tations are inadequate for effectively retrieving and answering multi-hop queries. We publicly release\n",
      "\n",
      "this challenging MultiHop-RAG dataset and hope it will be a valuable resource for the community in de- veloping and benchmarking RAG systems, thereby unleashing the great potential of generative AI in practice. 2 RAG with multi-Hop queries\n",
      "\n",
      "2.1 Retrieval-augmented Generation (RAG)\n",
      "\n",
      "In an RAG application, we utilize an external cor- pus, denoted as D, which comprises multiple docu- ments and serves as the knowledge base. Each doc- ument within this corpus, represented as di ∈ D, is segmented into a set of chunks.These chunks are then transformed into vector representations using an embedding model and stored in an embedding database. Given a user query q, the system typi- cally retrieves the top-K chunks that best match the query. These chunks constitute the retrieval set for query q, represented as Rq = {r1, r2, ..., rK}. The retrieved chunks, combined with the query and an optional prompt, are then fed into an LLM to generate a final answer, following the format: LLM(q, Rq, prompt) → answer.\n",
      "\n",
      "2.2 Multi-Hop Query\n",
      "\n",
      "We define a multi-hop query as one that requires retrieving and reasoning over multiple pieces of supporting evidence to provide an answer. In other words, for a multi-hop query q, the chunks in the retrieval set Rq collectively provide an answer to q. For example, the query \"Which company among Google, Apple, and Nvidia reported the largest profit margins in their third-quarter reports for 2023?\" requires 1) retrieving relevant pieces of evidence related to profit margins from the reports of the three companies; 2) generating an answer by comparing and reasoning from the multiple pieces of retrieved evidence. This differs from a single- hop query such as \"What is Google’s profit margin in the third-quarter reports for 2023,\" where the answer can be directly derived from a single piece of evidence.\n",
      "\n",
      "Based on the queries commonly used in real- world RAG systems, we identify four types of multi-hop queries. For each type, we present a hypothetical query within the context of a financial RAG system, where the knowledge base consists of a collection of annual reports.\n",
      "\n",
      "Inference query: For such a query q, the answer is deduced through reasoning from the retrieval set Rq. An example of an inference query might\n",
      "\n",
      "be: Which report discusses the supply chain risk of Apple, the 2019 annual report or the 2020 annual report?\n",
      "\n",
      "Comparison query: For such a query q, the an- swer requires a comparison of evidence within the retrieval set Rq. For instance, a comparison query might ask: Did Netflix or Google report higher revenue for the year 2023?\"\n",
      "\n",
      "Temporal query: For such a query q, the answer requires an analysis of the temporal information of the retrieved chunks. For example, a temporal query may ask: Did Apple introduce the AirTag tracking device before or after the launch of the 5th generation iPad Pro?\n",
      "\n",
      "Null query: For such as query q, the answer cannot be derived from the retrieved set Rq. We include the null query to assess the generation quality, es- pecially regarding the issue of hallucination. For a null query, even though a retrieved set is provided, an LLM should produce a null response instead of hallucinating an answer. For example, assum- ing ABCD is a non-existent company, a null query might ask: What are the sales of company ABCD as reported in its 2022 and 2023 annual reports? 2.3 Evaluation Metrics\n",
      "\n",
      "An RAG system handling multi-hop queries can be assessed from two key aspects: retrieval evaluation and generation evaluation.\n",
      "\n",
      "Retrieval Evaluation: Evidently, the quality of the retrieval set Rq determines the final genera- tion quality. We compare the retrieved set with the ground truth evidence associated with each query, except for the null queries, as they have no evidence to derive from. Assuming the top- K chunks are retrieved, i.e., |Rq| = K, we use retrieval evaluation metrics including Mean Aver- age Precision at K (MAP@K), Mean Reciprocal Rank at K (MRR@K), and Hit Rate at K (Hit@K). MAP@K measures the average top-K retrieval pre- cision across all queries. MRR@K calculates the average of the reciprocal ranks of the first relevant chunk for each query, considering the top-K re- trieved set. Hit@K metric measures the fraction of evidence that appears in the top-K retrieved set.\n",
      "\n",
      "Response Evaluation: Since the multi-hop query requires reasoning over multiple pieces of retrieved chunks, we can also evaluate the reason- ing capability of the LLM by comparing the LLM response with the ground truth answer of the query.\n",
      "\n",
      "| Download Dataset |! Dataset Collection | ---6 —————_ | | Select News | Extraction | Select Sentences | | Claim Generation . . ooo Claim Generation | --0| [pridge-Entity Generation | | Bridge-Topic Generation OO = Query and Answer all Inference || Comparison} | Generation ~ | Null Temporal ji | Manually Review | Quality Assurance +--Q = | | GPT-4 Review I\n",
      "\n",
      "Figure 2: MultiHop-RAG Construction Pipeline.\n",
      "\n",
      "3 A Benchmarking Dataset: MultiHop-RAG\n",
      "\n",
      "In this section, we provide detailed information on the construction of the MultiHop-RAG dataset. Specifically, we describe the process of creating a set of multi-hop queries, along with the correspond- ing ground truth evidence sets and answers derived from a collection of news articles. 3.1 MultiHop-RAG Construction\n",
      "\n",
      "Step 1: Dataset Collection. We download a news dataset using the mediastack API 1, a REST API in- terface delivering worldwide news data. The news data source comprises various English-language websites covering a range of news categories: en- tertainment, business, sports, technology, health, and science. To mimic real-world RAG scenarios, where the knowledge base data, such as an enter- prise’s internal data, may differ from the LLMs’ training data, we select news articles published from September 26, 2023, to December 26, 2023. This timeframe extends beyond the knowledge cut- off of some widely-used LLMs, including Chat- GPT and LLaMA, as of the time of writing. This selection also helps in teasing out the possibility of the underlying LLM having been exposed to these news articles. We only keep articles with a token length greater than or equal to 1,024. Every\n",
      "\n",
      "1https://mediastack.com/\n",
      "\n",
      "news article is paired with metadata, including the title, publish date, author, category, URL, and news source.\n",
      "\n",
      "Step 2: Evidence Extraction. For each article, we extract factual or opinion sentences using a trained language model 2. These factual sentences are later used as evidence for answering multi-hop queries. We retain only those news articles containing ev- idence that may have overlapping keywords with other news articles. This allows us to later create multi-hop queries where the answer’s evidences are drawn from multiple sources.\n",
      "\n",
      "Step 3: Claim, Bridge-Entity, Bridge-Topic Gen- eration. Our goal is to use GPT-4 to automatically generate high-quality multi-hop queries using the evidence set. However, the raw evidence obtained from Step 2 is not ideal for query generation due to inconsistency in linguistic structure. For exam- ple, some pieces of evidence use pronouns to refer to subjects and lack the actual entity in the text. To address this, we employ GPT-4 to paraphrase the evidence, which we refer to as claims, given the original evidence and its context. To ensure consistency between the generated claim and the evidence, we further perform fact-checking using the UniEval (Zhong et al., 2022) framework to ver- ify the alignment between the evidence and claim. Appendix A presents the prompt used for GPT-4 for claim generation.\n",
      "\n",
      "Bridge-Entity and Bridge-Topic: The shared en- tity or topic across pieces of evidence is referred to as the bridge-entity or bridge-topic. These bridge- entities or bridge-topics can be used to link dif- ferent pieces of evidence from which a multi-hop query’s answer is derived. For example, in a claim such as “Google reports its third-quarter results for 2023, showcasing a detailed overview of its finan- cial performance, including revenue growth, profit margins”, the term profit margin can be viewed as a bridge-topic and the term Google can be viewed as a bridge-entity that links the different pieces of evidence. We prompt GPT-4 to identify the bridge- entity and bridge-topic for each claim. Appendix A also presents the prompt used for GPT-4 for bridge generation.\n",
      "\n",
      "Step 4: Query and Answer Generation. In this step, we leverage the bridge-entity or bridge-topic to generate multi-hop queries. Specifically, we first group the claims having the same bridge-entity or\n",
      "\n",
      "2https://huggingface.co/lighteternal/fact-or-opinion-xlmr- el\n",
      "\n",
      "bridge-topic into a claim set. We restrict the claim set to have at least two claims but no more than four claims. For each type of query, we feed the claim set to GPT-4 and prompt it with an instruction to generate a query with information from each claim. Below, we explain the specifications for different multi-hop query types. In the construction of each query, we also include the source of the news article where the supporting evidence is associated with to mimic real-world RAG scenarios. Appendix A presents the prompts used for GPT-4 for query generation.\n",
      "\n",
      "Inference Query: These queries are formulated by synthesizing the various characterizations of the bridge-entity across multiple claims, with the final answer being the identification of the entity itself.\n",
      "\n",
      "Comparison Query: These queries are struc- tured to compare the similarities and differences related to the bridge entity or topic. The resultant answer to such queries is typically a definitive “yes” or “no”, based on the comparison.\n",
      "\n",
      "Temporal Query: These queries explore the temporal ordering of events across different points in time. The answer to such queries is typically a “yes” or “no” or a single temporal indicator word like “before” or “after”.\n"
     ]
    }
   ],
   "source": [
    "for d in relevant_docs:\n",
    "    if d.metadata[\"type\"] == \"image\":\n",
    "        plt_img_base64(d.page_content)\n",
    "    elif d.metadata[\"type\"] == \"table\":\n",
    "        display(HTML(d.page_content))\n",
    "    else:\n",
    "        print(\"Text: \",d.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bar charts comparing accuracy percentages of Mixtral-8x7B and GPT-4 models across categories: null, comparison, temporal, and inference. The top chart is for \"Retrieved Chunk,\" and the bottom is \"Ground-truth Chunk.\" GPT-4 generally shows higher accuracy.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_summaries[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Reranking and Document Selection (Leave this to the MultiModal Retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Augmented Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to write the prompt. It will basically instruct the LLM to generate result based on the {question} and the {context}.\n",
    "\n",
    "The context is inputted from the retrieved documents from p previous step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "QA_RAG = \"SIMPLE_QUESTION_ANSWER_RAG\"\n",
    "\n",
    "MM_QA_RAG = \"MULTIMODAL_QUESTION_ANSWER_RAG\"\n",
    "\n",
    "prompt_type = {\n",
    "    \"QA_RAG\" : \"SIMPLE_QUESTION_ANSWER_RAG\",\n",
    "    \"MM_QA_RAG\" : \"MULTIMODAL_QUESTION_ANSWER_RAG\",\n",
    "}\n",
    "\n",
    "simple_rag_template = \"\"\"\n",
    "Answer the question based on the context below. \n",
    "If you can't answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "multimodal_rag_template = \"\"\"\n",
    "To define the new Prompt.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "def initPrompt(type) -> ChatPromptTemplate:\n",
    "    #default\n",
    "    prompt = ChatPromptTemplate.from_template(simple_rag_template)\n",
    "    if type == prompt_type[\"QA_RAG\"]: \n",
    "        prompt = ChatPromptTemplate.from_template(simple_rag_template)\n",
    "    if type == prompt_type[\"MM_QA_RAG\"]: \n",
    "        prompt = ChatPromptTemplate.from_template(multimodal_rag_template)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now send the augmented prompt to instruct a LLM generating response to user's query. The response is finally parsed for readable. \n",
    "In this experiment, we use OpenAI model GPT3.5-Turbo. \n",
    "\n",
    "Note: There are many options for LLMs selection, from public to private, from simple to advance. Privacy, performance and quality should be considered to trade off. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. QA Generation \n",
    "Using LLM to generation response to augmented query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG chain\n",
    "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain_multimodal_rag.invoke(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided data, here's a comparison of the performance of GPT-4 and Mixtral-8x7B:\\n\\n1. **Accuracy**:\\n   - **Retrieved Chunk**: GPT-4 shows significantly higher accuracy compared to Mixtral-8x7B across all query types (null, comparison, temporal, inference).\\n   - **Ground-truth Chunk**: GPT-4 also outperforms Mixtral-8x7B, especially in inference and comparison queries.\\n\\n2. **Performance Metrics**:\\n   - GPT-4 consistently achieves higher accuracy percentages in both retrieved and ground-truth chunks, indicating better retrieval and reasoning capabilities.\\n\\n3. **Investment Advice**:\\n   - **GPT-4**: Given its superior performance in accuracy and handling complex multi-hop queries, investing in technologies or applications leveraging GPT-4 could be more promising.\\n   - **Mixtral-8x7B**: While it shows some capability, its lower performance suggests it may be less competitive in applications requiring high accuracy and complex reasoning.\\n\\nOverall, GPT-4 demonstrates a clear advantage in performance, making it a more reliable choice for applications requiring advanced language processing and retrieval-augmented generation.\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Retrieve Topic and Relevant Articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_catalog = pd.DataFrame()\n",
    "with open('document_catalog.pickle', 'rb') as pkl_file:\n",
    "        doc_catalog = pickle.load(pkl_file)\n",
    "articles = list(set([a.metadata[\"paper_id\"] for a in relevant_docs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_catalog[\"topic\"] = [\"a\",\"b\",\"c\",\"c\"]\n",
    "doc_catalog[\"summary\"] = [\"1111\",\"222\",\"333\",\"3333\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = list(set([doc_catalog[\"topic\"].loc[i] for i, docid in enumerate(list(doc_catalog[\"docid\"])) if docid in articles]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_articles = [doc_catalog[\"filename\"].loc[i][:-4] for i, docid, topic in zip(range(len(doc_catalog.index)),list(doc_catalog[\"docid\"]),list(doc_catalog[\"topic\"])) if docid not in articles and topic in topics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Retrieve Article Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_summary = []\n",
    "topic_articles_summary = []\n",
    "if articles:\n",
    "    articles_summary = [doc_catalog[\"summary\"].loc[i] for i, docid in enumerate(list(doc_catalog[\"docid\"])) if docid in articles]\n",
    "if topic_articles:\n",
    "    topic_articles_summary = [doc_catalog[\"summary\"].loc[i] for i, filename in enumerate(list(doc_catalog[\"filename\"])) if filename[:-4] in topic_articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Generate the final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Query: What is the pperformance of GPT-4 vs Mixtral?\n",
      "The answer: Based on the provided data, here's a comparison of the performance of GPT-4 and Mixtral-8x7B:\n",
      "\n",
      "1. **Accuracy**:\n",
      "   - **Retrieved Chunk**: GPT-4 shows significantly higher accuracy compared to Mixtral-8x7B across all query types (null, comparison, temporal, inference).\n",
      "   - **Ground-truth Chunk**: GPT-4 also outperforms Mixtral-8x7B, especially in inference and comparison queries.\n",
      "\n",
      "2. **Performance Metrics**:\n",
      "   - GPT-4 consistently achieves higher accuracy percentages in both retrieved and ground-truth chunks, indicating better retrieval and reasoning capabilities.\n",
      "\n",
      "3. **Investment Advice**:\n",
      "   - **GPT-4**: Given its superior performance in accuracy and handling complex multi-hop queries, investing in technologies or applications leveraging GPT-4 could be more promising.\n",
      "   - **Mixtral-8x7B**: While it shows some capability, its lower performance suggests it may be less competitive in applications requiring high accuracy and complex reasoning.\n",
      "\n",
      "Overall, GPT-4 demonstrates a clear advantage in performance, making it a more reliable choice for applications requiring advanced language processing and retrieval-augmented generation.\n",
      "\n",
      "You can find the details of the answer from the following articles\n",
      "\n",
      "Article 1: 2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries\n",
      "Article Summary:\n",
      "1111\n",
      "\n",
      "You seem interested in the topics: b \n",
      "You may be interested in other articles in those topics below:\n",
      "\n",
      "Article 1: 2407.21059v1.Modular_RAG__Transforming_RAG_Systems_into_LEGO_like_Reconfigurable_Frameworks\n",
      "Article Summary:\n",
      "222\n"
     ]
    }
   ],
   "source": [
    "print(\"Your Query:\", user_query)\n",
    "print(\"The answer:\", response)\n",
    "if articles:\n",
    "    print(\"\\nYou can find the details of the answer from the following articles\")\n",
    "    for i in range(len(articles)):\n",
    "        print(\"\\nArticle \"+str(i+1)+\": \"+ doc_catalog[doc_catalog[\"docid\"]==articles[i]][\"filename\"].loc[0][:-4])\n",
    "        print(\"Article Summary:\\n\"+articles_summary[i])\n",
    "if topic_articles:\n",
    "    print(\"\\nYou seem interested in the topics:\", \", \".join(topics),\"\\nYou may be interested in other articles in those topics below:\")\n",
    "    for i in range(len(topic_articles)):\n",
    "        print(\"\\nArticle \"+str(i+1)+\": \"+topic_articles[i])\n",
    "        print(\"Article Summary:\\n\"+topic_articles_summary[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Research Assistant Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration of Research Assistant for: \n",
    "- Answer queries\n",
    "- Relevant papers: from the query and from the topic\n",
    "- Summary of the recommanded papers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

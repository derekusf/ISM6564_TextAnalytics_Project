{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install neccessary Library\n",
    "The libraries include:\n",
    "- langchain framework'\n",
    "- GPT4ALL, OpenAI and HuggingFace for various embedding methods and LLMs\n",
    "- Document loaders\n",
    "- Dependent libraries\n",
    "\n",
    "__Note__ : \n",
    "- It requires C++ builder for building a dependant library for Chroma. Check out https://github.com/bycloudai/InstallVSBuildToolsWindows for instruction. \n",
    "- Python version: 3.12.4\n",
    "- Pydantic version: 2.7.3. There is issue with pydantic version 1.10.8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git (from -r requirements.txt (line 25))Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Cloning https://github.com/openai/whisper.git to c:\\users\\pavan teja\\appdata\\local\\temp\\pip-req-build-ndpjzro8\n",
      "  Resolved https://github.com/openai/whisper.git to commit 5979f03701209bb035a0a466f14131aeb1116cbb\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: pydantic in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from -r requirements.txt (line 1)) (2.9.2)\n",
      "Requirement already satisfied: openai in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from -r requirements.txt (line 2)) (1.53.0)\n",
      "Requirement already satisfied: langchain in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from -r requirements.txt (line 3)) (0.3.7)\n",
      "Requirement already satisfied: langchain_community in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from -r requirements.txt (line 4)) (0.3.5)\n",
      "Requirement already satisfied: langchain-openai in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from -r requirements.txt (line 5)) (0.2.5)\n",
      "Requirement already satisfied: langchain-ollama in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from -r requirements.txt (line 6)) (0.2.0)\n",
      "Requirement already satisfied: langchainhub in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from -r requirements.txt (line 7)) (0.1.21)\n",
      "Requirement already satisfied: langchain-chroma in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from -r requirements.txt (line 8)) (0.1.4)\n",
      "Requirement already satisfied: docarray in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from -r requirements.txt (line 9)) (0.40.0)\n",
      "Requirement already satisfied: pytube in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from -r requirements.txt (line 11)) (15.0.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from -r requirements.txt (line 12)) (1.0.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from -r requirements.txt (line 13)) (0.8.0)\n",
      "Requirement already satisfied: ruff in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from -r requirements.txt (line 14)) (0.7.2)\n",
      "Requirement already satisfied: pymupdf in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from -r requirements.txt (line 15)) (1.24.13)\n",
      "Requirement already satisfied: bs4 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from -r requirements.txt (line 16)) (0.0.2)\n",
      "Requirement already satisfied: arxiv in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from -r requirements.txt (line 17)) (2.1.3)\n",
      "Requirement already satisfied: ragas in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from -r requirements.txt (line 18)) (0.2.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from -r requirements.txt (line 19)) (4.66.6)\n",
      "Requirement already satisfied: gpt4all in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from -r requirements.txt (line 20)) (2.8.2)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from -r requirements.txt (line 21)) (0.26.2)\n",
      "Requirement already satisfied: langchain_huggingface in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from -r requirements.txt (line 22)) (0.1.2)\n",
      "Requirement already satisfied: pypdf in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from -r requirements.txt (line 23)) (5.1.0)\n",
      "Requirement already satisfied: rapidocr-onnxruntime in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from -r requirements.txt (line 24)) (1.3.25)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from pydantic->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from pydantic->-r requirements.txt (line 1)) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from pydantic->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain->-r requirements.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain->-r requirements.txt (line 3)) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain->-r requirements.txt (line 3)) (3.10.10)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain->-r requirements.txt (line 3)) (0.3.15)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain->-r requirements.txt (line 3)) (0.3.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain->-r requirements.txt (line 3)) (0.1.139)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain->-r requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain->-r requirements.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain->-r requirements.txt (line 3)) (9.0.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain_community->-r requirements.txt (line 4)) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain_community->-r requirements.txt (line 4)) (0.4.0)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain_community->-r requirements.txt (line 4)) (2.6.1)\n",
      "Requirement already satisfied: ollama<1,>=0.3.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain-ollama->-r requirements.txt (line 6)) (0.3.3)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchainhub->-r requirements.txt (line 7)) (24.1)\n",
      "Requirement already satisfied: types-requests<3.0.0.0,>=2.31.0.2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchainhub->-r requirements.txt (line 7)) (2.32.0.20241016)\n",
      "Requirement already satisfied: chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain-chroma->-r requirements.txt (line 8)) (0.5.17)\n",
      "Requirement already satisfied: fastapi<1,>=0.95.2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain-chroma->-r requirements.txt (line 8)) (0.115.4)\n",
      "Requirement already satisfied: orjson>=3.8.2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from docarray->-r requirements.txt (line 9)) (3.10.11)\n",
      "Requirement already satisfied: rich>=13.1.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from docarray->-r requirements.txt (line 9)) (13.9.4)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from docarray->-r requirements.txt (line 9)) (0.9.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from tiktoken->-r requirements.txt (line 13)) (2024.9.11)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from bs4->-r requirements.txt (line 16)) (4.12.3)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from arxiv->-r requirements.txt (line 17)) (6.0.11)\n",
      "Requirement already satisfied: datasets in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from ragas->-r requirements.txt (line 18)) (3.1.0)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from ragas->-r requirements.txt (line 18)) (1.6.0)\n",
      "Requirement already satisfied: appdirs in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from ragas->-r requirements.txt (line 18)) (1.4.4)\n",
      "Requirement already satisfied: pysbd>=0.3.4 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from ragas->-r requirements.txt (line 18)) (0.3.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from tqdm->-r requirements.txt (line 19)) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from huggingface_hub->-r requirements.txt (line 21)) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from huggingface_hub->-r requirements.txt (line 21)) (2024.9.0)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain_huggingface->-r requirements.txt (line 22)) (3.2.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain_huggingface->-r requirements.txt (line 22)) (0.20.1)\n",
      "Requirement already satisfied: transformers>=4.39.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain_huggingface->-r requirements.txt (line 22)) (4.46.1)\n",
      "Requirement already satisfied: pyclipper>=1.2.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from rapidocr-onnxruntime->-r requirements.txt (line 24)) (1.3.0.post6)\n",
      "Requirement already satisfied: opencv-python>=4.5.1.48 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from rapidocr-onnxruntime->-r requirements.txt (line 24)) (4.10.0.84)\n",
      "Requirement already satisfied: six>=1.15.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from rapidocr-onnxruntime->-r requirements.txt (line 24)) (1.16.0)\n",
      "Requirement already satisfied: Shapely!=2.0.4,>=1.7.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from rapidocr-onnxruntime->-r requirements.txt (line 24)) (2.0.6)\n",
      "Requirement already satisfied: Pillow in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from rapidocr-onnxruntime->-r requirements.txt (line 24)) (11.0.0)\n",
      "Requirement already satisfied: onnxruntime>=1.7.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from rapidocr-onnxruntime->-r requirements.txt (line 24)) (1.20.0)\n",
      "Requirement already satisfied: numba in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from openai-whisper==20240930->-r requirements.txt (line 25)) (0.60.0)\n",
      "Requirement already satisfied: torch in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from openai-whisper==20240930->-r requirements.txt (line 25)) (2.5.1)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from openai-whisper==20240930->-r requirements.txt (line 25)) (10.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3)) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3)) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3)) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3)) (1.17.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (1.2.2.post1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (0.7.6)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (0.32.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (3.7.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (1.27.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (1.67.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (0.12.5)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (31.0.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (5.0.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community->-r requirements.txt (line 4)) (3.23.1)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from fastapi<1,>=0.95.2->langchain-chroma->-r requirements.txt (line 8)) (0.41.2)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from feedparser~=6.0.10->arxiv->-r requirements.txt (line 17)) (1.0.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (0.14.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain->-r requirements.txt (line 3)) (1.33)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 3)) (1.0.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime->-r requirements.txt (line 24)) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime->-r requirements.txt (line 24)) (24.3.25)\n",
      "Requirement already satisfied: protobuf in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime->-r requirements.txt (line 24)) (4.25.5)\n",
      "Requirement already satisfied: sympy in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime->-r requirements.txt (line 24)) (1.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 3)) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 3)) (2.2.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from rich>=13.1.0->docarray->-r requirements.txt (line 9)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from rich>=13.1.0->docarray->-r requirements.txt (line 9)) (2.18.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface->-r requirements.txt (line 22)) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface->-r requirements.txt (line 22)) (1.14.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 3)) (3.1.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from torch->openai-whisper==20240930->-r requirements.txt (line 25)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from torch->openai-whisper==20240930->-r requirements.txt (line 25)) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from sympy->onnxruntime>=1.7.0->rapidocr-onnxruntime->-r requirements.txt (line 24)) (1.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface->-r requirements.txt (line 22)) (0.4.5)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from typing-inspect>=0.8.0->docarray->-r requirements.txt (line 9)) (1.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from beautifulsoup4->bs4->-r requirements.txt (line 16)) (2.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from datasets->ragas->-r requirements.txt (line 18)) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from datasets->ragas->-r requirements.txt (line 18)) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from datasets->ragas->-r requirements.txt (line 18)) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from datasets->ragas->-r requirements.txt (line 18)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from datasets->ragas->-r requirements.txt (line 18)) (0.70.16)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from numba->openai-whisper==20240930->-r requirements.txt (line 25)) (0.43.0)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (1.2.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain->-r requirements.txt (line 3)) (3.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (2.35.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (3.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (0.9)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray->-r requirements.txt (line 9)) (0.1.2)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (8.4.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (1.65.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.27.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.48b0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.48b0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.48b0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (0.48b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (65.5.0)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (1.16.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (2.2.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (0.6.4)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (0.24.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (13.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from coloredlogs->onnxruntime>=1.7.0->rapidocr-onnxruntime->-r requirements.txt (line 24)) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from jinja2->torch->openai-whisper==20240930->-r requirements.txt (line 25)) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from pandas->datasets->ragas->-r requirements.txt (line 18)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from pandas->datasets->ragas->-r requirements.txt (line 18)) (2024.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface->-r requirements.txt (line 22)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface->-r requirements.txt (line 22)) (3.5.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (4.9)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.7.0->rapidocr-onnxruntime->-r requirements.txt (line 24)) (3.5.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (3.20.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (0.6.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git 'C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-req-build-ndpjzro8'\n",
      "WARNING: langchain 0.3.7 does not provide the extra 'docarray'\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-experimental\n",
      "  Downloading langchain_experimental-0.3.3-py3-none-any.whl (208 kB)\n",
      "                                              0.0/209.0 kB ? eta -:--:--\n",
      "     -------------------------------------  204.8/209.0 kB 6.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 209.0/209.0 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain-experimental) (0.3.5)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain-experimental) (0.3.15)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<2.0.36,>=1.4 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.10.10)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.0)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.6 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.7)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.1.139)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.26.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-experimental) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-experimental) (24.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-experimental) (2.9.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-experimental) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.17.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-experimental) (3.0.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langchain<0.4.0,>=0.3.6->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-experimental) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.15->langchain-experimental) (2.23.4)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from SQLAlchemy<2.0.36,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.14.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.2.0)\n",
      "Installing collected packages: langchain-experimental\n",
      "Successfully installed langchain-experimental-0.3.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unstructured[all-docs]\n",
      "  Downloading unstructured-0.16.4-py3-none-any.whl (1.7 MB)\n",
      "                                              0.0/1.7 MB ? eta -:--:--\n",
      "     ---                                      0.2/1.7 MB 3.5 MB/s eta 0:00:01\n",
      "     ------------                             0.6/1.7 MB 5.8 MB/s eta 0:00:01\n",
      "     ------------------------------------     1.6/1.7 MB 11.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.7/1.7 MB 11.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pillow in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (11.0.0)\n",
      "Requirement already satisfied: pydantic in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (2.9.2)\n",
      "Collecting lxml\n",
      "  Downloading lxml-5.3.0-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "                                              0.0/3.8 MB ? eta -:--:--\n",
      "     ---------------                          1.5/3.8 MB 46.9 MB/s eta 0:00:01\n",
      "     ---------------------------              2.6/3.8 MB 33.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------  3.8/3.8 MB 34.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.8/3.8 MB 27.2 MB/s eta 0:00:00\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.2-cp311-cp311-win_amd64.whl (7.8 MB)\n",
      "                                              0.0/7.8 MB ? eta -:--:--\n",
      "     ------                                   1.2/7.8 MB 26.4 MB/s eta 0:00:01\n",
      "     ----------------                         3.3/7.8 MB 35.2 MB/s eta 0:00:01\n",
      "     -----------------------                  4.5/7.8 MB 36.0 MB/s eta 0:00:01\n",
      "     -----------------------                  4.5/7.8 MB 36.0 MB/s eta 0:00:01\n",
      "     -----------------------                  4.6/7.8 MB 21.1 MB/s eta 0:00:01\n",
      "     -----------------------------------      6.9/7.8 MB 27.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------  7.7/7.8 MB 24.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  7.8/7.8 MB 23.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 7.8/7.8 MB 20.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: chromadb in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (0.5.17)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (0.8.0)\n",
      "Collecting chardet (from unstructured[all-docs])\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "                                              0.0/199.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 199.4/199.4 kB ? eta 0:00:00\n",
      "Collecting filetype (from unstructured[all-docs])\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting python-magic (from unstructured[all-docs])\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Collecting nltk (from unstructured[all-docs])\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "                                              0.0/1.5 MB ? eta -:--:--\n",
      "     -----------------                        0.7/1.5 MB 21.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  1.5/1.5 MB 19.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 15.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from unstructured[all-docs]) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from unstructured[all-docs]) (4.12.3)\n",
      "Collecting emoji (from unstructured[all-docs])\n",
      "  Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
      "                                              0.0/586.9 kB ? eta -:--:--\n",
      "     ------------------------------------- 586.9/586.9 kB 38.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from unstructured[all-docs]) (0.6.7)\n",
      "Collecting python-iso639 (from unstructured[all-docs])\n",
      "  Downloading python_iso639-2024.10.22-py3-none-any.whl (274 kB)\n",
      "                                              0.0/274.9 kB ? eta -:--:--\n",
      "     ------------------------------------- 274.9/274.9 kB 16.5 MB/s eta 0:00:00\n",
      "Collecting langdetect (from unstructured[all-docs])\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from unstructured[all-docs]) (1.26.4)\n",
      "Collecting rapidfuzz (from unstructured[all-docs])\n",
      "  Downloading rapidfuzz-3.10.1-cp311-cp311-win_amd64.whl (1.6 MB)\n",
      "                                              0.0/1.6 MB ? eta -:--:--\n",
      "     ---------------------------------        1.3/1.6 MB 42.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.6/1.6 MB 25.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: backoff in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from unstructured[all-docs]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from unstructured[all-docs]) (4.12.2)\n",
      "Collecting unstructured-client (from unstructured[all-docs])\n",
      "  Downloading unstructured_client-0.26.2-py3-none-any.whl (59 kB)\n",
      "                                              0.0/60.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.0/60.0 kB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wrapt in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from unstructured[all-docs]) (1.16.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from unstructured[all-docs]) (4.66.6)\n",
      "Requirement already satisfied: psutil in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from unstructured[all-docs]) (6.1.0)\n",
      "Collecting python-oxmsg (from unstructured[all-docs])\n",
      "  Downloading python_oxmsg-0.0.1-py3-none-any.whl (31 kB)\n",
      "Collecting html5lib (from unstructured[all-docs])\n",
      "  Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "                                              0.0/112.2 kB ? eta -:--:--\n",
      "     -------------------------------------- 112.2/112.2 kB 3.3 MB/s eta 0:00:00\n",
      "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[all-docs])\n",
      "  Downloading unstructured.pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Collecting openpyxl (from unstructured[all-docs])\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "                                              0.0/250.9 kB ? eta -:--:--\n",
      "     -------------------------------------- 250.9/250.9 kB 5.1 MB/s eta 0:00:00\n",
      "Collecting pdf2image (from unstructured[all-docs])\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Collecting markdown (from unstructured[all-docs])\n",
      "  Downloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "                                              0.0/106.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 106.3/106.3 kB ? eta 0:00:00\n",
      "Requirement already satisfied: networkx in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from unstructured[all-docs]) (3.4.2)\n",
      "Collecting pi-heif (from unstructured[all-docs])\n",
      "  Downloading pi_heif-0.20.0-cp311-cp311-win_amd64.whl (1.7 MB)\n",
      "                                              0.0/1.7 MB ? eta -:--:--\n",
      "     ---------------------------------------  1.7/1.7 MB 51.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.7/1.7 MB 26.3 MB/s eta 0:00:00\n",
      "Collecting effdet (from unstructured[all-docs])\n",
      "  Downloading effdet-0.4.1-py3-none-any.whl (112 kB)\n",
      "                                              0.0/112.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 112.5/112.5 kB 6.4 MB/s eta 0:00:00\n",
      "Collecting python-docx>=1.1.2 (from unstructured[all-docs])\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "                                              0.0/244.3 kB ? eta -:--:--\n",
      "     ------------------------------------- 244.3/244.3 kB 15.6 MB/s eta 0:00:00\n",
      "Collecting unstructured-inference==0.8.1 (from unstructured[all-docs])\n",
      "  Downloading unstructured_inference-0.8.1-py3-none-any.whl (48 kB)\n",
      "                                              0.0/48.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 48.4/48.4 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting python-pptx>=1.0.1 (from unstructured[all-docs])\n",
      "  Downloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
      "                                              0.0/472.8 kB ? eta -:--:--\n",
      "     ------------------------------------- 472.8/472.8 kB 30.8 MB/s eta 0:00:00\n",
      "Collecting pikepdf (from unstructured[all-docs])\n",
      "  Downloading pikepdf-9.4.0-cp311-cp311-win_amd64.whl (3.5 MB)\n",
      "                                              0.0/3.5 MB ? eta -:--:--\n",
      "     ----------------------                   2.0/3.5 MB 30.9 MB/s eta 0:00:01\n",
      "     -------------------------------          2.7/3.5 MB 24.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  3.5/3.5 MB 27.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.5/3.5 MB 20.1 MB/s eta 0:00:00\n",
      "Collecting pypandoc (from unstructured[all-docs])\n",
      "  Downloading pypandoc-1.14-py3-none-any.whl (21 kB)\n",
      "Collecting onnx (from unstructured[all-docs])\n",
      "  Downloading onnx-1.17.0-cp311-cp311-win_amd64.whl (14.5 MB)\n",
      "                                              0.0/14.5 MB ? eta -:--:--\n",
      "     --                                       1.0/14.5 MB 31.0 MB/s eta 0:00:01\n",
      "     ------                                   2.5/14.5 MB 26.9 MB/s eta 0:00:01\n",
      "     ----------                               3.7/14.5 MB 29.5 MB/s eta 0:00:01\n",
      "     --------------                           5.1/14.5 MB 29.7 MB/s eta 0:00:01\n",
      "     -----------------                        6.5/14.5 MB 29.6 MB/s eta 0:00:01\n",
      "     ---------------------                    7.8/14.5 MB 31.3 MB/s eta 0:00:01\n",
      "     -------------------------                9.2/14.5 MB 31.0 MB/s eta 0:00:01\n",
      "     ----------------------------            10.6/14.5 MB 31.2 MB/s eta 0:00:01\n",
      "     ---------------------------------       12.6/14.5 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  14.4/14.5 MB 32.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  14.5/14.5 MB 32.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 14.5/14.5 MB 25.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pypdf in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from unstructured[all-docs]) (5.1.0)\n",
      "Collecting google-cloud-vision (from unstructured[all-docs])\n",
      "  Downloading google_cloud_vision-3.8.0-py2.py3-none-any.whl (488 kB)\n",
      "                                              0.0/488.5 kB ? eta -:--:--\n",
      "     ------------------------------------- 488.5/488.5 kB 29.9 MB/s eta 0:00:00\n",
      "Collecting xlrd (from unstructured[all-docs])\n",
      "  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "                                              0.0/96.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 96.5/96.5 kB 5.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from unstructured[all-docs]) (2.2.3)\n",
      "Collecting pdfminer.six (from unstructured[all-docs])\n",
      "  Downloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
      "                                              0.0/5.6 MB ? eta -:--:--\n",
      "     ----------                               1.5/5.6 MB 30.9 MB/s eta 0:00:01\n",
      "     --------------------                     2.8/5.6 MB 30.0 MB/s eta 0:00:01\n",
      "     ----------------------------             4.0/5.6 MB 28.5 MB/s eta 0:00:01\n",
      "     --------------------------------------   5.3/5.6 MB 26.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.6/5.6 MB 27.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 5.6/5.6 MB 22.4 MB/s eta 0:00:00\n",
      "Collecting layoutparser (from unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Downloading layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
      "                                              0.0/19.2 MB ? eta -:--:--\n",
      "     --                                       1.4/19.2 MB 44.3 MB/s eta 0:00:01\n",
      "     ------                                   3.0/19.2 MB 38.1 MB/s eta 0:00:01\n",
      "     ----------                               4.9/19.2 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------                           6.8/19.2 MB 36.3 MB/s eta 0:00:01\n",
      "     -----------------                        8.4/19.2 MB 33.8 MB/s eta 0:00:01\n",
      "     --------------------                     9.9/19.2 MB 35.2 MB/s eta 0:00:01\n",
      "     ----------------------                  11.0/19.2 MB 31.1 MB/s eta 0:00:01\n",
      "     ---------------------------             13.3/19.2 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------------------------          15.0/19.2 MB 32.7 MB/s eta 0:00:01\n",
      "     ----------------------------------      16.9/19.2 MB 32.7 MB/s eta 0:00:01\n",
      "     -------------------------------------   18.4/19.2 MB 32.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  19.2/19.2 MB 32.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  19.2/19.2 MB 32.7 MB/s eta 0:00:01\n",
      "     --------------------------------------- 19.2/19.2 MB 24.2 MB/s eta 0:00:00\n",
      "Collecting python-multipart (from unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Downloading python_multipart-0.0.17-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (0.26.2)\n",
      "Requirement already satisfied: opencv-python!=4.7.0.68 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (4.10.0.84)\n",
      "Requirement already satisfied: onnxruntime>=1.17.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (1.20.0)\n",
      "Requirement already satisfied: torch in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (2.5.1)\n",
      "Collecting timm (from unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Downloading timm-1.0.11-py3-none-any.whl (2.3 MB)\n",
      "                                              0.0/2.3 MB ? eta -:--:--\n",
      "     ---------------------------------        2.0/2.3 MB 41.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.3/2.3 MB 29.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: transformers>=4.25.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (4.46.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from pydantic) (2.23.4)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.0-cp311-cp311-win_amd64.whl (217 kB)\n",
      "                                              0.0/217.2 kB ? eta -:--:--\n",
      "     ------------------------------------- 217.2/217.2 kB 13.8 MB/s eta 0:00:00\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.54.1-cp311-cp311-win_amd64.whl (2.2 MB)\n",
      "                                              0.0/2.2 MB ? eta -:--:--\n",
      "     --------------------                     1.1/2.2 MB 34.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.2/2.2 MB 34.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.2/2.2 MB 27.9 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.7-cp311-cp311-win_amd64.whl (56 kB)\n",
      "                                              0.0/56.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 56.0/56.0 kB 2.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "                                              0.0/106.9 kB ? eta -:--:--\n",
      "     -------------------------------------- 106.9/106.9 kB 6.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb) (0.115.4)\n",
      "Requirement already satisfied: uvicorn[standard]>=0.18.3 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb) (0.32.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb) (3.7.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb) (1.27.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb) (0.20.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb) (1.67.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb) (0.12.5)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb) (31.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb) (5.0.1)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb) (3.10.11)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb) (0.27.2)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.41.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.6)\n",
      "Requirement already satisfied: idna in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: sniffio in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.35.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.3)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (24.3.25)\n",
      "Requirement already satisfied: protobuf in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (4.25.5)\n",
      "Requirement already satisfied: sympy in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.4.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.65.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.27.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.27.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.48b0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.48b0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.48b0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.48b0)\n",
      "Requirement already satisfied: setuptools>=16.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (65.5.0)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Collecting XlsxWriter>=0.5.7 (from python-pptx>=1.0.1->unstructured[all-docs])\n",
      "  Downloading XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
      "                                              0.0/159.9 kB ? eta -:--:--\n",
      "     ------------------------------------- 159.9/159.9 kB 10.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from requests->unstructured[all-docs]) (3.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.24.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (13.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from beautifulsoup4->unstructured[all-docs]) (2.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from dataclasses-json->unstructured[all-docs]) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from dataclasses-json->unstructured[all-docs]) (0.9.0)\n",
      "Collecting torchvision (from effdet->unstructured[all-docs])\n",
      "  Downloading torchvision-0.20.1-cp311-cp311-win_amd64.whl (1.6 MB)\n",
      "                                              0.0/1.6 MB ? eta -:--:--\n",
      "     ---------------------------------------  1.6/1.6 MB 49.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.6/1.6 MB 25.0 MB/s eta 0:00:00\n",
      "Collecting pycocotools>=2.0.2 (from effdet->unstructured[all-docs])\n",
      "  Downloading pycocotools-2.0.8-cp311-cp311-win_amd64.whl (85 kB)\n",
      "                                              0.0/85.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 85.3/85.3 kB 5.0 MB/s eta 0:00:00\n",
      "Collecting omegaconf>=2.0 (from effdet->unstructured[all-docs])\n",
      "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "                                              0.0/79.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 79.5/79.5 kB 4.6 MB/s eta 0:00:00\n",
      "Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 (from google-cloud-vision->unstructured[all-docs])\n",
      "  Downloading google_api_core-2.22.0-py3-none-any.whl (156 kB)\n",
      "                                              0.0/156.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 156.5/156.5 kB 9.1 MB/s eta 0:00:00\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-cloud-vision->unstructured[all-docs])\n",
      "  Downloading proto_plus-1.25.0-py3-none-any.whl (50 kB)\n",
      "                                              0.0/50.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.1/50.1 kB 2.5 MB/s eta 0:00:00\n",
      "Collecting webencodings (from html5lib->unstructured[all-docs])\n",
      "  Downloading webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from nltk->unstructured[all-docs]) (1.4.2)\n",
      "Collecting et-xmlfile (from openpyxl->unstructured[all-docs])\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from pandas->unstructured[all-docs]) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from pandas->unstructured[all-docs]) (2024.2)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six->unstructured[all-docs])\n",
      "  Downloading cryptography-43.0.3-cp39-abi3-win_amd64.whl (3.1 MB)\n",
      "                                              0.0/3.1 MB ? eta -:--:--\n",
      "     ---------------------------              2.1/3.1 MB 44.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 3.1/3.1 MB 38.7 MB/s eta 0:00:00\n",
      "Collecting olefile (from python-oxmsg->unstructured[all-docs])\n",
      "  Downloading olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
      "                                              0.0/114.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 114.6/114.6 kB ? eta 0:00:00\n",
      "Collecting eval-type-backport<0.3.0,>=0.2.0 (from unstructured-client->unstructured[all-docs])\n",
      "  Downloading eval_type_backport-0.2.0-py3-none-any.whl (5.9 kB)\n",
      "Collecting jsonpath-python<2.0.0,>=1.0.6 (from unstructured-client->unstructured[all-docs])\n",
      "  Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (1.6.0)\n",
      "Collecting python-dateutil>=2.7 (from matplotlib)\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "                                              0.0/247.7 kB ? eta -:--:--\n",
      "     ------------------------------------- 247.7/247.7 kB 15.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (1.0.0)\n",
      "Collecting cffi>=1.12 (from cryptography>=36.0.0->pdfminer.six->unstructured[all-docs])\n",
      "  Downloading cffi-1.17.1-cp311-cp311-win_amd64.whl (181 kB)\n",
      "                                              0.0/181.4 kB ? eta -:--:--\n",
      "     ------------------------------------- 181.4/181.4 kB 10.7 MB/s eta 0:00:00\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs])\n",
      "  Downloading grpcio_status-1.67.1-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from huggingface-hub->unstructured-inference==0.8.1->unstructured[all-docs]) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from huggingface-hub->unstructured-inference==0.8.1->unstructured[all-docs]) (2024.9.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->unstructured[all-docs])\n",
      "  Using cached antlr4_python3_runtime-4.9.3-py3-none-any.whl\n",
      "Requirement already satisfied: safetensors in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from timm->unstructured-inference==0.8.1->unstructured[all-docs]) (0.4.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from sympy->onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[all-docs]) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (10.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from layoutparser->unstructured-inference==0.8.1->unstructured[all-docs]) (1.14.1)\n",
      "Collecting iopath (from layoutparser->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Using cached iopath-0.1.10-py3-none-any.whl\n",
      "Collecting pdfplumber (from layoutparser->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
      "                                              0.0/59.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 59.2/59.2 kB ? eta 0:00:00\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[all-docs])\n",
      "  Downloading pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "                                              0.0/117.6 kB ? eta -:--:--\n",
      "     -------------------------------------- 117.6/117.6 kB 6.7 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs])\n",
      "  Downloading grpcio_status-1.67.0-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.66.2-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.66.1-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.66.0-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.65.5-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.65.4-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.65.2-py3-none-any.whl (14 kB)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.65.1-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.64.3-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.64.1-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.64.0-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.63.2-py3-none-any.whl (14 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading grpcio_status-1.63.0-py3-none-any.whl (14 kB)\n",
      "  Downloading grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Collecting portalocker (from iopath->layoutparser->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Downloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from jinja2->torch->unstructured-inference==0.8.1->unstructured[all-docs]) (3.0.2)\n",
      "Collecting pdfminer.six (from unstructured[all-docs])\n",
      "  Downloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "                                              0.0/5.6 MB ? eta -:--:--\n",
      "     ---------                                1.3/5.6 MB 41.6 MB/s eta 0:00:01\n",
      "     -----------------------                  3.3/5.6 MB 42.3 MB/s eta 0:00:01\n",
      "     ------------------------------------     5.1/5.6 MB 40.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.6/5.6 MB 40.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.6/5.6 MB 40.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.6/5.6 MB 40.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 5.6/5.6 MB 18.9 MB/s eta 0:00:00\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Downloading pypdfium2-4.30.0-py3-none-win_amd64.whl (2.9 MB)\n",
      "                                              0.0/2.9 MB ? eta -:--:--\n",
      "     -------------------------------          2.3/2.9 MB 47.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.9/2.9 MB 45.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.9/2.9 MB 20.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from portalocker->iopath->layoutparser->unstructured-inference==0.8.1->unstructured[all-docs]) (308)\n",
      "Installing collected packages: webencodings, filetype, antlr4-python3-runtime, XlsxWriter, xlrd, unstructured.pytesseract, rapidfuzz, python-multipart, python-magic, python-iso639, python-dateutil, pypdfium2, pyparsing, pypandoc, pycparser, proto-plus, portalocker, pi-heif, pdf2image, onnx, omegaconf, olefile, markdown, lxml, langdetect, kiwisolver, jsonpath-python, html5lib, fonttools, eval-type-backport, et-xmlfile, emoji, cycler, contourpy, chardet, python-pptx, python-oxmsg, python-docx, pikepdf, openpyxl, nltk, matplotlib, iopath, grpcio-status, cffi, torchvision, pycocotools, google-api-core, cryptography, unstructured-client, timm, pdfminer.six, unstructured, pdfplumber, google-cloud-vision, effdet, layoutparser, unstructured-inference\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "Successfully installed XlsxWriter-3.2.0 antlr4-python3-runtime-4.9.3 cffi-1.17.1 chardet-5.2.0 contourpy-1.3.0 cryptography-43.0.3 cycler-0.12.1 effdet-0.4.1 emoji-2.14.0 et-xmlfile-2.0.0 eval-type-backport-0.2.0 filetype-1.2.0 fonttools-4.54.1 google-api-core-2.22.0 google-cloud-vision-3.8.0 grpcio-status-1.62.3 html5lib-1.1 iopath-0.1.10 jsonpath-python-1.0.6 kiwisolver-1.4.7 langdetect-1.0.9 layoutparser-0.3.4 lxml-5.3.0 markdown-3.7 matplotlib-3.9.2 nltk-3.9.1 olefile-0.47 omegaconf-2.3.0 onnx-1.17.0 openpyxl-3.1.5 pdf2image-1.17.0 pdfminer.six-20231228 pdfplumber-0.11.4 pi-heif-0.20.0 pikepdf-9.4.0 portalocker-2.10.1 proto-plus-1.25.0 pycocotools-2.0.8 pycparser-2.22 pypandoc-1.14 pyparsing-3.2.0 pypdfium2-4.30.0 python-dateutil-2.8.2 python-docx-1.1.2 python-iso639-2024.10.22 python-magic-0.4.27 python-multipart-0.0.17 python-oxmsg-0.0.1 python-pptx-1.0.2 rapidfuzz-3.10.1 timm-1.0.11 torchvision-0.20.1 unstructured-0.16.4 unstructured-client-0.26.2 unstructured-inference-0.8.1 unstructured.pytesseract-0.3.13 webencodings-0.5.1 xlrd-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install \"unstructured[all-docs]\" pillow pydantic lxml pillow matplotlib chromadb tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdf2image in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (1.17.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pillow in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from pdf2image) (11.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (3.9.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [55 lines of output]\n",
      "      \u001b[36m\u001b[1m+ meson setup --native-file=C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-install-ne6nx7mn\\scipy_971c465db688481aac47a85aeab36b3f\\.mesonpy-native-file.ini -Ddebug=false -Doptimization=2 --prefix=C:\\Users\\Pavan Teja\\anaconda3 C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-install-ne6nx7mn\\scipy_971c465db688481aac47a85aeab36b3f C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-install-ne6nx7mn\\scipy_971c465db688481aac47a85aeab36b3f\\.mesonpy-9p92y_ag\\build\u001b[0m\n",
      "      The Meson build system\n",
      "      Version: 0.62.2\n",
      "      Source dir: C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-install-ne6nx7mn\\scipy_971c465db688481aac47a85aeab36b3f\n",
      "      Build dir: C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-install-ne6nx7mn\\scipy_971c465db688481aac47a85aeab36b3f\\.mesonpy-9p92y_ag\\build\n",
      "      Build type: native build\n",
      "      Project name: SciPy\n",
      "      Project version: 1.9.1\n",
      "      WARNING: Failed to activate VS environment: Could not find C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe\n",
      "      \n",
      "      ..\\..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
      "      The following exception(s) were encountered:\n",
      "      Running \"icl \" gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running \"cl /?\" gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running \"cc --version\" gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running \"gcc --version\" gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running \"clang --version\" gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running \"clang-cl /?\" gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running \"pgcc --version\" gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      \n",
      "      A full log can be found at C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-install-ne6nx7mn\\scipy_971c465db688481aac47a85aeab36b3f\\.mesonpy-9p92y_ag\\build\\meson-logs\\meson-log.txt\n",
      "      Traceback (most recent call last):\n",
      "        File \"c:\\Users\\Pavan Teja\\Documents\\TextAnalyticsProject\\ISM6564_TextAnalytics_Project\\ism6564\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "        File \"c:\\Users\\Pavan Teja\\Documents\\TextAnalyticsProject\\ISM6564_TextAnalytics_Project\\ism6564\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"c:\\Users\\Pavan Teja\\Documents\\TextAnalyticsProject\\ISM6564_TextAnalytics_Project\\ism6564\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "          return hook(config_settings)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-build-env-f9t1txyt\\overlay\\Lib\\site-packages\\mesonpy\\__init__.py\", line 969, in get_requires_for_build_wheel\n",
      "          with _project(config_settings) as project:\n",
      "        File \"C:\\Users\\Pavan Teja\\anaconda3\\Lib\\contextlib.py\", line 137, in __enter__\n",
      "          return next(self.gen)\n",
      "                 ^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-build-env-f9t1txyt\\overlay\\Lib\\site-packages\\mesonpy\\__init__.py\", line 948, in _project\n",
      "          with Project.with_temp_working_dir(\n",
      "        File \"C:\\Users\\Pavan Teja\\anaconda3\\Lib\\contextlib.py\", line 137, in __enter__\n",
      "          return next(self.gen)\n",
      "                 ^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-build-env-f9t1txyt\\overlay\\Lib\\site-packages\\mesonpy\\__init__.py\", line 777, in with_temp_working_dir\n",
      "          yield cls(source_dir, tmpdir, build_dir)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-build-env-f9t1txyt\\overlay\\Lib\\site-packages\\mesonpy\\__init__.py\", line 682, in __init__\n",
      "          self._configure(reconfigure=bool(build_dir) and not native_file_mismatch)\n",
      "        File \"C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-build-env-f9t1txyt\\overlay\\Lib\\site-packages\\mesonpy\\__init__.py\", line 713, in _configure\n",
      "          self._meson(\n",
      "        File \"C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-build-env-f9t1txyt\\overlay\\Lib\\site-packages\\mesonpy\\__init__.py\", line 696, in _meson\n",
      "          return self._proc('meson', *args)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-build-env-f9t1txyt\\overlay\\Lib\\site-packages\\mesonpy\\__init__.py\", line 691, in _proc\n",
      "          subprocess.check_call(list(args))\n",
      "        File \"C:\\Users\\Pavan Teja\\anaconda3\\Lib\\subprocess.py\", line 413, in check_call\n",
      "          raise CalledProcessError(retcode, cmd)\n",
      "      subprocess.CalledProcessError: Command '['meson', 'setup', '--native-file=C:\\\\Users\\\\Pavan Teja\\\\AppData\\\\Local\\\\Temp\\\\pip-install-ne6nx7mn\\\\scipy_971c465db688481aac47a85aeab36b3f\\\\.mesonpy-native-file.ini', '-Ddebug=false', '-Doptimization=2', '--prefix=C:\\\\Users\\\\Pavan Teja\\\\anaconda3', 'C:\\\\Users\\\\Pavan Teja\\\\AppData\\\\Local\\\\Temp\\\\pip-install-ne6nx7mn\\\\scipy_971c465db688481aac47a85aeab36b3f', 'C:\\\\Users\\\\Pavan Teja\\\\AppData\\\\Local\\\\Temp\\\\pip-install-ne6nx7mn\\\\scipy_971c465db688481aac47a85aeab36b3f\\\\.mesonpy-9p92y_ag\\\\build']' returned non-zero exit status 1.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: openpyxl in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (3.9.2)\n",
      "Collecting textblob\n",
      "  Using cached textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.2-cp311-cp311-win_amd64.whl (12.2 MB)\n",
      "                                              0.0/12.2 MB ? eta -:--:--\n",
      "                                              0.2/12.2 MB 3.5 MB/s eta 0:00:04\n",
      "     -                                        0.6/12.2 MB 7.0 MB/s eta 0:00:02\n",
      "     -----                                    1.6/12.2 MB 11.0 MB/s eta 0:00:01\n",
      "     --------                                 2.7/12.2 MB 15.4 MB/s eta 0:00:01\n",
      "     -------------                            4.0/12.2 MB 18.4 MB/s eta 0:00:01\n",
      "     -----------------                        5.3/12.2 MB 20.0 MB/s eta 0:00:01\n",
      "     ---------------------                    6.7/12.2 MB 20.4 MB/s eta 0:00:01\n",
      "     -----------------------                  7.3/12.2 MB 20.3 MB/s eta 0:00:01\n",
      "     -----------------------------            8.9/12.2 MB 21.9 MB/s eta 0:00:01\n",
      "     ---------------------------------       10.4/12.2 MB 24.2 MB/s eta 0:00:01\n",
      "     -----------------------------------     11.2/12.2 MB 26.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.2/12.2 MB 27.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.2/12.2 MB 27.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.2/12.2 MB 27.3 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.2/12.2 MB 19.8 MB/s eta 0:00:00\n",
      "Collecting gensim\n",
      "  Using cached gensim-4.3.3-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: click in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from nltk) (4.66.6)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Using cached murmurhash-1.0.10-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Using cached cymem-2.0.8-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n",
      "  Downloading thinc-8.3.2-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "                                              0.0/1.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 1.5/1.5 MB 31.5 MB/s eta 0:00:00\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Using cached srsly-2.4.8-cp311-cp311-win_amd64.whl (479 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from spacy) (0.12.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.4.1-py3-none-any.whl (182 kB)\n",
      "                                              0.0/182.4 kB ? eta -:--:--\n",
      "     ------------------------------------- 182.4/182.4 kB 10.8 MB/s eta 0:00:00\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-win_amd64.whl (46.2 MB)\n",
      "                                              0.0/46.2 MB ? eta -:--:--\n",
      "     -                                        1.5/46.2 MB 48.6 MB/s eta 0:00:01\n",
      "     --                                       2.9/46.2 MB 30.8 MB/s eta 0:00:02\n",
      "     ----                                     4.8/46.2 MB 33.8 MB/s eta 0:00:02\n",
      "     -----                                    6.4/46.2 MB 34.1 MB/s eta 0:00:02\n",
      "     ------                                   7.3/46.2 MB 33.4 MB/s eta 0:00:02\n",
      "     --------                                10.0/46.2 MB 35.6 MB/s eta 0:00:02\n",
      "     ---------                               10.8/46.2 MB 32.8 MB/s eta 0:00:02\n",
      "     ----------                              12.7/46.2 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------                            14.3/46.2 MB 36.4 MB/s eta 0:00:01\n",
      "     -------------                           15.7/46.2 MB 34.4 MB/s eta 0:00:01\n",
      "     -------------                           15.7/46.2 MB 34.4 MB/s eta 0:00:01\n",
      "     -------------                           15.9/46.2 MB 27.3 MB/s eta 0:00:02\n",
      "     -------------                           16.5/46.2 MB 23.4 MB/s eta 0:00:02\n",
      "     ---------------                         18.5/46.2 MB 24.2 MB/s eta 0:00:02\n",
      "     -----------------                       20.5/46.2 MB 23.4 MB/s eta 0:00:02\n",
      "     --------------------                    24.6/46.2 MB 24.2 MB/s eta 0:00:01\n",
      "     ----------------------                  27.0/46.2 MB 38.6 MB/s eta 0:00:01\n",
      "     --------------------------              31.3/46.2 MB 40.9 MB/s eta 0:00:01\n",
      "     --------------------------              31.9/46.2 MB 36.4 MB/s eta 0:00:01\n",
      "     ----------------------------            33.5/46.2 MB 36.3 MB/s eta 0:00:01\n",
      "     -----------------------------           35.3/46.2 MB 38.5 MB/s eta 0:00:01\n",
      "     -------------------------------         37.1/46.2 MB 38.5 MB/s eta 0:00:01\n",
      "     ---------------------------------       39.2/46.2 MB 40.9 MB/s eta 0:00:01\n",
      "     ----------------------------------      41.4/46.2 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------------------    43.7/46.2 MB 38.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  45.9/46.2 MB 43.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 43.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 43.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 43.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 43.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 43.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 46.2/46.2 MB 19.8 MB/s eta 0:00:00\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "                                              0.0/61.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.4/61.4 kB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: wrapt in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Collecting blis<1.1.0,>=1.0.0 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Downloading blis-1.0.1-cp311-cp311-win_amd64.whl (6.3 MB)\n",
      "                                              0.0/6.3 MB ? eta -:--:--\n",
      "     ----------                               1.6/6.3 MB 34.9 MB/s eta 0:00:01\n",
      "     ---------------------                    3.4/6.3 MB 43.5 MB/s eta 0:00:01\n",
      "     ---------------------                    3.4/6.3 MB 43.5 MB/s eta 0:00:01\n",
      "     -------------------------------          5.0/6.3 MB 26.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  6.3/6.3 MB 27.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 6.3/6.3 MB 22.5 MB/s eta 0:00:00\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "INFO: pip is looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n",
      "  Downloading thinc-8.3.1-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "                                              0.0/1.5 MB ? eta -:--:--\n",
      "     ----------------------------------       1.3/1.5 MB 40.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 31.5 MB/s eta 0:00:00\n",
      "  Downloading thinc-8.3.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "                                              0.0/1.5 MB ? eta -:--:--\n",
      "     ---------------------------------------  1.5/1.5 MB 47.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 32.1 MB/s eta 0:00:00\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.0-cp311-cp311-win_amd64.whl (46.2 MB)\n",
      "                                              0.0/46.2 MB ? eta -:--:--\n",
      "     -                                        1.9/46.2 MB 40.7 MB/s eta 0:00:02\n",
      "     --                                       3.1/46.2 MB 39.6 MB/s eta 0:00:02\n",
      "     ----                                     5.2/46.2 MB 41.9 MB/s eta 0:00:01\n",
      "     ------                                   7.4/46.2 MB 39.4 MB/s eta 0:00:01\n",
      "     -------                                  8.4/46.2 MB 35.9 MB/s eta 0:00:02\n",
      "     --------                                10.0/46.2 MB 35.4 MB/s eta 0:00:02\n",
      "     ---------                               11.6/46.2 MB 34.4 MB/s eta 0:00:02\n",
      "     -----------                             13.4/46.2 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------                            15.0/46.2 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------                          16.9/46.2 MB 34.4 MB/s eta 0:00:01\n",
      "     ---------------                         18.4/46.2 MB 34.6 MB/s eta 0:00:01\n",
      "     -----------------                       20.5/46.2 MB 36.4 MB/s eta 0:00:01\n",
      "     -------------------                     23.1/46.2 MB 38.5 MB/s eta 0:00:01\n",
      "     ---------------------                   25.1/46.2 MB 38.6 MB/s eta 0:00:01\n",
      "     -----------------------                 27.4/46.2 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------                29.2/46.2 MB 38.5 MB/s eta 0:00:01\n",
      "     --------------------------              31.3/46.2 MB 36.4 MB/s eta 0:00:01\n",
      "     ----------------------------            33.3/46.2 MB 38.5 MB/s eta 0:00:01\n",
      "     -----------------------------           34.7/46.2 MB 38.6 MB/s eta 0:00:01\n",
      "     ------------------------------          36.2/46.2 MB 40.9 MB/s eta 0:00:01\n",
      "     --------------------------------        38.6/46.2 MB 38.5 MB/s eta 0:00:01\n",
      "     ---------------------------------       39.4/46.2 MB 34.4 MB/s eta 0:00:01\n",
      "     -----------------------------------     42.3/46.2 MB 34.4 MB/s eta 0:00:01\n",
      "     -------------------------------------   43.9/46.2 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.0/46.2 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 34.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 34.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 34.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 34.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 34.6 MB/s eta 0:00:01\n",
      "     --------------------------------------- 46.2/46.2 MB 17.7 MB/s eta 0:00:00\n",
      "  Downloading scipy-1.12.0-cp311-cp311-win_amd64.whl (46.2 MB)\n",
      "                                              0.0/46.2 MB ? eta -:--:--\n",
      "     -                                        1.3/46.2 MB 42.3 MB/s eta 0:00:02\n",
      "     -                                        1.8/46.2 MB 22.7 MB/s eta 0:00:02\n",
      "     -                                        2.1/46.2 MB 19.1 MB/s eta 0:00:03\n",
      "     --                                       2.4/46.2 MB 13.0 MB/s eta 0:00:04\n",
      "     --                                       2.9/46.2 MB 12.3 MB/s eta 0:00:04\n",
      "     ---                                      4.2/46.2 MB 14.9 MB/s eta 0:00:03\n",
      "     ----                                     5.2/46.2 MB 16.8 MB/s eta 0:00:03\n",
      "     -----                                    6.9/46.2 MB 19.1 MB/s eta 0:00:03\n",
      "     ------                                   7.9/46.2 MB 18.6 MB/s eta 0:00:03\n",
      "     -------                                  8.4/46.2 MB 19.8 MB/s eta 0:00:02\n",
      "     --------                                 9.4/46.2 MB 18.8 MB/s eta 0:00:02\n",
      "     ---------                               11.1/46.2 MB 18.2 MB/s eta 0:00:02\n",
      "     ----------                              12.6/46.2 MB 23.4 MB/s eta 0:00:02\n",
      "     ------------                            14.2/46.2 MB 24.2 MB/s eta 0:00:02\n",
      "     -------------                           16.0/46.2 MB 27.3 MB/s eta 0:00:02\n",
      "     ---------------                         17.9/46.2 MB 28.4 MB/s eta 0:00:01\n",
      "     ----------------                        19.1/46.2 MB 32.7 MB/s eta 0:00:01\n",
      "     -----------------                       20.5/46.2 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------                      21.5/46.2 MB 31.1 MB/s eta 0:00:01\n",
      "     -------------------                     23.1/46.2 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------                    24.4/46.2 MB 29.7 MB/s eta 0:00:01\n",
      "     ---------------------                   25.5/46.2 MB 28.4 MB/s eta 0:00:01\n",
      "     ----------------------                  26.2/46.2 MB 27.3 MB/s eta 0:00:01\n",
      "     -----------------------                 27.8/46.2 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------------                29.0/46.2 MB 26.2 MB/s eta 0:00:01\n",
      "     -------------------------               30.3/46.2 MB 25.2 MB/s eta 0:00:01\n",
      "     ---------------------------             32.3/46.2 MB 26.2 MB/s eta 0:00:01\n",
      "     ----------------------------            33.2/46.2 MB 26.2 MB/s eta 0:00:01\n",
      "     ----------------------------            34.1/46.2 MB 25.2 MB/s eta 0:00:01\n",
      "     ------------------------------          35.7/46.2 MB 26.2 MB/s eta 0:00:01\n",
      "     -------------------------------         37.4/46.2 MB 29.7 MB/s eta 0:00:01\n",
      "     ---------------------------------       39.4/46.2 MB 32.7 MB/s eta 0:00:01\n",
      "     ----------------------------------      41.2/46.2 MB 32.8 MB/s eta 0:00:01\n",
      "     -----------------------------------     42.3/46.2 MB 29.7 MB/s eta 0:00:01\n",
      "     -------------------------------------   44.7/46.2 MB 34.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 36.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 36.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 36.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 36.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 36.3 MB/s eta 0:00:01\n",
      "     --------------------------------------- 46.2/46.2 MB 16.8 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading scipy-1.11.4-cp311-cp311-win_amd64.whl (44.1 MB)\n",
      "                                              0.0/44.1 MB ? eta -:--:--\n",
      "     -                                        1.5/44.1 MB 31.4 MB/s eta 0:00:02\n",
      "     ---                                      3.4/44.1 MB 43.2 MB/s eta 0:00:01\n",
      "     ----                                     4.9/44.1 MB 34.9 MB/s eta 0:00:02\n",
      "     -----                                    6.5/44.1 MB 37.7 MB/s eta 0:00:01\n",
      "     -------                                  8.1/44.1 MB 37.0 MB/s eta 0:00:01\n",
      "     --------                                 9.4/44.1 MB 33.4 MB/s eta 0:00:02\n",
      "     ---------                               11.1/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     -----------                             13.3/44.1 MB 34.4 MB/s eta 0:00:01\n",
      "     -------------                           15.4/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------                          16.9/44.1 MB 36.3 MB/s eta 0:00:01\n",
      "     ----------------                        18.8/44.1 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------------                      20.8/44.1 MB 38.6 MB/s eta 0:00:01\n",
      "     --------------------                    22.8/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     ---------------------                   24.2/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     ----------------------                  25.9/44.1 MB 36.3 MB/s eta 0:00:01\n",
      "     ------------------------                27.2/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     -------------------------               29.3/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     ---------------------------             30.6/44.1 MB 32.7 MB/s eta 0:00:01\n",
      "     ----------------------------            32.3/44.1 MB 32.7 MB/s eta 0:00:01\n",
      "     ------------------------------          34.5/44.1 MB 36.3 MB/s eta 0:00:01\n",
      "     ------------------------------          34.7/44.1 MB 31.1 MB/s eta 0:00:01\n",
      "     --------------------------------        36.5/44.1 MB 31.2 MB/s eta 0:00:01\n",
      "     ----------------------------------      38.5/44.1 MB 32.8 MB/s eta 0:00:01\n",
      "     -----------------------------------     40.1/44.1 MB 34.4 MB/s eta 0:00:01\n",
      "     -------------------------------------   42.1/44.1 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.1/44.1 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.1/44.1 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.1/44.1 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.1/44.1 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.1/44.1 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 44.1/44.1 MB 17.2 MB/s eta 0:00:00\n",
      "  Downloading scipy-1.11.3-cp311-cp311-win_amd64.whl (44.1 MB)\n",
      "                                              0.0/44.1 MB ? eta -:--:--\n",
      "     -                                        1.3/44.1 MB 27.2 MB/s eta 0:00:02\n",
      "     --                                       2.4/44.1 MB 25.6 MB/s eta 0:00:02\n",
      "     ---                                      4.0/44.1 MB 31.7 MB/s eta 0:00:02\n",
      "     -----                                    5.6/44.1 MB 32.4 MB/s eta 0:00:02\n",
      "     ------                                   7.3/44.1 MB 35.9 MB/s eta 0:00:02\n",
      "     -------                                  8.4/44.1 MB 33.5 MB/s eta 0:00:02\n",
      "     -------                                  8.6/44.1 MB 28.9 MB/s eta 0:00:02\n",
      "     ---------                               10.4/44.1 MB 28.4 MB/s eta 0:00:02\n",
      "     ----------                              11.7/44.1 MB 27.3 MB/s eta 0:00:02\n",
      "     ----------                              12.0/44.1 MB 26.2 MB/s eta 0:00:02\n",
      "     -----------                             12.6/44.1 MB 24.2 MB/s eta 0:00:02\n",
      "     ------------                            14.2/44.1 MB 23.4 MB/s eta 0:00:02\n",
      "     --------------                          16.1/44.1 MB 24.2 MB/s eta 0:00:02\n",
      "     ---------------                         17.8/44.1 MB 23.4 MB/s eta 0:00:02\n",
      "     ----------------                        18.9/44.1 MB 27.3 MB/s eta 0:00:01\n",
      "     ----------------                        19.0/44.1 MB 23.4 MB/s eta 0:00:02\n",
      "     -----------------                       19.9/44.1 MB 21.8 MB/s eta 0:00:02\n",
      "     -------------------                     21.7/44.1 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------                    23.3/44.1 MB 26.2 MB/s eta 0:00:01\n",
      "     ---------------------                   24.8/44.1 MB 26.2 MB/s eta 0:00:01\n",
      "     ----------------------                  25.2/44.1 MB 27.3 MB/s eta 0:00:01\n",
      "     -----------------------                 26.1/44.1 MB 23.4 MB/s eta 0:00:01\n",
      "     ------------------------                27.3/44.1 MB 22.6 MB/s eta 0:00:01\n",
      "     -------------------------               28.3/44.1 MB 22.6 MB/s eta 0:00:01\n",
      "     -------------------------               28.6/44.1 MB 20.5 MB/s eta 0:00:01\n",
      "     ---------------------------             30.6/44.1 MB 25.2 MB/s eta 0:00:01\n",
      "     ----------------------------            32.6/44.1 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------------------          34.5/44.1 MB 27.3 MB/s eta 0:00:01\n",
      "     --------------------------------        36.3/44.1 MB 31.2 MB/s eta 0:00:01\n",
      "     ---------------------------------       37.7/44.1 MB 31.2 MB/s eta 0:00:01\n",
      "     ----------------------------------      39.5/44.1 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------    40.9/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     -------------------------------------   42.7/44.1 MB 32.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  43.0/44.1 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  43.1/44.1 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  43.8/44.1 MB 24.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.1/44.1 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.1/44.1 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.1/44.1 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.1/44.1 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.1/44.1 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 44.1/44.1 MB 13.3 MB/s eta 0:00:00\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading scipy-1.11.2-cp311-cp311-win_amd64.whl (44.0 MB)\n",
      "                                              0.0/44.0 MB ? eta -:--:--\n",
      "     -                                        1.9/44.0 MB 39.9 MB/s eta 0:00:02\n",
      "     --                                       2.6/44.0 MB 28.0 MB/s eta 0:00:02\n",
      "     ----                                     4.5/44.0 MB 31.9 MB/s eta 0:00:02\n",
      "     -----                                    6.3/44.0 MB 33.5 MB/s eta 0:00:02\n",
      "     --------                                 8.9/44.0 MB 33.7 MB/s eta 0:00:02\n",
      "     ---------                               10.9/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "     ----------                              12.2/44.0 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------                            14.0/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------                          16.1/44.0 MB 38.5 MB/s eta 0:00:01\n",
      "     ----------------                        18.1/44.0 MB 38.5 MB/s eta 0:00:01\n",
      "     -----------------                       20.0/44.0 MB 38.5 MB/s eta 0:00:01\n",
      "     -------------------                     21.7/44.0 MB 36.3 MB/s eta 0:00:01\n",
      "     --------------------                    23.2/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     ----------------------                  25.1/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     -----------------------                 26.3/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "     -------------------------               28.7/44.0 MB 36.3 MB/s eta 0:00:01\n",
      "     ---------------------------             30.7/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "     ----------------------------            32.3/44.0 MB 38.6 MB/s eta 0:00:01\n",
      "     ------------------------------          34.7/44.0 MB 38.5 MB/s eta 0:00:01\n",
      "     -------------------------------         35.9/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     ---------------------------------       38.2/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     -----------------------------------     39.5/44.0 MB 36.3 MB/s eta 0:00:01\n",
      "     ------------------------------------    41.0/44.0 MB 38.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.9/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 44.0/44.0 MB 15.2 MB/s eta 0:00:00\n",
      "  Downloading scipy-1.11.1-cp311-cp311-win_amd64.whl (44.0 MB)\n",
      "                                              0.0/44.0 MB ? eta -:--:--\n",
      "     -                                        1.5/44.0 MB 49.5 MB/s eta 0:00:01\n",
      "     ---                                      3.4/44.0 MB 43.2 MB/s eta 0:00:01\n",
      "     ----                                     4.6/44.0 MB 37.0 MB/s eta 0:00:02\n",
      "     ----                                     5.5/44.0 MB 31.8 MB/s eta 0:00:02\n",
      "     ------                                   6.8/44.0 MB 31.2 MB/s eta 0:00:02\n",
      "     -------                                  8.4/44.0 MB 31.7 MB/s eta 0:00:02\n",
      "     ---------                               10.6/44.0 MB 32.7 MB/s eta 0:00:02\n",
      "     ----------                              12.3/44.0 MB 32.7 MB/s eta 0:00:01\n",
      "     -------------                           14.7/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "     -------------                           15.1/44.0 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------                          16.8/44.0 MB 38.5 MB/s eta 0:00:01\n",
      "     ----------------                        18.3/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "     ----------------                        18.9/44.0 MB 36.3 MB/s eta 0:00:01\n",
      "     ------------------                      20.4/44.0 MB 31.2 MB/s eta 0:00:01\n",
      "     -------------------                     22.1/44.0 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------                    23.4/44.0 MB 29.7 MB/s eta 0:00:01\n",
      "     ----------------------                  25.0/44.0 MB 28.5 MB/s eta 0:00:01\n",
      "     -----------------------                 26.2/44.0 MB 27.3 MB/s eta 0:00:01\n",
      "     ------------------------                28.0/44.0 MB 28.4 MB/s eta 0:00:01\n",
      "     --------------------------              29.6/44.0 MB 32.7 MB/s eta 0:00:01\n",
      "     ---------------------------             31.4/44.0 MB 31.2 MB/s eta 0:00:01\n",
      "     -----------------------------           33.1/44.0 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------          34.9/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------        36.9/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     ----------------------------------      38.8/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------    40.7/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------------------    41.2/44.0 MB 32.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  43.0/44.0 MB 32.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 32.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 32.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 32.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 32.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 32.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 44.0/44.0 MB 17.7 MB/s eta 0:00:00\n",
      "  Downloading scipy-1.10.1-cp311-cp311-win_amd64.whl (42.2 MB)\n",
      "                                              0.0/42.2 MB ? eta -:--:--\n",
      "     -                                        1.5/42.2 MB 31.8 MB/s eta 0:00:02\n",
      "     ---                                      3.8/42.2 MB 40.7 MB/s eta 0:00:01\n",
      "     ----                                     5.0/42.2 MB 31.7 MB/s eta 0:00:02\n",
      "     ------                                   6.9/42.2 MB 36.8 MB/s eta 0:00:01\n",
      "     --------                                 8.5/42.2 MB 34.1 MB/s eta 0:00:01\n",
      "     ---------                               10.0/42.2 MB 35.5 MB/s eta 0:00:01\n",
      "     ----------                              11.8/42.2 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------                            13.7/42.2 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------                          15.7/42.2 MB 38.5 MB/s eta 0:00:01\n",
      "     ----------------                        17.8/42.2 MB 38.5 MB/s eta 0:00:01\n",
      "     ------------------                      19.5/42.2 MB 38.6 MB/s eta 0:00:01\n",
      "     -------------------                     21.3/42.2 MB 40.9 MB/s eta 0:00:01\n",
      "     ---------------------                   23.0/42.2 MB 38.5 MB/s eta 0:00:01\n",
      "     -----------------------                 24.9/42.2 MB 36.3 MB/s eta 0:00:01\n",
      "     ------------------------                26.5/42.2 MB 38.5 MB/s eta 0:00:01\n",
      "     --------------------------              28.5/42.2 MB 34.6 MB/s eta 0:00:01\n",
      "     ---------------------------             29.7/42.2 MB 34.4 MB/s eta 0:00:01\n",
      "     ---------------------------             30.2/42.2 MB 34.4 MB/s eta 0:00:01\n",
      "     ---------------------------             30.2/42.2 MB 34.4 MB/s eta 0:00:01\n",
      "     ----------------------------            30.5/42.2 MB 24.2 MB/s eta 0:00:01\n",
      "     ------------------------------          33.5/42.2 MB 26.2 MB/s eta 0:00:01\n",
      "     ---------------------------------       36.5/42.2 MB 27.3 MB/s eta 0:00:01\n",
      "     -----------------------------------     38.9/42.2 MB 29.7 MB/s eta 0:00:01\n",
      "     -------------------------------------   41.0/42.2 MB 59.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 54.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 54.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 54.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 54.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 54.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 42.2/42.2 MB 21.8 MB/s eta 0:00:00\n",
      "  Downloading scipy-1.10.0-cp311-cp311-win_amd64.whl (42.2 MB)\n",
      "                                              0.0/42.2 MB ? eta -:--:--\n",
      "     -                                        1.6/42.2 MB 50.5 MB/s eta 0:00:01\n",
      "     --                                       3.1/42.2 MB 40.2 MB/s eta 0:00:01\n",
      "     ----                                     4.4/42.2 MB 35.3 MB/s eta 0:00:02\n",
      "     -----                                    5.4/42.2 MB 31.4 MB/s eta 0:00:02\n",
      "     ------                                   7.3/42.2 MB 36.1 MB/s eta 0:00:01\n",
      "     --------                                 9.0/42.2 MB 33.8 MB/s eta 0:00:01\n",
      "     ---------                               10.8/42.2 MB 34.6 MB/s eta 0:00:01\n",
      "     -----------                             12.6/42.2 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------                            13.9/42.2 MB 32.7 MB/s eta 0:00:01\n",
      "     -------------                           15.1/42.2 MB 31.2 MB/s eta 0:00:01\n",
      "     ---------------                         16.8/42.2 MB 36.4 MB/s eta 0:00:01\n",
      "     ---------------                         17.2/42.2 MB 29.7 MB/s eta 0:00:01\n",
      "     -----------------                       19.1/42.2 MB 31.2 MB/s eta 0:00:01\n",
      "     -------------------                     21.3/42.2 MB 32.7 MB/s eta 0:00:01\n",
      "     ---------------------                   23.0/42.2 MB 31.2 MB/s eta 0:00:01\n",
      "     -----------------------                 25.2/42.2 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------                26.9/42.2 MB 32.7 MB/s eta 0:00:01\n",
      "     --------------------------              29.2/42.2 MB 40.9 MB/s eta 0:00:01\n",
      "     ----------------------------            31.0/42.2 MB 40.9 MB/s eta 0:00:01\n",
      "     ------------------------------          33.1/42.2 MB 40.9 MB/s eta 0:00:01\n",
      "     --------------------------------        34.9/42.2 MB 40.9 MB/s eta 0:00:01\n",
      "     ---------------------------------       36.7/42.2 MB 38.5 MB/s eta 0:00:01\n",
      "     -----------------------------------     38.5/42.2 MB 38.5 MB/s eta 0:00:01\n",
      "     -------------------------------------   40.6/42.2 MB 38.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  41.3/42.2 MB 43.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 42.2/42.2 MB 15.2 MB/s eta 0:00:00\n",
      "  Downloading scipy-1.9.3-cp311-cp311-win_amd64.whl (39.9 MB)\n",
      "                                              0.0/39.9 MB ? eta -:--:--\n",
      "     -                                        1.3/39.9 MB 27.7 MB/s eta 0:00:02\n",
      "     ---                                      3.1/39.9 MB 32.8 MB/s eta 0:00:02\n",
      "     -----                                    5.1/39.9 MB 40.6 MB/s eta 0:00:01\n",
      "     ------                                   6.3/39.9 MB 33.6 MB/s eta 0:00:01\n",
      "     --------                                 8.6/39.9 MB 34.2 MB/s eta 0:00:01\n",
      "     ---------                                9.2/39.9 MB 32.9 MB/s eta 0:00:01\n",
      "     ----------                              11.0/39.9 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------                            12.4/39.9 MB 32.8 MB/s eta 0:00:01\n",
      "     --------------                          14.5/39.9 MB 34.4 MB/s eta 0:00:01\n",
      "     ---------------                         16.1/39.9 MB 34.4 MB/s eta 0:00:01\n",
      "     -----------------                       18.0/39.9 MB 36.3 MB/s eta 0:00:01\n",
      "     -------------------                     19.8/39.9 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------                    21.4/39.9 MB 36.4 MB/s eta 0:00:01\n",
      "     ----------------------                  23.1/39.9 MB 38.6 MB/s eta 0:00:01\n",
      "     ------------------------                25.0/39.9 MB 38.6 MB/s eta 0:00:01\n",
      "     --------------------------              27.0/39.9 MB 40.9 MB/s eta 0:00:01\n",
      "     ---------------------------             28.6/39.9 MB 36.3 MB/s eta 0:00:01\n",
      "     -----------------------------           30.4/39.9 MB 36.4 MB/s eta 0:00:01\n",
      "     -------------------------------         32.3/39.9 MB 36.4 MB/s eta 0:00:01\n",
      "     ---------------------------------       33.8/39.9 MB 34.4 MB/s eta 0:00:01\n",
      "     ----------------------------------      35.7/39.9 MB 36.4 MB/s eta 0:00:01\n",
      "     -------------------------------------   38.0/39.9 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  39.9/39.9 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  39.9/39.9 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  39.9/39.9 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  39.9/39.9 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 39.9/39.9 MB 18.7 MB/s eta 0:00:00\n",
      "Collecting numpy>=1.23 (from matplotlib)\n",
      "  Downloading numpy-1.25.2-cp311-cp311-win_amd64.whl (15.5 MB)\n",
      "                                              0.0/15.5 MB ? eta -:--:--\n",
      "     ----                                     1.8/15.5 MB 37.0 MB/s eta 0:00:01\n",
      "     -------                                  3.0/15.5 MB 38.0 MB/s eta 0:00:01\n",
      "     ------------                             4.8/15.5 MB 30.3 MB/s eta 0:00:01\n",
      "     -----------------                        6.8/15.5 MB 36.1 MB/s eta 0:00:01\n",
      "     --------------------                     8.0/15.5 MB 31.9 MB/s eta 0:00:01\n",
      "     -------------------------               10.0/15.5 MB 33.5 MB/s eta 0:00:01\n",
      "     ---------------------------             11.1/15.5 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------------------------          12.3/15.5 MB 31.2 MB/s eta 0:00:01\n",
      "     -----------------------------------     14.2/15.5 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  15.5/15.5 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  15.5/15.5 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 15.5/15.5 MB 24.2 MB/s eta 0:00:00\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.9.2-cp311-cp311-win_amd64.whl (39.9 MB)\n",
      "                                              0.0/39.9 MB ? eta -:--:--\n",
      "     -                                        1.2/39.9 MB 24.4 MB/s eta 0:00:02\n",
      "     --                                       2.1/39.9 MB 33.0 MB/s eta 0:00:02\n",
      "     --                                       2.1/39.9 MB 33.0 MB/s eta 0:00:02\n",
      "     --                                       2.1/39.9 MB 33.0 MB/s eta 0:00:02\n",
      "     --                                       2.1/39.9 MB 8.9 MB/s eta 0:00:05\n",
      "     ---                                      3.3/39.9 MB 11.0 MB/s eta 0:00:04\n",
      "     ----                                     4.2/39.9 MB 13.3 MB/s eta 0:00:03\n",
      "     ----                                     4.2/39.9 MB 13.3 MB/s eta 0:00:03\n",
      "     -----                                    5.1/39.9 MB 11.6 MB/s eta 0:00:03\n",
      "     -----                                    5.7/39.9 MB 11.7 MB/s eta 0:00:03\n",
      "     ------                                   6.8/39.9 MB 12.4 MB/s eta 0:00:03\n",
      "     --------                                 8.3/39.9 MB 14.4 MB/s eta 0:00:03\n",
      "     ---------                                9.4/39.9 MB 15.4 MB/s eta 0:00:02\n",
      "     ---------                                9.6/39.9 MB 14.2 MB/s eta 0:00:03\n",
      "     ----------                              10.2/39.9 MB 14.5 MB/s eta 0:00:03\n",
      "     ----------                              10.5/39.9 MB 13.9 MB/s eta 0:00:03\n",
      "     -----------                             11.7/39.9 MB 13.4 MB/s eta 0:00:03\n",
      "     -------------                           13.4/39.9 MB 18.2 MB/s eta 0:00:02\n",
      "     --------------                          14.7/39.9 MB 22.6 MB/s eta 0:00:02\n",
      "     --------------                          14.7/39.9 MB 22.6 MB/s eta 0:00:02\n",
      "     --------------                          14.7/39.9 MB 22.6 MB/s eta 0:00:02\n",
      "     ---------------                         15.7/39.9 MB 18.2 MB/s eta 0:00:02\n",
      "     ----------------                        16.7/39.9 MB 17.7 MB/s eta 0:00:02\n",
      "     -----------------                       17.8/39.9 MB 18.2 MB/s eta 0:00:02\n",
      "     ------------------                      18.7/39.9 MB 17.7 MB/s eta 0:00:02\n",
      "     -------------------                     19.9/39.9 MB 18.7 MB/s eta 0:00:02\n",
      "     -------------------                     19.9/39.9 MB 18.7 MB/s eta 0:00:02\n",
      "     -------------------                     19.9/39.9 MB 18.7 MB/s eta 0:00:02\n",
      "     -------------------                     20.2/39.9 MB 14.5 MB/s eta 0:00:02\n",
      "     ----------------------                  22.6/39.9 MB 16.0 MB/s eta 0:00:02\n",
      "     -----------------------                 24.1/39.9 MB 15.6 MB/s eta 0:00:02\n",
      "     ------------------------                25.4/39.9 MB 19.3 MB/s eta 0:00:01\n",
      "     --------------------------              27.2/39.9 MB 20.5 MB/s eta 0:00:01\n",
      "     --------------------------              27.5/39.9 MB 19.8 MB/s eta 0:00:01\n",
      "     -----------------------------           29.7/39.9 MB 21.1 MB/s eta 0:00:01\n",
      "     ------------------------------          31.0/39.9 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------        33.1/39.9 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------        33.5/39.9 MB 29.7 MB/s eta 0:00:01\n",
      "     ---------------------------------       33.9/39.9 MB 22.6 MB/s eta 0:00:01\n",
      "     ---------------------------------       34.6/39.9 MB 22.6 MB/s eta 0:00:01\n",
      "     ----------------------------------      35.2/39.9 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------    37.3/39.9 MB 19.9 MB/s eta 0:00:01\n",
      "     -------------------------------------   38.6/39.9 MB 20.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  39.9/39.9 MB 19.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  39.9/39.9 MB 19.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  39.9/39.9 MB 19.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  39.9/39.9 MB 19.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  39.9/39.9 MB 19.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  39.9/39.9 MB 19.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  39.9/39.9 MB 19.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 39.9/39.9 MB 11.7 MB/s eta 0:00:00\n",
      "  Downloading scipy-1.9.1.tar.gz (42.0 MB)\n",
      "                                              0.0/42.0 MB ? eta -:--:--\n",
      "     --                                       2.2/42.0 MB 67.2 MB/s eta 0:00:01\n",
      "     ---                                      3.2/42.0 MB 41.1 MB/s eta 0:00:01\n",
      "     -----                                    5.3/42.0 MB 37.7 MB/s eta 0:00:01\n",
      "     ------                                   7.3/42.0 MB 42.5 MB/s eta 0:00:01\n",
      "     -------                                  8.2/42.0 MB 37.2 MB/s eta 0:00:01\n",
      "     ---------                                9.8/42.0 MB 36.7 MB/s eta 0:00:01\n",
      "     ----------                              11.7/42.0 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------                            13.4/42.0 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------                          15.3/42.0 MB 34.4 MB/s eta 0:00:01\n",
      "     ---------------                         16.6/42.0 MB 32.8 MB/s eta 0:00:01\n",
      "     ----------------                        17.8/42.0 MB 36.3 MB/s eta 0:00:01\n",
      "     -----------------                       18.8/42.0 MB 28.5 MB/s eta 0:00:01\n",
      "     ------------------                      19.9/42.0 MB 28.4 MB/s eta 0:00:01\n",
      "     -------------------                     21.0/42.0 MB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------                   23.2/42.0 MB 23.4 MB/s eta 0:00:01\n",
      "     -----------------------                 25.6/42.0 MB 24.2 MB/s eta 0:00:01\n",
      "     -------------------------               27.0/42.0 MB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------------             29.3/42.0 MB 29.8 MB/s eta 0:00:01\n",
      "     ----------------------------            30.6/42.0 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------          33.1/42.0 MB 31.2 MB/s eta 0:00:01\n",
      "     -------------------------------         34.4/42.0 MB 34.4 MB/s eta 0:00:01\n",
      "     ----------------------------------      36.7/42.0 MB 34.4 MB/s eta 0:00:01\n",
      "     -----------------------------------     37.7/42.0 MB 34.4 MB/s eta 0:00:01\n",
      "     -------------------------------------   40.1/42.0 MB 32.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  41.5/42.0 MB 38.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.0/42.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.0/42.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.0/42.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.0/42.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 42.0/42.0 MB 19.3 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade nltk openpyxl matplotlib textblob spacy gensim scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (4.66.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx==1.16.1\n",
      "  Downloading onnx-1.16.1-cp311-cp311-win_amd64.whl (14.4 MB)\n",
      "                                              0.0/14.4 MB ? eta -:--:--\n",
      "                                              0.1/14.4 MB 3.3 MB/s eta 0:00:05\n",
      "     -                                        0.6/14.4 MB 7.4 MB/s eta 0:00:02\n",
      "     -----                                    2.2/14.4 MB 15.4 MB/s eta 0:00:01\n",
      "     ----------                               3.8/14.4 MB 22.0 MB/s eta 0:00:01\n",
      "     -------------                            5.0/14.4 MB 23.0 MB/s eta 0:00:01\n",
      "     -----------------                        6.4/14.4 MB 25.7 MB/s eta 0:00:01\n",
      "     ---------------------                    7.8/14.4 MB 25.1 MB/s eta 0:00:01\n",
      "     ------------------------                 8.8/14.4 MB 24.5 MB/s eta 0:00:01\n",
      "     ----------------------------            10.4/14.4 MB 27.3 MB/s eta 0:00:01\n",
      "     --------------------------------        12.0/14.4 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------------    13.6/14.4 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  14.4/14.4 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  14.4/14.4 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 14.4/14.4 MB 24.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from onnx==1.16.1) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from onnx==1.16.1) (4.25.5)\n",
      "Installing collected packages: onnx\n",
      "  Attempting uninstall: onnx\n",
      "    Found existing installation: onnx 1.17.0\n",
      "    Uninstalling onnx-1.17.0:\n",
      "      Successfully uninstalled onnx-1.17.0\n",
      "Successfully installed onnx-1.16.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install onnx==1.16.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import textblob \n",
    "#!python -m textblob.download_corpora\n",
    "#import spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('universal_tagset')\n",
    "#nltk.download('averaged_perceptron_tagger_eng')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('punkt_tab')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Environment Parameters\n",
    "Prepare the list of parameter in .env file for later use. \n",
    "Parameters: \n",
    "- API keys for LLMs\n",
    "    - OPENAI_API_KEY \n",
    "    - HUGGINGFACEHUB_API_TOKEN \n",
    "- Directory / location for documents and vector databases\n",
    "    - DOC_ARVIX = \"./source/from_arvix/\"\n",
    "    - DOC_WIKI = \"./source/from_wiki/\"\n",
    "    - VECTORDB_OPENAI_EM = \"./vector_db/openai_embedding/\"\n",
    "    - VECTORDB_MINILM_EM = \"./vector_db/gpt4all_miniLM/\"\n",
    "    - TS_RAGAS = \"./evaluation/testset/by_RAGAS/\"\n",
    "    - TS_PROMPT = \"./evaluation/testset/by_direct_prompt/\"\n",
    "    - EVAL_DATASET = \"./evaluation/evaluation_data_set/\"\n",
    "    - EVAL_METRIC = \"./evaluation/evaluation_metric\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Simple RAG Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagrams/HL architecture.png\" alt=\"HL arc\" title= \"HL Architecture\" />\n",
    "\n",
    "The system comprises of 5 components: \n",
    "\n",
    "- Internal data, documents: The system starts with a collection of internal documents and / or structured databases. Documents can be in text, PDF, photo or video formats. These documents and data are sources for the specified knowledgebase.\n",
    "\n",
    "- Embedding processor: The documents and database entries are processed to create vector embeddings. Embeddings are numerical representations of the documents in a high-dimensional space that capture their semantic meaning. \n",
    "\n",
    "- Vector database: the vectorized chunk of documents and database entries are stored on vector database to be search and retrieved in a later stage. \n",
    "\n",
    "- Query processor: The query processor takes the user's query and performs semantic search against the vectorized database. This component ensures that the query is interpreted correctly and retrieves relevant document embeddings from the vectorized DB. It combines the user's original query with the retrieved document embeddings to form a context-rich query. This augmented query provides additional context that can help in generating a more accurate and relevant response.\n",
    "\n",
    "- LLM: pre-trained large language model where the augmented query is passed to for generating a response based on the query and the relevant documents.\n",
    "\n",
    "The system involves 2 main pipelines: the embedding pipeline and the retrieval pipeline. Each pipeline has specific stages and processes that contribute to the overall functionality of the system.\n",
    "\n",
    "In this experiment, we use Langchain as a framework to build a simple RAG as a chain of tasks, which interacts with surrounding services like parsing, embedding, vector database and LLMs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. MultiModal RAG Architecture\n",
    "<img src=\"diagrams/ISM6564-Project.png\" alt=\"HL arc\" title= \"MM HL Architecture\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the environment parameters\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we load data from various sources. Make them ready to ingest.\n",
    "We will download 5 articles from ARVIX with query \"RAG for Large Language Model\" and store them locally and ready for next steps of embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From ARXIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv \n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "  query = \"RAG for Large Language Model\",     # To get more of other topics and number of papers. \n",
    "  max_results = 5,\n",
    "#  sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "results = client.results(search)\n",
    "all_results = list(client.results(search)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks http://arxiv.org/abs/2407.21059v1\n",
      "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation http://arxiv.org/abs/2408.02545v1\n",
      "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries http://arxiv.org/abs/2401.15391v1\n",
      "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge Update http://arxiv.org/abs/2410.20299v1\n",
      "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models http://arxiv.org/abs/2410.07176v1\n"
     ]
    }
   ],
   "source": [
    "# Print out the articles' titles\n",
    "for r in all_results:\n",
    "    print(f\"{r.title} {r.entry_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: download articles and save them in pre-defined location for later use\n",
    "# Prepare: create the environment paramter DOC_ARVIX for the path to save articles. \n",
    "# Download and save articles in PDF format to the \"RAG_for_LLM\" folder under ARVIX_DOC path\n",
    "DOC_ARVIX = os.getenv(\"DOC_ARVIX\") \n",
    "directory_path = os.path.join(DOC_ARVIX) \n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "for r in all_results:\n",
    "    r.download_pdf(dirpath=directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Springer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Lexis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step and the previous one are usually processed together. I try to separate them to make attention that these are not always coupled.\n",
    "We use available library DirectoryLoader and PyMuPDFLoader from Langchain to load and parse all .pdf files in the directory.\n",
    "We can use corresponding loader for other data types such as excel, presentation, unstructured ... \n",
    "\n",
    "Refer to https://python.langchain.com/v0.1/docs/integrations/document_loaders/ for other available loaders. \n",
    "We also use the OCR library rapidocr to extract image as text. Certainly, the trade-off is processing time. It took 18 minutes to parse 5 pdf files with OCR compared to 0.1 second without. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Util functions for Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pavan Teja\\Documents\\TextAnalyticsProject\\ISM6564_TextAnalytics_Project\\ism6564\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.documents.elements import NarrativeText\n",
    "\n",
    "\n",
    "# Extract elements from PDF\n",
    "def extract_pdf_elements(path, fname,img_path=\"\"):\n",
    "    \"\"\"\n",
    "    Extract images, tables, and chunk text from a PDF file.\n",
    "    path: File path, which is used to dump images (.jpg)\n",
    "    fname: File name\n",
    "    \"\"\"\n",
    "    if img_path == \"\":\n",
    "        img_path = path\n",
    "    if not os.path.exists(img_path):\n",
    "        os.makedirs(img_path)\n",
    "    return partition_pdf(\n",
    "        filename=path + fname,\n",
    "        extract_images_in_pdf=True,\n",
    "        infer_table_structure=True,\n",
    "        chunking_strategy=\"by_title\",\n",
    "        max_characters=4000,\n",
    "        new_after_n_chars=3800,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        image_output_dir_path=img_path,\n",
    "        form_extraction_skip_tables=False,\n",
    "        extract_image_block_output_dir = img_path\n",
    "    )\n",
    "\n",
    "def extract_pdf_elements_v2(path, fname,img_path=\"\"):\n",
    "    \"\"\"\n",
    "    Extract images, tables, and chunk text from a PDF file.\n",
    "    path: File path, which is used to dump images (.jpg)\n",
    "    fname: File name\n",
    "    \"\"\"\n",
    "    if img_path == \"\":\n",
    "        img_path = path\n",
    "    if not os.path.exists(img_path):\n",
    "        os.makedirs(img_path)\n",
    "    return partition_pdf(\n",
    "        filename=path + fname,\n",
    "        extract_images_in_pdf=True,\n",
    "        infer_table_structure=True,\n",
    "        strategy=\"hi_res\",\n",
    "        max_characters=4000,\n",
    "        new_after_n_chars=3800,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        image_output_dir_path=img_path,\n",
    "        form_extraction_skip_tables=False,\n",
    "        extract_image_block_output_dir = img_path\n",
    "    )\n",
    "\n",
    "# Categorize elements by type\n",
    "def categorize_elements(raw_pdf_elements):\n",
    "    \"\"\"\n",
    "    Categorize extracted elements from a PDF into tables and texts.\n",
    "    raw_pdf_elements: List of unstructured.documents.elements\n",
    "    \"\"\"\n",
    "    tables = []\n",
    "    texts = []\n",
    "    for element in raw_pdf_elements:\n",
    "        if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            tables.append(element.to_dict()[\"metadata\"][\"text_as_html\"])\n",
    "        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "            texts.append(str(element))\n",
    "    return texts, tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Generate summaries of text elements\n",
    "def generate_text_summaries(texts, tables, summarize_texts=False):\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str\n",
    "    summarize_texts: Bool to summarize texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "    # Text summary chain\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4\")\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "    table_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    if texts and summarize_texts:\n",
    "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "    elif texts:\n",
    "        text_summaries = texts\n",
    "\n",
    "    # Apply to tables if tables are provided\n",
    "    if tables:\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
    "\n",
    "    return text_summaries, table_summaries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def image_summarize(img_base64, prompt):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "    chat = ChatOpenAI(model=\"gpt-4o\", max_tokens=1024)\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "def generate_img_summaries(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            print(img_file)\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summaries.append(image_summarize(base64_image, prompt))\n",
    "\n",
    "    return img_base64_list, image_summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure the API key for Google Generative AI\n",
    "genai.configure(api_key=\"AIzaSyBkc0QW2Lww4vVxGNPQ23Qb0oFUnVxIn88\")\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def image_summarize_gemini(img_base64, prompt):\n",
    "\n",
    "    # Create the combined prompt with the base64 image\n",
    "    combined_prompt = f\"{prompt}\\n\\n![image](data:image/jpeg;base64,{img_base64})\"\n",
    "\n",
    "    # Choose a Gemini model\n",
    "    model = genai.GenerativeModel(model_name=\"gemini-1.5-pro\")\n",
    "\n",
    "    # Make the LLM inference request for summarization\n",
    "    print(\"Making LLM inference request...\")\n",
    "    try:\n",
    "        response = model.generate_content([combined_prompt], request_options={\"timeout\": 600})\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error during API request: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_img_summaries_gemini(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval. \\\n",
    "    Summarize this image, focusing on the objects in the foreground. \"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            print(f\"Processing: {img_file}\")\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summary = image_summarize_gemini(base64_image, prompt)\n",
    "            image_summaries.append(image_summary)\n",
    "\n",
    "    return img_base64_list, image_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using BLIP Transformers\n",
    "import base64\n",
    "import os\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Initialize processor and model\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def image_summarize_blip(img_base64, prompt):\n",
    "    image = Image.open(BytesIO(base64.b64decode(img_base64)))\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs)\n",
    "    return processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def generate_img_summaries_blip(path):\n",
    "    img_base64_list, image_summaries = [], []\n",
    "    prompt = \"Provide a concise, retrieval-optimized image summary.\"\n",
    "\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summaries.append(image_summarize_blip(base64_image, prompt))\n",
    "\n",
    "    return img_base64_list, image_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "from io import BytesIO\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load the CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Encode an image file to base64 format.\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def image_summarize_clip(img_base64, prompt):\n",
    "    \"\"\"Summarizes the image based on the provided prompt using CLIP.\"\"\"\n",
    "    image = Image.open(BytesIO(base64.b64decode(img_base64)))\n",
    "    \n",
    "    # Create the input for CLIP\n",
    "    inputs = processor(text=[prompt], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits_per_image = model(**inputs).logits_per_image\n",
    "        probs = logits_per_image.softmax(dim=1)\n",
    "\n",
    "    # Since we have only one prompt, we can simply return a formatted summary\n",
    "    return f\"Image summary based on prompt: '{prompt}'\"\n",
    "\n",
    "def generate_img_summaries_clip(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images.\n",
    "    path: Path to list of .jpg files extracted by Unstructured.\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            print(f\"Processing: {img_file}\")\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summary = image_summarize_clip(base64_image, prompt)\n",
    "            image_summaries.append(image_summary)\n",
    "\n",
    "    return img_base64_list, image_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, T5Tokenizer, T5ForConditionalGeneration, ViTModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import os\n",
    "\n",
    "# Initialize feature extractor and models\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "vit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Encode an image file to base64 format.\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def image_summarize_vit_t5(img_base64):\n",
    "    \"\"\"Summarizes the image content using ViT and T5 with an English prompt.\"\"\"\n",
    "    image = Image.open(BytesIO(base64.b64decode(img_base64)))\n",
    "    \n",
    "    # Extract features from the image\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        embeddings = vit_model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    # Generate a summary using T5 with specific English prompt\n",
    "    prompt = \"Provide a concise English summary of the image content.\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    output = t5_model.generate(\n",
    "        input_ids, \n",
    "        max_length=30, \n",
    "        num_beams=5, \n",
    "        early_stopping=True, \n",
    "        repetition_penalty=2.0  # Helps to reduce repetitive text\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def generate_img_summaries_vit_t5(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images.\n",
    "    path: Path to list of .jpg files.\n",
    "    \"\"\"\n",
    "    img_base64_list, image_summaries = [], []\n",
    "    \n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            print(f\"Processing: {img_file}\")\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summary = image_summarize_vit_t5(base64_image)\n",
    "            image_summaries.append(image_summary)\n",
    "\n",
    "    return img_base64_list, image_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "# Load the processor and model\n",
    "processor = AutoProcessor.from_pretrained(\"google/deplot\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"google/deplot\")\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Encode an image file to base64 format.\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "def image_summarize_deplot(img_base64):\n",
    "    \"\"\"Summarizes the image content using the DePlot model with a refined prompt.\"\"\"\n",
    "    # Decode the image from base64\n",
    "    image = Image.open(BytesIO(base64.b64decode(img_base64)))\n",
    "\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval. \\\n",
    "    Describe the overall concept of the image.\"\"\"\n",
    "    \n",
    "    # Create inputs with a detailed prompt for better summarization\n",
    "    inputs = processor(\n",
    "        images=image,\n",
    "        text=prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Generate summary with adjusted parameters\n",
    "    output = model.generate(**inputs, max_length=30, num_beams=5, early_stopping=True)\n",
    "    summary = processor.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "\n",
    "def generate_img_summaries_deplot(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images.\n",
    "    path: Path to list of .jpg files.\n",
    "    \"\"\"\n",
    "    img_base64_list, image_summaries = [], []\n",
    "    \n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            print(f\"Processing: {img_file}\")\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summary = image_summarize_deplot(base64_image)\n",
    "            image_summaries.append(image_summary)\n",
    "\n",
    "    return img_base64_list, image_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "CHROMA_OPENAI_RAG_FOR_LLM = \"CHROMA_OPENAI_RAG_FOR_LLM\"\n",
    "CHROMA_HF_RAG_FOR_LLM = \"CHROMA_HF_RAG_FOR_LLM\"\n",
    "CHROMA_MINILM_RAG_FOR_LLM = \"CHROMA_MINILM_RAG_FOR_LLM\"\n",
    "CHROMA_OLLAMA_RAG_FOR_LLM = \"CHROMA_OLLAMA_RAG_FOR_LLM\"\n",
    "\n",
    "#IMPORTANT: THE CHROMA INSTANCE CANNOT INITIATED WITHIN A .PY. IT WILL CRASH THE KERNEL. \n",
    "class VectorBD:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vectordb_name) -> None:\n",
    "        load_dotenv()\n",
    "#       OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "#       print(OPENAI_API_KEY)\n",
    "        if vectordb_name == CHROMA_OPENAI_RAG_FOR_LLM:\n",
    "            self.vectordb_directory = os.path.join(os.getenv(\"VECTORDB_OPENAI_EM\"),\"RAG_for_LLM\")\n",
    "            self.embeddings = OpenAIEmbeddings()\n",
    "            self.vectordb =  Chroma(persist_directory=self.vectordb_directory, embedding_function=self.embeddings)\n",
    "            self.retriever = self.vectordb.as_retriever()\n",
    "\n",
    "        if vectordb_name == CHROMA_MINILM_RAG_FOR_LLM:\n",
    "            self.vectordb_directory = os.path.join(os.getenv(\"VECTORDB_MINILM_EM\"),\"RAG_for_LLM\")\n",
    "            self.embeddings = GPT4AllEmbeddings(model_name=\"all-MiniLM-L6-v2.gguf2.f16.gguf\", gpt4all_kwargs={'allow_download': 'True'})\n",
    "            self.vectordb =  Chroma(persist_directory=self.vectordb_directory, embedding_function=self.embeddings)\n",
    "            self.retriever = self.vectordb.as_retriever()\n",
    "\n",
    "        if vectordb_name == CHROMA_OLLAMA_RAG_FOR_LLM:\n",
    "            self.vectordb_directory = os.path.join(os.getenv(\"VECTORDB_OLLAMA_EM\"),\"RAG_for_LLM\")\n",
    "            self.embeddings = OllamaEmbeddings(model=\"llama3.1\")\n",
    "            self.vectordb =  Chroma(persist_directory=self.vectordb_directory, embedding_function=self.embeddings)\n",
    "            self.retriever = self.vectordb.as_retriever()\n",
    "\n",
    "        if vectordb_name == CHROMA_HF_RAG_FOR_LLM:\n",
    "            self.vectordb_directory = os.path.join(os.getenv(\"VECTORDB_HF_EM\"),\"RAG_for_LLM\")\n",
    "            self.embeddings = HuggingFaceEmbeddings()\n",
    "            self.vectordb =  Chroma(persist_directory=self.vectordb_directory, embedding_function=self.embeddings)\n",
    "            self.retriever = self.vectordb.as_retriever()       \n",
    "\n",
    "    def vectorizing(self, documents):\n",
    "        self.vectordb = Chroma.from_documents(documents=documents,embedding=self.embeddings, persist_directory=self.vectordb_directory)\n",
    "        self.vectordb.persist()\n",
    "\n",
    "    def invoke(self,question):\n",
    "#       print(self.retriever.invoke(\"What is RAG?\"))\n",
    "        return self.retriever.invoke(question)\n",
    "\n",
    "def connect_km(km_name):\n",
    "    load_dotenv()\n",
    "#   OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "#   print(OPENAI_API_KEY)\n",
    "    if km_name == CHROMA_OPENAI_RAG_FOR_LLM:\n",
    "        km_dir = os.path.join(os.getenv(\"VECTORDB_OPENAI_EM\"),\"RAG_for_LLM\")\n",
    "        km_embeddings = OpenAIEmbeddings()\n",
    "        km_db =  Chroma(persist_directory=km_dir, embedding_function=km_embeddings)\n",
    "        return km_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Connect to LLM \n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_huggingface import HuggingFaceEndpoint \n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "llm_model = {\n",
    "    \"GPT_3_5_TURBO\" : \"gpt-3.5-turbo\",\n",
    "    \"GPT_4\" : \"gpt-4\",\n",
    "    \"GPT_4o\" : \"gpt-4o\",  #For vision\n",
    "    \"GPT_4_PREVIEW\" : \"gpt-4-1106-preview\",\n",
    "    \"LOCAL_GPT4ALL\" : \"\",\n",
    "    \"MISRALAI\" : \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"LLAMA3_70B\" : \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "    \"ZEPHYR_7B\" : \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    \"OLLAMA_GEMMA2\" : \"gemma2\",\n",
    "    \"OLLAMA_LLAMA3\" : \"llama3\",\n",
    "    \"OLLAMA_LLAMA3.1\" : \"llama3.1\"\n",
    "}\n",
    "\n",
    "def connectLLM(model, temperature = 0):\n",
    "    load_dotenv()\n",
    "\n",
    "    # Connect to Open AI chat model: Online, Token-base\n",
    "    if model == \"GPT_3_5_TURBO\" or model == \"GPT_4_PREVIEW\" or model == \"GPT_4\" or model == \"GPT_4o\":\n",
    "#       print(\"connect llm\")\n",
    "        return ChatOpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY\"), model=llm_model[model], temperature=temperature)\n",
    "    \n",
    "    # Connect to HuggingFace chat model: Online, Token-base\n",
    "    # Note: to use Llama3, we need to register on HuggingFace website\n",
    "    if model == \"LLAMA3_70B\" or model == \"MISRALAI\" or model == \"ZEPHYR_7B\":\n",
    "        repo_id = llm_model[model]\n",
    "        return HuggingFaceEndpoint(\n",
    "            repo_id=repo_id,\n",
    "            max_length=128,\n",
    "            temperature=temperature, # Should be 0.5 \n",
    "            huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "        )\n",
    "    \n",
    "    # Connect to Ollama for Llama3, Llama3.1 and Gemma2 chat models\n",
    "    # Need these models are working locally, they must have been downloaded. Check instruction for downloading Ollama and models\n",
    "    if model == \"OLLAMA_GEMMA2\" or model == \"OLLAMA_LLAMA3\" or model == \"OLLAMA_LLAMA3.1\":\n",
    "        return ChatOllama(model=llm_model[model], temperature=temperature)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "def create_multi_vector_retriever(\n",
    "    vectorstore, text_summaries, texts, table_summaries, tables, image_summaries, images,image_file = [],\n",
    "    document_meta ={} # is the meta data to be stored together with the vectorized content\n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    # Helper function to add documents to the vectorstore and docstore\n",
    "    def add_documents(retriever, doc_summaries, doc_contents,type = \"\"):\n",
    "        doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "        summary_docs = [\n",
    "            Document(page_content=s, \n",
    "                     metadata={\n",
    "                         id_key: doc_ids[i],\n",
    "                         'source': document_meta.get(\"filename\",\"\"),\n",
    "                         'type': type,\n",
    "                         'paper_id': document_meta.get(\"docid\",\"\")\n",
    "                         }\n",
    "                     )\n",
    "            for i, s in enumerate(doc_summaries)\n",
    "        ]\n",
    "        content_docs = [\n",
    "            Document(page_content=s, \n",
    "                     metadata={\n",
    "                         id_key: doc_ids[i],\n",
    "                         'source': document_meta.get(\"filename\",\"\"),\n",
    "                         'type': type,\n",
    "                         'paper_id': document_meta.get(\"docid\",\"\")\n",
    "                         }\n",
    "                     )\n",
    "            for i, s in enumerate(doc_contents)\n",
    "        ]\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, content_docs)))\n",
    "\n",
    "    # Add texts, tables, and images\n",
    "    # Check that text_summaries is not empty before adding\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts,\"text\")\n",
    "    # Check that table_summaries is not empty before adding\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables,\"table\")\n",
    "    # Check that image_summaries is not empty before adding\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images,\"image\")\n",
    "\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries.pdf',\n",
       " '2407.21059v1.Modular_RAG__Transforming_RAG_Systems_into_LEGO_like_Reconfigurable_Frameworks.pdf',\n",
       " '2408.02545v1.RAG_Foundry__A_Framework_for_Enhancing_LLMs_for_Retrieval_Augmented_Generation.pdf',\n",
       " '2410.07176v1.Astute_RAG__Overcoming_Imperfect_Retrieval_Augmentation_and_Knowledge_Conflicts_for_Large_Language_Models.pdf',\n",
       " '2410.20299v1.EACO_RAG__Edge_Assisted_and_Collaborative_RAG_with_Adaptive_Knowledge_Update.pdf']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOC_ARVIX = os.getenv(\"DOC_ARVIX\") \n",
    "directory_path = os.path.join(DOC_ARVIX) \n",
    "pdffiles = [f for f in os.listdir(directory_path) if f.endswith(\".pdf\")]\n",
    "pdffiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>filename</th>\n",
       "      <th>status</th>\n",
       "      <th>topic</th>\n",
       "      <th>summary</th>\n",
       "      <th>img_folder</th>\n",
       "      <th>imgs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a5cdaa51-39b4-42fe-bc76-e19fb729c37b</td>\n",
       "      <td>2401.15391v1.MultiHop_RAG__Benchmarking_Retrie...</td>\n",
       "      <td>new</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>./figure/document_0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>012e560a-9388-4f1f-9ae3-1b4afc2a0bcd</td>\n",
       "      <td>2407.21059v1.Modular_RAG__Transforming_RAG_Sys...</td>\n",
       "      <td>new</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>./figure/document_1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a4b74ce1-b399-4b18-93ac-0c620d1438c7</td>\n",
       "      <td>2408.02545v1.RAG_Foundry__A_Framework_for_Enha...</td>\n",
       "      <td>new</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>./figure/document_2</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>decc1461-6857-422c-b0d4-b6f6420e0d6a</td>\n",
       "      <td>2410.20299v1.EACO_RAG__Edge_Assisted_and_Colla...</td>\n",
       "      <td>new</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>./figure/document_3</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  docid  \\\n",
       "0  a5cdaa51-39b4-42fe-bc76-e19fb729c37b   \n",
       "1  012e560a-9388-4f1f-9ae3-1b4afc2a0bcd   \n",
       "2  a4b74ce1-b399-4b18-93ac-0c620d1438c7   \n",
       "3  decc1461-6857-422c-b0d4-b6f6420e0d6a   \n",
       "\n",
       "                                            filename status topic summary  \\\n",
       "0  2401.15391v1.MultiHop_RAG__Benchmarking_Retrie...    new                 \n",
       "1  2407.21059v1.Modular_RAG__Transforming_RAG_Sys...    new                 \n",
       "2  2408.02545v1.RAG_Foundry__A_Framework_for_Enha...    new                 \n",
       "3  2410.20299v1.EACO_RAG__Edge_Assisted_and_Colla...    new                 \n",
       "\n",
       "            img_folder imgs  \n",
       "0  ./figure/document_0   []  \n",
       "1  ./figure/document_1   []  \n",
       "2  ./figure/document_2   []  \n",
       "3  ./figure/document_3   []  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import uuid\n",
    "\n",
    "# Load document catalog from picker files\n",
    "if os.path.exists('document_catalog.pickle'):\n",
    "    with open('document_catalog.pickle', 'rb') as pkl_file:\n",
    "        df_documents = pickle.load(pkl_file) \n",
    "else:\n",
    "    df_documents = pd.DataFrame(columns=[\"docid\",\"filename\",\"status\",\"topic\",\"summary\",\"img_folder\",\"imgs\"])\n",
    "\n",
    "# Load new files for processing\n",
    "new_item = False\n",
    "existing_files = list(df_documents[\"filename\"])\n",
    "for fn in existing_files:\n",
    "    if fn not in existing_files:\n",
    "        i = len(df_documents.index)\n",
    "        df_documents.loc[len(df_documents.index)]={\"docid\":str(uuid.uuid4()),\"filename\":fn,\"status\":\"new\",\"topic\":\"\",\"summary\":\"\",\"img_folder\":\"./figure/document_\"+str(i),\"imgs\":[]}\n",
    "        new_item = True\n",
    "if new_item:\n",
    "    with open('document_catalog.pickle', 'wb') as pkl_file:\n",
    "        pickle.dump(df_documents,pkl_file)\n",
    "df_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Text Parsing and Image Extraction\n",
    "\n",
    "From each of pdf, extracts images. Expected return a list of images for each PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example for a subset of dataframe aka the first document / paper\n",
    "df_subset = df_documents.loc[0]\n",
    "directory_path = os.path.join(os.getenv(\"DOC_ARVIX\"))\n",
    "doc_elements = extract_pdf_elements(directory_path,df_subset[\"filename\"],df_subset[\"img_folder\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, tables = categorize_elements(doc_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Text Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the data into smaller chunks for better handling, processing, and retrieving.\n",
    "There is a limitation on number of tokens which the embedding service can process at later stage which requires documents are chunked in smaller size.\n",
    "There are many of chunking methods from Langchain. In which, Recursive CharacterText and Semantic are most popular. \n",
    "\n",
    "Reference: https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=4000, chunk_overlap=0\n",
    ")\n",
    "joined_texts = \" \".join(texts)\n",
    "texts_4k_token = text_splitter.split_text(joined_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Table Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "import html_to_json \n",
    "import html\n",
    "for element in tables:\n",
    "    display(HTML(element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Text, Table and Image Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LLM e.g. Llama3.1 or Gemini to provide summary for an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get text, table summaries\n",
    "text_summaries, table_summaries = generate_text_summaries(\n",
    "    texts_4k_token, tables, summarize_texts=False # Will use the original Text for getting vectorized\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "figure-1-1.jpg\n",
      "figure-4-2.jpg\n",
      "figure-7-3.jpg\n"
     ]
    }
   ],
   "source": [
    "## return string of summary for an input of image\n",
    "# Image summaries\n",
    "img_base64_list, image_summaries = generate_img_summaries(df_subset[\"img_folder\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pavan Teja\\Documents\\TextAnalyticsProject\\ISM6564_TextAnalytics_Project\\ism6564\\Lib\\site-packages\\transformers\\generation\\utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a diagram of the architecture of a computer',\n",
       " 'a diagram of the different types of the genome',\n",
       " 'a bar graph shows the number of different types of the different types of the different types of the']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_base64_list_blip, image_summaries_blip = generate_img_summaries_blip(df_subset[\"img_folder\"])\n",
    "image_summaries_blip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: figure-1-1.jpg\n",
      "Processing: figure-4-2.jpg\n",
      "Processing: figure-7-3.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Image summary based on prompt: 'You are an assistant tasked with summarizing images for retrieval.     These summaries will be embedded and used to retrieve the raw image.     Give a concise summary of the image that is well optimized for retrieval.'\",\n",
       " \"Image summary based on prompt: 'You are an assistant tasked with summarizing images for retrieval.     These summaries will be embedded and used to retrieve the raw image.     Give a concise summary of the image that is well optimized for retrieval.'\",\n",
       " \"Image summary based on prompt: 'You are an assistant tasked with summarizing images for retrieval.     These summaries will be embedded and used to retrieve the raw image.     Give a concise summary of the image that is well optimized for retrieval.'\"]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "img_base64_list_clip, image_summaries_clip = generate_img_summaries_clip(df_subset[\"img_folder\"])\n",
    "image_summaries_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: figure-1-1.jpg\n",
      "Processing: figure-4-2.jpg\n",
      "Processing: figure-7-3.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Provide a concise English summary of the image content.',\n",
       " 'Provide a concise English summary of the image content.',\n",
       " 'Provide a concise English summary of the image content.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "img_base64_list_vit, image_summaries_vit = generate_img_summaries_vit_t5(df_subset[\"img_folder\"])\n",
    "image_summaries_vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: figure-1-1.jpg\n",
      "Processing: figure-4-2.jpg\n",
      "Processing: figure-7-3.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['TITLE |  <0x0A> Multi-Documents | Google | Chunk <0x0A> Which company among<0x0A>Google, Apple, and Nvidia<0x0A>reported the',\n",
       " 'TITLE |  <0x0A>  | Extract Factual | Select Sentences <0x0A> Quality Assurance | 0.97 | 0',\n",
       " 'TITLE | Retrieved Chunk <0x0A>  | Mixtral-8x7B | GPT-4 <0x0A> inference |']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_base64_list_deplot, image_summaries_deplot = generate_img_summaries_deplot(df_subset[\"img_folder\"])\n",
    "image_summaries_deplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: figure-1-1.jpg\n",
      "Making LLM inference request...\n",
      "Processing: figure-4-2.jpg\n",
      "Making LLM inference request...\n",
      "Error during API request: 429 Resource has been exhausted (e.g. check quota).\n",
      "Processing: figure-7-3.jpg\n",
      "Making LLM inference request...\n",
      "Error during API request: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['A close-up of several pink flowers, likely tulips, with green stems and leaves. They appear to be in a garden or similar setting.\\n',\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_base64_list_gemini, image_summaries_gemini = generate_img_summaries_gemini(df_subset[\"img_folder\"])\n",
    "image_summaries_gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Text, Table and Image Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors are semantic representation of texts. \n",
    "This is an important step to make documents searchable in the later pipeline. \n",
    "Embedding is an essential step in Transformer architecture, underlined to every modern LLMs. Therefore, many LLMs provide their embedding functions as services which are ready to use, e.g. OpenAI embedding API. However, it is important to consider privacy risk when exposing internal data to those services.\n",
    "\n",
    "IMPORTANT NOTE: \n",
    "1. the embedding method to perform similarity search in the retrieval pipeline must be the same to the one used to vectorize documents in this step. \n",
    "2. Public embedding method such as OpenAIEmbedding may cost a fraction of money and leak internal data.  \n",
    "\n",
    "Reference: https://python.langchain.com/v0.1/docs/modules/data_connection/text_embedding/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings #To use other embeddings e.g. Llama or Gemini\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectordb_directory = os.path.join(os.getenv(\"VECTORDB_OPENAI_EM\"),\"\")\n",
    "vectorstore =  Chroma( collection_name=\"research_paper\",persist_directory=vectordb_directory, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.reset_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever\n",
    "retriever_multi_vector_img = create_multi_vector_retriever(\n",
    "    vectorstore,\n",
    "    text_summaries,\n",
    "    texts_4k_token,\n",
    "    table_summaries,\n",
    "    tables,\n",
    "    image_summaries,\n",
    "    img_base64_list,\n",
    "    df_subset[\"imgs\"],\n",
    "    dict(df_subset)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Article Summary\n",
    "Using LLM to summarize the paper (as text or as image (convert pdf to image ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Store Article Summary + Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Store Vector DB (New version of Chroma persists data automatically after vectorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some vector databases of choices: Chroma, FAISS, Pinecone ... \n",
    "We will create Chroma vector database with openai embedding method. \n",
    "\n",
    "Note: different embedding methods will result different vector dimensions and cannot be stored together. \n",
    "The same embedding method to be used in retrieval pipeline\n",
    "\n",
    "Reference: https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval pipeline is to retrieve relevant chunk of knowledge from pre-prepared vectorized knowledge to enrich the LLM prompt with specified context. This pipeline is run to respond to each user’s query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to load from store if there is, here is Chroma vectordb we have just persisted. \n",
    "Perform a semantic search in the vectorized database to retrieve relevant embedded documents.\n",
    "\n",
    "NOTE: The embedding method used in this step must be same as which used to vectorize knowledges in the previous pipeline.\n",
    "\n",
    "There is opportunity to improve efficiency and quality of similarity search, especially when the knowledgebase gets larger and more complicated (type of sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Util functions for retrieval and response processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xff\\xd8\\xff\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            doc = resize_base64_image(doc, size=(1300, 600))\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    return {\"images\": b64_images, \"texts\": texts}\n",
    "\n",
    "\n",
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are financial analyst tasking with providing investment advice.\\n\"\n",
    "            \"You will be given a mixed of text, tables, and image(s) usually of charts or graphs.\\n\"\n",
    "            \"Use this information to provide investment advice related to the user question. \\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "\n",
    "def multi_modal_rag_chain(retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Multi-modal LLM\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o\", max_tokens=1024)\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Process Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is the pperformance of GPT-4 vs Mixtral?\"\n",
    "#user_query = \"Describe the RAG-Sequence Model?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Retrieve Relevant Docs - Text, Table, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = retriever_multi_vector_img.invoke(user_query, limit=3) # Top k relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'doc_id': '7b933c86-50a5-44f1-905b-17ea7464eca9', 'source': '2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries.pdf', 'type': 'image', 'paper_id': 'a5cdaa51-39b4-42fe-bc76-e19fb729c37b'}, page_content='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'),\n",
       " Document(metadata={'doc_id': '9e4dd23d-9d21-4ee8-a6a1-4be3fb875f58', 'source': '2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries.pdf', 'type': 'table', 'paper_id': 'a5cdaa51-39b4-42fe-bc76-e19fb729c37b'}, page_content='<table><tr><td>Models</td><td>Accuracy</td></tr><tr><td>Retrieved Chunk</td><td>~Ground-truth Chunk</td></tr><tr><td>GPT-4</td><td>0.56</td><td>0.89</td></tr><tr><td>ChatGPT</td><td>0.44</td><td>0.57</td></tr><tr><td>Llama-2-70b-chat-hf</td><td>0.28</td><td>0.32</td></tr><tr><td>Mixtral-8x7B-Instruct</td><td>0.32</td><td>0.36</td></tr><tr><td>Claude-2.1</td><td>0.52</td><td>0.56</td></tr><tr><td>Google-PaLM</td><td>0.47</td><td>0.74</td></tr></table>'),\n",
       " Document(metadata={'doc_id': '1f131d66-bc00-4202-888e-ee02bf909761', 'source': '2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries.pdf', 'type': 'table', 'paper_id': 'a5cdaa51-39b4-42fe-bc76-e19fb729c37b'}, page_content='<table><tr><td>Embedding</td><td/><td>Without</td><td>Reranker</td><td/><td>With bge-reranker-large</td></tr><tr><td>MRR@10</td><td>MAP@10</td><td>Hits@10</td><td>Hits@4</td><td>MRR@10</td><td>MAP@10</td><td>Hits@10</td><td>Hits@4</td></tr><tr><td>text-embedding-ada-002</td><td>0.4203</td><td>0.3431</td><td>0.6381</td><td>0.504</td><td>0.5477</td><td>0.4625</td><td>0.7059</td><td>0.6169</td></tr><tr><td>text-search-ada-query-001</td><td>0.4203</td><td>0.3431</td><td>0.6399</td><td>0.5031</td><td>0.5483</td><td>0.4625</td><td>0.7064</td><td>0.6174</td></tr><tr><td>Ilm-embedder</td><td>0.2558</td><td>0.1725</td><td>0.4499</td><td>0.3189</td><td>0.425</td><td>0.3059</td><td>0.5478</td><td>0.4756</td></tr><tr><td>bge-large-en-v1.5</td><td>0.4298</td><td>0.3423</td><td>0.6718</td><td>= 0.5221</td><td>0.563</td><td>0.4759</td><td>0.7183</td><td>0.6364</td></tr><tr><td>jina-embeddings-v2-base-en</td><td>0.0621</td><td>0.031</td><td>0.1479</td><td>0.0802</td><td>0.1412</td><td>0.0772</td><td>0.1909</td><td>0.1639</td></tr><tr><td>intfloat/e5-base-v2</td><td>0.1843</td><td>0.1161</td><td>0.3556</td><td>= 0.2334</td><td>0.3237</td><td>0.2165</td><td>0.4176</td><td>0.3716</td></tr><tr><td>voyage-02</td><td>0.3934</td><td>0.3143</td><td>0.6506</td><td>0.4619</td><td>0.586</td><td>0.4795</td><td>0.7467</td><td>0.6625</td></tr><tr><td>hkun!p/instructor-large</td><td>0.3458</td><td>0.265</td><td>0.5717</td><td>0.4229</td><td>0.5115</td><td>0.4118</td><td>0.659</td><td>0.5775</td></tr></table>'),\n",
       " Document(metadata={'doc_id': '58e493de-a66f-4e2b-bc38-1014fbf33436', 'source': '2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries.pdf', 'type': 'text', 'paper_id': 'a5cdaa51-39b4-42fe-bc76-e19fb729c37b'}, page_content='4\\n\\n2024\\n\\n2\\n\\n0\\n\\n2\\n\\nn a J 7 2 ] L C . s c [ 1 v 1 9 3 5 1 . 1 0 4 2\\n\\n:\\n\\nv\\n\\ni\\n\\nX\\n\\nr\\n\\na\\n\\nMultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries\\n\\nYixuan Tang and Yi Yang Hong Kong University of Science and Technology {yixuantang,imyiyang}@ust.hk\\n\\nAbstract\\n\\nRetrieval-augmented generation (RAG) aug-\\n\\nments large language models (LLM) by re- trieving relevant knowledge, showing promis- ing potential in mitigating LLM hallucinations and enhancing response quality, thereby facil- itating the great adoption of LLMs in prac- tice. However, we find that existing RAG sys- tems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence. Furthermore, to our knowledge, no existing RAG benchmarking dataset focuses on multi- hop queries. In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a knowledge base, a large collection of multi- hop queries, their ground-truth answers, and the associated supporting evidence. We detail the procedure of building the dataset, utiliz- ing an English news article dataset as the un- derlying RAG knowledge base. We demon- strate the benchmarking utility of MultiHop- RAG in two experiments. The first experiment compares different embedding models for re- trieving evidence for multi-hop queries. In the second experiment, we examine the capabili- ties of various state-of-the-art LLMs, includ- ing GPT-4, PaLM, and Llama2-70B, in rea- soning and answering multi-hop queries given the evidence. Both experiments reveal that ex- isting RAG methods perform unsatisfactorily in retrieving and answering multi-hop queries. We hope MultiHop-RAG will be a valuable re- source for the community in developing effec- tive RAG systems, thereby facilitating greater adoption of LLMs in practice. The MultiHop- RAG and implemented RAG system is publicly available at https://github.com/yixuantt/ MultiHop-RAG/.\\n\\nIntroduction\\n\\nThe emergence of large language models (LLMs), such as ChatGPT, has fostered a wide range of inno- vations, powering intelligent chatbots and other nat- ural language processing (NLP) applications (Ope-\\n\\nMulti-Documents Which company among \\' | Google, Apple, and Nvidia Google Chunk]| ! | reported the largest profit 4 \\' | margins in their third- nae Chunk]) (1 1 | quarter reports for 2023? q Database” || Nvidia -—[Chunk]} ) |\\n\\nFigure 1: RAG with multi-hop query.\\n\\nnAI, 2023). One promising use case is Retrieval- Augmented Generation (RAG) (Asai et al., 2023), which optimizes the output of a large language model by referencing an external knowledge base outside of the LLM training data sources before generating a response. RAG improves LLM’s re- sponse (Borgeaud et al., 2022) and also mitigates the occurrence of hallucinations, thereby enhancing the models’ credibility (Gao et al., 2023). LLM- based frameworks, such as LlamaIndex (Liu, 2022) and LangChain (Chase, 2022), specialize in sup- porting RAG pipelines.\\n\\nIn real-world Retrieval-Augmented Generation (RAG) applications, a user’s query often necessi- tates retrieving and reasoning over evidence from multiple documents, a process known as multi-hop query. For instance, consider financial analysis us- ing a database of financial reports. A financial ana- lyst might query, Which company among Google, Apple, and Nvidia reported the largest profit mar- gins in their third-quarter reports for 2023? or inquire about a specific company’s performance over time, such as How does Apple’s sales trend look over the past three years? These queries re- quire evidence from multiple documents to formu- late an answer. Due to the multifaceted nature of such queries, involving information from various sources, traditional similarity matching methods like cosine similarity between query and financial Answer\\n\\nYes\\n\\nTable 1: An example of a multi-hop query, including supporting evidence from two news articles, the paraphrased claim, the bridge-topic and bridge-entity, and the corresponding answer.\\n\\nreport chunk embeddings might not yield optimal results. We demonstrate this multi-hop retrieval process in Figure 1.\\n\\nHowever, existing RAG benchmarks, such as RGB (Chen et al., 2023) and RECALL (Liu et al., 2023), mainly evaluate a simple case where the an- swer of a query can be retrieved and solved using one single piece of evidence. None of these bench- marks assess the retrieval and reasoning capability of LLMs for complex multi-hop queries. To ad- dress this gap and make RAG benchmarking more closely resemble real-world scenarios, in this paper, we introduce MultiHop-RAG. To our knowledge, MultiHop-RAG is one of the first RAG datasets focusing specifically on multi-hop queries.\\n\\nBased on the RAG queries commonly encoun- tered in real-world scenarios, we first categorize multi-hop queries into four types: Inference query, Comparison query, Temporal query, and Null query. The first three types — Inference, Com- parison, and Temporal — require the retrieval and analysis of evidence from multiple sources, encom- passing tasks like inferring relationships, compar- ing data points, and sequencing events over time. The Null query represents a scenario where the query cannot be derived from the knowledge base. This category is crucial for assessing whether an LLM might hallucinate an answer to a multi-hop query when the retrieved text lacks relevance.\\n\\nWe construct our RAG knowledge base using a collection of news articles. Using GPT-4 as a data generator, we then take an extensive procedure to construct a diverse set of multi-hop queries, each requiring the retrieval and reasoning over multiple documents. An example of query construction is shown in Table 1. First, we begin by extracting\\n\\nfactual sentences from each news article as evi-\\n\\ndence. For example, an extracted piece of evidence from an article may state: “Back then, just like today, home prices had boomed for years before Fed officials were ultimately forced to hike interest rates aggressively in an attempt to fight inflation.” Second, we input each evidence piece into GPT-4, prompting it to rephrase the evidence into a claim. This claim is clarified with a disambiguated topic and entity. For instance, GPT-4 might rephrase the aforementioned evidence into: “Federal Reserve officials were forced to aggressively hike interest rates to combat inflation after years of booming home prices”, identifying “Interest rate hikes to combat inflation” as the topic and “Federal Re- serve” as the entity. These topics and entities act as bridges for constructing multi-hop queries, known as bridge-topic or bridge-entity. Next, we use GPT- 4 to generate specific multi-hop queries related to the same bridge-topic or bridge-entity, accompa- nied by the correct answers. Lastly, we undertake a validation step to ensure the data quality.\\n\\nWe demonstrate the benchmarking capabilities of MultiHop-RAG using two experiments, utilizing a RAG system implemented with LlamaIndex (Liu, 2022). The first experiment involves a comparison of different embedding models for retrieving rele- vant evidence for multi-hop queries. In the second experiment, we assess the reasoning and answering abilities of various state-of-the-art LLMs, including GPT-4, GPT-3.5, PaLM, Claude-2, Llama2-70B, and Mixtral-8x7B, for multi-hop queries when re- trieved text is provided. The results from both ex- periments indicate that the current RAG implemen- tations are inadequate for effectively retrieving and answering multi-hop queries. We publicly release\\n\\nthis challenging MultiHop-RAG dataset and hope it will be a valuable resource for the community in de- veloping and benchmarking RAG systems, thereby unleashing the great potential of generative AI in practice. 2 RAG with multi-Hop queries\\n\\n2.1 Retrieval-augmented Generation (RAG)\\n\\nIn an RAG application, we utilize an external cor- pus, denoted as D, which comprises multiple docu- ments and serves as the knowledge base. Each doc- ument within this corpus, represented as di ∈ D, is segmented into a set of chunks.These chunks are then transformed into vector representations using an embedding model and stored in an embedding database. Given a user query q, the system typi- cally retrieves the top-K chunks that best match the query. These chunks constitute the retrieval set for query q, represented as Rq = {r1, r2, ..., rK}. The retrieved chunks, combined with the query and an optional prompt, are then fed into an LLM to generate a final answer, following the format: LLM(q, Rq, prompt) → answer.\\n\\n2.2 Multi-Hop Query\\n\\nWe define a multi-hop query as one that requires retrieving and reasoning over multiple pieces of supporting evidence to provide an answer. In other words, for a multi-hop query q, the chunks in the retrieval set Rq collectively provide an answer to q. For example, the query \"Which company among Google, Apple, and Nvidia reported the largest profit margins in their third-quarter reports for 2023?\" requires 1) retrieving relevant pieces of evidence related to profit margins from the reports of the three companies; 2) generating an answer by comparing and reasoning from the multiple pieces of retrieved evidence. This differs from a single- hop query such as \"What is Google’s profit margin in the third-quarter reports for 2023,\" where the answer can be directly derived from a single piece of evidence.\\n\\nBased on the queries commonly used in real- world RAG systems, we identify four types of multi-hop queries. For each type, we present a hypothetical query within the context of a financial RAG system, where the knowledge base consists of a collection of annual reports.\\n\\nInference query: For such a query q, the answer is deduced through reasoning from the retrieval set Rq. An example of an inference query might\\n\\nbe: Which report discusses the supply chain risk of Apple, the 2019 annual report or the 2020 annual report?\\n\\nComparison query: For such a query q, the an- swer requires a comparison of evidence within the retrieval set Rq. For instance, a comparison query might ask: Did Netflix or Google report higher revenue for the year 2023?\"\\n\\nTemporal query: For such a query q, the answer requires an analysis of the temporal information of the retrieved chunks. For example, a temporal query may ask: Did Apple introduce the AirTag tracking device before or after the launch of the 5th generation iPad Pro?\\n\\nNull query: For such as query q, the answer cannot be derived from the retrieved set Rq. We include the null query to assess the generation quality, es- pecially regarding the issue of hallucination. For a null query, even though a retrieved set is provided, an LLM should produce a null response instead of hallucinating an answer. For example, assum- ing ABCD is a non-existent company, a null query might ask: What are the sales of company ABCD as reported in its 2022 and 2023 annual reports? 2.3 Evaluation Metrics\\n\\nAn RAG system handling multi-hop queries can be assessed from two key aspects: retrieval evaluation and generation evaluation.\\n\\nRetrieval Evaluation: Evidently, the quality of the retrieval set Rq determines the final genera- tion quality. We compare the retrieved set with the ground truth evidence associated with each query, except for the null queries, as they have no evidence to derive from. Assuming the top- K chunks are retrieved, i.e., |Rq| = K, we use retrieval evaluation metrics including Mean Aver- age Precision at K (MAP@K), Mean Reciprocal Rank at K (MRR@K), and Hit Rate at K (Hit@K). MAP@K measures the average top-K retrieval pre- cision across all queries. MRR@K calculates the average of the reciprocal ranks of the first relevant chunk for each query, considering the top-K re- trieved set. Hit@K metric measures the fraction of evidence that appears in the top-K retrieved set.\\n\\nResponse Evaluation: Since the multi-hop query requires reasoning over multiple pieces of retrieved chunks, we can also evaluate the reason- ing capability of the LLM by comparing the LLM response with the ground truth answer of the query.\\n\\n| Download Dataset |! Dataset Collection | ---6 —————_ | | Select News | Extraction | Select Sentences | | Claim Generation . . ooo Claim Generation | --0| [pridge-Entity Generation | | Bridge-Topic Generation OO = Query and Answer all Inference || Comparison} | Generation ~ | Null Temporal ji | Manually Review | Quality Assurance +--Q = | | GPT-4 Review I\\n\\nFigure 2: MultiHop-RAG Construction Pipeline.\\n\\n3 A Benchmarking Dataset: MultiHop-RAG\\n\\nIn this section, we provide detailed information on the construction of the MultiHop-RAG dataset. Specifically, we describe the process of creating a set of multi-hop queries, along with the correspond- ing ground truth evidence sets and answers derived from a collection of news articles. 3.1 MultiHop-RAG Construction\\n\\nStep 1: Dataset Collection. We download a news dataset using the mediastack API 1, a REST API in- terface delivering worldwide news data. The news data source comprises various English-language websites covering a range of news categories: en- tertainment, business, sports, technology, health, and science. To mimic real-world RAG scenarios, where the knowledge base data, such as an enter- prise’s internal data, may differ from the LLMs’ training data, we select news articles published from September 26, 2023, to December 26, 2023. This timeframe extends beyond the knowledge cut- off of some widely-used LLMs, including Chat- GPT and LLaMA, as of the time of writing. This selection also helps in teasing out the possibility of the underlying LLM having been exposed to these news articles. We only keep articles with a token length greater than or equal to 1,024. Every\\n\\n1https://mediastack.com/\\n\\nnews article is paired with metadata, including the title, publish date, author, category, URL, and news source.\\n\\nStep 2: Evidence Extraction. For each article, we extract factual or opinion sentences using a trained language model 2. These factual sentences are later used as evidence for answering multi-hop queries. We retain only those news articles containing ev- idence that may have overlapping keywords with other news articles. This allows us to later create multi-hop queries where the answer’s evidences are drawn from multiple sources.\\n\\nStep 3: Claim, Bridge-Entity, Bridge-Topic Gen- eration. Our goal is to use GPT-4 to automatically generate high-quality multi-hop queries using the evidence set. However, the raw evidence obtained from Step 2 is not ideal for query generation due to inconsistency in linguistic structure. For exam- ple, some pieces of evidence use pronouns to refer to subjects and lack the actual entity in the text. To address this, we employ GPT-4 to paraphrase the evidence, which we refer to as claims, given the original evidence and its context. To ensure consistency between the generated claim and the evidence, we further perform fact-checking using the UniEval (Zhong et al., 2022) framework to ver- ify the alignment between the evidence and claim. Appendix A presents the prompt used for GPT-4 for claim generation.\\n\\nBridge-Entity and Bridge-Topic: The shared en- tity or topic across pieces of evidence is referred to as the bridge-entity or bridge-topic. These bridge- entities or bridge-topics can be used to link dif- ferent pieces of evidence from which a multi-hop query’s answer is derived. For example, in a claim such as “Google reports its third-quarter results for 2023, showcasing a detailed overview of its finan- cial performance, including revenue growth, profit margins”, the term profit margin can be viewed as a bridge-topic and the term Google can be viewed as a bridge-entity that links the different pieces of evidence. We prompt GPT-4 to identify the bridge- entity and bridge-topic for each claim. Appendix A also presents the prompt used for GPT-4 for bridge generation.\\n\\nStep 4: Query and Answer Generation. In this step, we leverage the bridge-entity or bridge-topic to generate multi-hop queries. Specifically, we first group the claims having the same bridge-entity or\\n\\n2https://huggingface.co/lighteternal/fact-or-opinion-xlmr- el\\n\\nbridge-topic into a claim set. We restrict the claim set to have at least two claims but no more than four claims. For each type of query, we feed the claim set to GPT-4 and prompt it with an instruction to generate a query with information from each claim. Below, we explain the specifications for different multi-hop query types. In the construction of each query, we also include the source of the news article where the supporting evidence is associated with to mimic real-world RAG scenarios. Appendix A presents the prompts used for GPT-4 for query generation.\\n\\nInference Query: These queries are formulated by synthesizing the various characterizations of the bridge-entity across multiple claims, with the final answer being the identification of the entity itself.\\n\\nComparison Query: These queries are struc- tured to compare the similarities and differences related to the bridge entity or topic. The resultant answer to such queries is typically a definitive “yes” or “no”, based on the comparison.\\n\\nTemporal Query: These queries explore the temporal ordering of events across different points in time. The answer to such queries is typically a “yes” or “no” or a single temporal indicator word like “before” or “after”.')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/jpeg;base64,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\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Models</td><td>Accuracy</td></tr><tr><td>Retrieved Chunk</td><td>~Ground-truth Chunk</td></tr><tr><td>GPT-4</td><td>0.56</td><td>0.89</td></tr><tr><td>ChatGPT</td><td>0.44</td><td>0.57</td></tr><tr><td>Llama-2-70b-chat-hf</td><td>0.28</td><td>0.32</td></tr><tr><td>Mixtral-8x7B-Instruct</td><td>0.32</td><td>0.36</td></tr><tr><td>Claude-2.1</td><td>0.52</td><td>0.56</td></tr><tr><td>Google-PaLM</td><td>0.47</td><td>0.74</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Embedding</td><td/><td>Without</td><td>Reranker</td><td/><td>With bge-reranker-large</td></tr><tr><td>MRR@10</td><td>MAP@10</td><td>Hits@10</td><td>Hits@4</td><td>MRR@10</td><td>MAP@10</td><td>Hits@10</td><td>Hits@4</td></tr><tr><td>text-embedding-ada-002</td><td>0.4203</td><td>0.3431</td><td>0.6381</td><td>0.504</td><td>0.5477</td><td>0.4625</td><td>0.7059</td><td>0.6169</td></tr><tr><td>text-search-ada-query-001</td><td>0.4203</td><td>0.3431</td><td>0.6399</td><td>0.5031</td><td>0.5483</td><td>0.4625</td><td>0.7064</td><td>0.6174</td></tr><tr><td>Ilm-embedder</td><td>0.2558</td><td>0.1725</td><td>0.4499</td><td>0.3189</td><td>0.425</td><td>0.3059</td><td>0.5478</td><td>0.4756</td></tr><tr><td>bge-large-en-v1.5</td><td>0.4298</td><td>0.3423</td><td>0.6718</td><td>= 0.5221</td><td>0.563</td><td>0.4759</td><td>0.7183</td><td>0.6364</td></tr><tr><td>jina-embeddings-v2-base-en</td><td>0.0621</td><td>0.031</td><td>0.1479</td><td>0.0802</td><td>0.1412</td><td>0.0772</td><td>0.1909</td><td>0.1639</td></tr><tr><td>intfloat/e5-base-v2</td><td>0.1843</td><td>0.1161</td><td>0.3556</td><td>= 0.2334</td><td>0.3237</td><td>0.2165</td><td>0.4176</td><td>0.3716</td></tr><tr><td>voyage-02</td><td>0.3934</td><td>0.3143</td><td>0.6506</td><td>0.4619</td><td>0.586</td><td>0.4795</td><td>0.7467</td><td>0.6625</td></tr><tr><td>hkun!p/instructor-large</td><td>0.3458</td><td>0.265</td><td>0.5717</td><td>0.4229</td><td>0.5115</td><td>0.4118</td><td>0.659</td><td>0.5775</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  4\n",
      "\n",
      "2024\n",
      "\n",
      "2\n",
      "\n",
      "0\n",
      "\n",
      "2\n",
      "\n",
      "n a J 7 2 ] L C . s c [ 1 v 1 9 3 5 1 . 1 0 4 2\n",
      "\n",
      ":\n",
      "\n",
      "v\n",
      "\n",
      "i\n",
      "\n",
      "X\n",
      "\n",
      "r\n",
      "\n",
      "a\n",
      "\n",
      "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries\n",
      "\n",
      "Yixuan Tang and Yi Yang Hong Kong University of Science and Technology {yixuantang,imyiyang}@ust.hk\n",
      "\n",
      "Abstract\n",
      "\n",
      "Retrieval-augmented generation (RAG) aug-\n",
      "\n",
      "ments large language models (LLM) by re- trieving relevant knowledge, showing promis- ing potential in mitigating LLM hallucinations and enhancing response quality, thereby facil- itating the great adoption of LLMs in prac- tice. However, we find that existing RAG sys- tems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence. Furthermore, to our knowledge, no existing RAG benchmarking dataset focuses on multi- hop queries. In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a knowledge base, a large collection of multi- hop queries, their ground-truth answers, and the associated supporting evidence. We detail the procedure of building the dataset, utiliz- ing an English news article dataset as the un- derlying RAG knowledge base. We demon- strate the benchmarking utility of MultiHop- RAG in two experiments. The first experiment compares different embedding models for re- trieving evidence for multi-hop queries. In the second experiment, we examine the capabili- ties of various state-of-the-art LLMs, includ- ing GPT-4, PaLM, and Llama2-70B, in rea- soning and answering multi-hop queries given the evidence. Both experiments reveal that ex- isting RAG methods perform unsatisfactorily in retrieving and answering multi-hop queries. We hope MultiHop-RAG will be a valuable re- source for the community in developing effec- tive RAG systems, thereby facilitating greater adoption of LLMs in practice. The MultiHop- RAG and implemented RAG system is publicly available at https://github.com/yixuantt/ MultiHop-RAG/.\n",
      "\n",
      "Introduction\n",
      "\n",
      "The emergence of large language models (LLMs), such as ChatGPT, has fostered a wide range of inno- vations, powering intelligent chatbots and other nat- ural language processing (NLP) applications (Ope-\n",
      "\n",
      "Multi-Documents Which company among ' | Google, Apple, and Nvidia Google Chunk]| ! | reported the largest profit 4 ' | margins in their third- nae Chunk]) (1 1 | quarter reports for 2023? q Database” || Nvidia -—[Chunk]} ) |\n",
      "\n",
      "Figure 1: RAG with multi-hop query.\n",
      "\n",
      "nAI, 2023). One promising use case is Retrieval- Augmented Generation (RAG) (Asai et al., 2023), which optimizes the output of a large language model by referencing an external knowledge base outside of the LLM training data sources before generating a response. RAG improves LLM’s re- sponse (Borgeaud et al., 2022) and also mitigates the occurrence of hallucinations, thereby enhancing the models’ credibility (Gao et al., 2023). LLM- based frameworks, such as LlamaIndex (Liu, 2022) and LangChain (Chase, 2022), specialize in sup- porting RAG pipelines.\n",
      "\n",
      "In real-world Retrieval-Augmented Generation (RAG) applications, a user’s query often necessi- tates retrieving and reasoning over evidence from multiple documents, a process known as multi-hop query. For instance, consider financial analysis us- ing a database of financial reports. A financial ana- lyst might query, Which company among Google, Apple, and Nvidia reported the largest profit mar- gins in their third-quarter reports for 2023? or inquire about a specific company’s performance over time, such as How does Apple’s sales trend look over the past three years? These queries re- quire evidence from multiple documents to formu- late an answer. Due to the multifaceted nature of such queries, involving information from various sources, traditional similarity matching methods like cosine similarity between query and financial Answer\n",
      "\n",
      "Yes\n",
      "\n",
      "Table 1: An example of a multi-hop query, including supporting evidence from two news articles, the paraphrased claim, the bridge-topic and bridge-entity, and the corresponding answer.\n",
      "\n",
      "report chunk embeddings might not yield optimal results. We demonstrate this multi-hop retrieval process in Figure 1.\n",
      "\n",
      "However, existing RAG benchmarks, such as RGB (Chen et al., 2023) and RECALL (Liu et al., 2023), mainly evaluate a simple case where the an- swer of a query can be retrieved and solved using one single piece of evidence. None of these bench- marks assess the retrieval and reasoning capability of LLMs for complex multi-hop queries. To ad- dress this gap and make RAG benchmarking more closely resemble real-world scenarios, in this paper, we introduce MultiHop-RAG. To our knowledge, MultiHop-RAG is one of the first RAG datasets focusing specifically on multi-hop queries.\n",
      "\n",
      "Based on the RAG queries commonly encoun- tered in real-world scenarios, we first categorize multi-hop queries into four types: Inference query, Comparison query, Temporal query, and Null query. The first three types — Inference, Com- parison, and Temporal — require the retrieval and analysis of evidence from multiple sources, encom- passing tasks like inferring relationships, compar- ing data points, and sequencing events over time. The Null query represents a scenario where the query cannot be derived from the knowledge base. This category is crucial for assessing whether an LLM might hallucinate an answer to a multi-hop query when the retrieved text lacks relevance.\n",
      "\n",
      "We construct our RAG knowledge base using a collection of news articles. Using GPT-4 as a data generator, we then take an extensive procedure to construct a diverse set of multi-hop queries, each requiring the retrieval and reasoning over multiple documents. An example of query construction is shown in Table 1. First, we begin by extracting\n",
      "\n",
      "factual sentences from each news article as evi-\n",
      "\n",
      "dence. For example, an extracted piece of evidence from an article may state: “Back then, just like today, home prices had boomed for years before Fed officials were ultimately forced to hike interest rates aggressively in an attempt to fight inflation.” Second, we input each evidence piece into GPT-4, prompting it to rephrase the evidence into a claim. This claim is clarified with a disambiguated topic and entity. For instance, GPT-4 might rephrase the aforementioned evidence into: “Federal Reserve officials were forced to aggressively hike interest rates to combat inflation after years of booming home prices”, identifying “Interest rate hikes to combat inflation” as the topic and “Federal Re- serve” as the entity. These topics and entities act as bridges for constructing multi-hop queries, known as bridge-topic or bridge-entity. Next, we use GPT- 4 to generate specific multi-hop queries related to the same bridge-topic or bridge-entity, accompa- nied by the correct answers. Lastly, we undertake a validation step to ensure the data quality.\n",
      "\n",
      "We demonstrate the benchmarking capabilities of MultiHop-RAG using two experiments, utilizing a RAG system implemented with LlamaIndex (Liu, 2022). The first experiment involves a comparison of different embedding models for retrieving rele- vant evidence for multi-hop queries. In the second experiment, we assess the reasoning and answering abilities of various state-of-the-art LLMs, including GPT-4, GPT-3.5, PaLM, Claude-2, Llama2-70B, and Mixtral-8x7B, for multi-hop queries when re- trieved text is provided. The results from both ex- periments indicate that the current RAG implemen- tations are inadequate for effectively retrieving and answering multi-hop queries. We publicly release\n",
      "\n",
      "this challenging MultiHop-RAG dataset and hope it will be a valuable resource for the community in de- veloping and benchmarking RAG systems, thereby unleashing the great potential of generative AI in practice. 2 RAG with multi-Hop queries\n",
      "\n",
      "2.1 Retrieval-augmented Generation (RAG)\n",
      "\n",
      "In an RAG application, we utilize an external cor- pus, denoted as D, which comprises multiple docu- ments and serves as the knowledge base. Each doc- ument within this corpus, represented as di ∈ D, is segmented into a set of chunks.These chunks are then transformed into vector representations using an embedding model and stored in an embedding database. Given a user query q, the system typi- cally retrieves the top-K chunks that best match the query. These chunks constitute the retrieval set for query q, represented as Rq = {r1, r2, ..., rK}. The retrieved chunks, combined with the query and an optional prompt, are then fed into an LLM to generate a final answer, following the format: LLM(q, Rq, prompt) → answer.\n",
      "\n",
      "2.2 Multi-Hop Query\n",
      "\n",
      "We define a multi-hop query as one that requires retrieving and reasoning over multiple pieces of supporting evidence to provide an answer. In other words, for a multi-hop query q, the chunks in the retrieval set Rq collectively provide an answer to q. For example, the query \"Which company among Google, Apple, and Nvidia reported the largest profit margins in their third-quarter reports for 2023?\" requires 1) retrieving relevant pieces of evidence related to profit margins from the reports of the three companies; 2) generating an answer by comparing and reasoning from the multiple pieces of retrieved evidence. This differs from a single- hop query such as \"What is Google’s profit margin in the third-quarter reports for 2023,\" where the answer can be directly derived from a single piece of evidence.\n",
      "\n",
      "Based on the queries commonly used in real- world RAG systems, we identify four types of multi-hop queries. For each type, we present a hypothetical query within the context of a financial RAG system, where the knowledge base consists of a collection of annual reports.\n",
      "\n",
      "Inference query: For such a query q, the answer is deduced through reasoning from the retrieval set Rq. An example of an inference query might\n",
      "\n",
      "be: Which report discusses the supply chain risk of Apple, the 2019 annual report or the 2020 annual report?\n",
      "\n",
      "Comparison query: For such a query q, the an- swer requires a comparison of evidence within the retrieval set Rq. For instance, a comparison query might ask: Did Netflix or Google report higher revenue for the year 2023?\"\n",
      "\n",
      "Temporal query: For such a query q, the answer requires an analysis of the temporal information of the retrieved chunks. For example, a temporal query may ask: Did Apple introduce the AirTag tracking device before or after the launch of the 5th generation iPad Pro?\n",
      "\n",
      "Null query: For such as query q, the answer cannot be derived from the retrieved set Rq. We include the null query to assess the generation quality, es- pecially regarding the issue of hallucination. For a null query, even though a retrieved set is provided, an LLM should produce a null response instead of hallucinating an answer. For example, assum- ing ABCD is a non-existent company, a null query might ask: What are the sales of company ABCD as reported in its 2022 and 2023 annual reports? 2.3 Evaluation Metrics\n",
      "\n",
      "An RAG system handling multi-hop queries can be assessed from two key aspects: retrieval evaluation and generation evaluation.\n",
      "\n",
      "Retrieval Evaluation: Evidently, the quality of the retrieval set Rq determines the final genera- tion quality. We compare the retrieved set with the ground truth evidence associated with each query, except for the null queries, as they have no evidence to derive from. Assuming the top- K chunks are retrieved, i.e., |Rq| = K, we use retrieval evaluation metrics including Mean Aver- age Precision at K (MAP@K), Mean Reciprocal Rank at K (MRR@K), and Hit Rate at K (Hit@K). MAP@K measures the average top-K retrieval pre- cision across all queries. MRR@K calculates the average of the reciprocal ranks of the first relevant chunk for each query, considering the top-K re- trieved set. Hit@K metric measures the fraction of evidence that appears in the top-K retrieved set.\n",
      "\n",
      "Response Evaluation: Since the multi-hop query requires reasoning over multiple pieces of retrieved chunks, we can also evaluate the reason- ing capability of the LLM by comparing the LLM response with the ground truth answer of the query.\n",
      "\n",
      "| Download Dataset |! Dataset Collection | ---6 —————_ | | Select News | Extraction | Select Sentences | | Claim Generation . . ooo Claim Generation | --0| [pridge-Entity Generation | | Bridge-Topic Generation OO = Query and Answer all Inference || Comparison} | Generation ~ | Null Temporal ji | Manually Review | Quality Assurance +--Q = | | GPT-4 Review I\n",
      "\n",
      "Figure 2: MultiHop-RAG Construction Pipeline.\n",
      "\n",
      "3 A Benchmarking Dataset: MultiHop-RAG\n",
      "\n",
      "In this section, we provide detailed information on the construction of the MultiHop-RAG dataset. Specifically, we describe the process of creating a set of multi-hop queries, along with the correspond- ing ground truth evidence sets and answers derived from a collection of news articles. 3.1 MultiHop-RAG Construction\n",
      "\n",
      "Step 1: Dataset Collection. We download a news dataset using the mediastack API 1, a REST API in- terface delivering worldwide news data. The news data source comprises various English-language websites covering a range of news categories: en- tertainment, business, sports, technology, health, and science. To mimic real-world RAG scenarios, where the knowledge base data, such as an enter- prise’s internal data, may differ from the LLMs’ training data, we select news articles published from September 26, 2023, to December 26, 2023. This timeframe extends beyond the knowledge cut- off of some widely-used LLMs, including Chat- GPT and LLaMA, as of the time of writing. This selection also helps in teasing out the possibility of the underlying LLM having been exposed to these news articles. We only keep articles with a token length greater than or equal to 1,024. Every\n",
      "\n",
      "1https://mediastack.com/\n",
      "\n",
      "news article is paired with metadata, including the title, publish date, author, category, URL, and news source.\n",
      "\n",
      "Step 2: Evidence Extraction. For each article, we extract factual or opinion sentences using a trained language model 2. These factual sentences are later used as evidence for answering multi-hop queries. We retain only those news articles containing ev- idence that may have overlapping keywords with other news articles. This allows us to later create multi-hop queries where the answer’s evidences are drawn from multiple sources.\n",
      "\n",
      "Step 3: Claim, Bridge-Entity, Bridge-Topic Gen- eration. Our goal is to use GPT-4 to automatically generate high-quality multi-hop queries using the evidence set. However, the raw evidence obtained from Step 2 is not ideal for query generation due to inconsistency in linguistic structure. For exam- ple, some pieces of evidence use pronouns to refer to subjects and lack the actual entity in the text. To address this, we employ GPT-4 to paraphrase the evidence, which we refer to as claims, given the original evidence and its context. To ensure consistency between the generated claim and the evidence, we further perform fact-checking using the UniEval (Zhong et al., 2022) framework to ver- ify the alignment between the evidence and claim. Appendix A presents the prompt used for GPT-4 for claim generation.\n",
      "\n",
      "Bridge-Entity and Bridge-Topic: The shared en- tity or topic across pieces of evidence is referred to as the bridge-entity or bridge-topic. These bridge- entities or bridge-topics can be used to link dif- ferent pieces of evidence from which a multi-hop query’s answer is derived. For example, in a claim such as “Google reports its third-quarter results for 2023, showcasing a detailed overview of its finan- cial performance, including revenue growth, profit margins”, the term profit margin can be viewed as a bridge-topic and the term Google can be viewed as a bridge-entity that links the different pieces of evidence. We prompt GPT-4 to identify the bridge- entity and bridge-topic for each claim. Appendix A also presents the prompt used for GPT-4 for bridge generation.\n",
      "\n",
      "Step 4: Query and Answer Generation. In this step, we leverage the bridge-entity or bridge-topic to generate multi-hop queries. Specifically, we first group the claims having the same bridge-entity or\n",
      "\n",
      "2https://huggingface.co/lighteternal/fact-or-opinion-xlmr- el\n",
      "\n",
      "bridge-topic into a claim set. We restrict the claim set to have at least two claims but no more than four claims. For each type of query, we feed the claim set to GPT-4 and prompt it with an instruction to generate a query with information from each claim. Below, we explain the specifications for different multi-hop query types. In the construction of each query, we also include the source of the news article where the supporting evidence is associated with to mimic real-world RAG scenarios. Appendix A presents the prompts used for GPT-4 for query generation.\n",
      "\n",
      "Inference Query: These queries are formulated by synthesizing the various characterizations of the bridge-entity across multiple claims, with the final answer being the identification of the entity itself.\n",
      "\n",
      "Comparison Query: These queries are struc- tured to compare the similarities and differences related to the bridge entity or topic. The resultant answer to such queries is typically a definitive “yes” or “no”, based on the comparison.\n",
      "\n",
      "Temporal Query: These queries explore the temporal ordering of events across different points in time. The answer to such queries is typically a “yes” or “no” or a single temporal indicator word like “before” or “after”.\n"
     ]
    }
   ],
   "source": [
    "for d in relevant_docs:\n",
    "    if d.metadata[\"type\"] == \"image\":\n",
    "        plt_img_base64(d.page_content)\n",
    "    elif d.metadata[\"type\"] == \"table\":\n",
    "        display(HTML(d.page_content))\n",
    "    else:\n",
    "        print(\"Text: \",d.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bar charts comparing accuracy percentages of Mixtral-8x7B and GPT-4 models across categories: null, comparison, temporal, and inference. The top chart is for \"Retrieved Chunk,\" and the bottom is \"Ground-truth Chunk.\" GPT-4 generally shows higher accuracy.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_summaries[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Reranking and Document Selection (Leave this to the MultiModal Retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Augmented Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to write the prompt. It will basically instruct the LLM to generate result based on the {question} and the {context}.\n",
    "\n",
    "The context is inputted from the retrieved documents from p previous step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "QA_RAG = \"SIMPLE_QUESTION_ANSWER_RAG\"\n",
    "\n",
    "MM_QA_RAG = \"MULTIMODAL_QUESTION_ANSWER_RAG\"\n",
    "\n",
    "prompt_type = {\n",
    "    \"QA_RAG\" : \"SIMPLE_QUESTION_ANSWER_RAG\",\n",
    "    \"MM_QA_RAG\" : \"MULTIMODAL_QUESTION_ANSWER_RAG\",\n",
    "}\n",
    "\n",
    "simple_rag_template = \"\"\"\n",
    "Answer the question based on the context below. \n",
    "If you can't answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "multimodal_rag_template = \"\"\"\n",
    "To define the new Prompt.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "def initPrompt(type) -> ChatPromptTemplate:\n",
    "    #default\n",
    "    prompt = ChatPromptTemplate.from_template(simple_rag_template)\n",
    "    if type == prompt_type[\"QA_RAG\"]: \n",
    "        prompt = ChatPromptTemplate.from_template(simple_rag_template)\n",
    "    if type == prompt_type[\"MM_QA_RAG\"]: \n",
    "        prompt = ChatPromptTemplate.from_template(multimodal_rag_template)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now send the augmented prompt to instruct a LLM generating response to user's query. The response is finally parsed for readable. \n",
    "In this experiment, we use OpenAI model GPT3.5-Turbo. \n",
    "\n",
    "Note: There are many options for LLMs selection, from public to private, from simple to advance. Privacy, performance and quality should be considered to trade off. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. QA Generation \n",
    "Using LLM to generation response to augmented query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG chain\n",
    "chain_multimodal_rag = multi_modal_rag_chain(retriever_multi_vector_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chain_multimodal_rag.invoke(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided data, here's a comparison of the performance of GPT-4 and Mixtral-8x7B:\\n\\n1. **Accuracy**:\\n   - **Retrieved Chunk**: GPT-4 shows significantly higher accuracy compared to Mixtral-8x7B across all query types (null, comparison, temporal, inference).\\n   - **Ground-truth Chunk**: GPT-4 also outperforms Mixtral-8x7B, especially in inference and comparison queries.\\n\\n2. **Performance Metrics**:\\n   - GPT-4 consistently achieves higher accuracy percentages in both retrieved and ground-truth chunks, indicating better retrieval and reasoning capabilities.\\n\\n3. **Investment Advice**:\\n   - **GPT-4**: Given its superior performance in accuracy and handling complex multi-hop queries, investing in technologies or applications leveraging GPT-4 could be more promising.\\n   - **Mixtral-8x7B**: While it shows some capability, its lower performance suggests it may be less competitive in applications requiring high accuracy and complex reasoning.\\n\\nOverall, GPT-4 demonstrates a clear advantage in performance, making it a more reliable choice for applications requiring advanced language processing and retrieval-augmented generation.\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Retrieve Topic and Relevant Articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_catalog = pd.DataFrame()\n",
    "with open('document_catalog.pickle', 'rb') as pkl_file:\n",
    "        doc_catalog = pickle.load(pkl_file)\n",
    "articles = list(set([a.metadata[\"paper_id\"] for a in relevant_docs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_catalog[\"topic\"] = [\"a\",\"b\",\"c\",\"c\"]\n",
    "doc_catalog[\"summary\"] = [\"1111\",\"222\",\"333\",\"3333\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = list(set([doc_catalog[\"topic\"].loc[i] for i, docid in enumerate(list(doc_catalog[\"docid\"])) if docid in articles]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_articles = [doc_catalog[\"filename\"].loc[i][:-4] for i, docid, topic in zip(range(len(doc_catalog.index)),list(doc_catalog[\"docid\"]),list(doc_catalog[\"topic\"])) if docid not in articles and topic in topics]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Retrieve Article Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_summary = []\n",
    "topic_articles_summary = []\n",
    "if articles:\n",
    "    articles_summary = [doc_catalog[\"summary\"].loc[i] for i, docid in enumerate(list(doc_catalog[\"docid\"])) if docid in articles]\n",
    "if topic_articles:\n",
    "    topic_articles_summary = [doc_catalog[\"summary\"].loc[i] for i, filename in enumerate(list(doc_catalog[\"filename\"])) if filename[:-4] in topic_articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Generate the final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Query: What is the pperformance of GPT-4 vs Mixtral?\n",
      "The answer: Based on the provided data, here's a comparison of the performance of GPT-4 and Mixtral-8x7B:\n",
      "\n",
      "1. **Accuracy**:\n",
      "   - **Retrieved Chunk**: GPT-4 shows significantly higher accuracy compared to Mixtral-8x7B across all query types (null, comparison, temporal, inference).\n",
      "   - **Ground-truth Chunk**: GPT-4 also outperforms Mixtral-8x7B, especially in inference and comparison queries.\n",
      "\n",
      "2. **Performance Metrics**:\n",
      "   - GPT-4 consistently achieves higher accuracy percentages in both retrieved and ground-truth chunks, indicating better retrieval and reasoning capabilities.\n",
      "\n",
      "3. **Investment Advice**:\n",
      "   - **GPT-4**: Given its superior performance in accuracy and handling complex multi-hop queries, investing in technologies or applications leveraging GPT-4 could be more promising.\n",
      "   - **Mixtral-8x7B**: While it shows some capability, its lower performance suggests it may be less competitive in applications requiring high accuracy and complex reasoning.\n",
      "\n",
      "Overall, GPT-4 demonstrates a clear advantage in performance, making it a more reliable choice for applications requiring advanced language processing and retrieval-augmented generation.\n",
      "\n",
      "You can find the details of the answer from the following articles\n",
      "\n",
      "Article 1: 2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries\n",
      "Article Summary:\n",
      "1111\n",
      "\n",
      "You seem interested in the topics: b \n",
      "You may be interested in other articles in those topics below:\n",
      "\n",
      "Article 1: 2407.21059v1.Modular_RAG__Transforming_RAG_Systems_into_LEGO_like_Reconfigurable_Frameworks\n",
      "Article Summary:\n",
      "222\n"
     ]
    }
   ],
   "source": [
    "print(\"Your Query:\", user_query)\n",
    "print(\"The answer:\", response)\n",
    "if articles:\n",
    "    print(\"\\nYou can find the details of the answer from the following articles\")\n",
    "    for i in range(len(articles)):\n",
    "        print(\"\\nArticle \"+str(i+1)+\": \"+ doc_catalog[doc_catalog[\"docid\"]==articles[i]][\"filename\"].loc[0][:-4])\n",
    "        print(\"Article Summary:\\n\"+articles_summary[i])\n",
    "if topic_articles:\n",
    "    print(\"\\nYou seem interested in the topics:\", \", \".join(topics),\"\\nYou may be interested in other articles in those topics below:\")\n",
    "    for i in range(len(topic_articles)):\n",
    "        print(\"\\nArticle \"+str(i+1)+\": \"+topic_articles[i])\n",
    "        print(\"Article Summary:\\n\"+topic_articles_summary[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Research Assistant Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration of Research Assistant for: \n",
    "- Answer queries\n",
    "- Relevant papers: from the query and from the topic\n",
    "- Summary of the recommanded papers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ism6564",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

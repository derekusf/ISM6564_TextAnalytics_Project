{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install neccessary Library\n",
    "The libraries include:\n",
    "- langchain framework'\n",
    "- GPT4ALL, OpenAI and HuggingFace for various embedding methods and LLMs\n",
    "- Document loaders\n",
    "- Dependent libraries\n",
    "\n",
    "__Note__ : \n",
    "- It requires C++ builder for building a dependant library for Chroma. Check out https://github.com/bycloudai/InstallVSBuildToolsWindows for instruction. \n",
    "- Python version: 3.12.4\n",
    "- Pydantic version: 2.7.3. There is issue with pydantic version 1.10.8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git (from -r requirements.txt (line 25))\n",
      "  Cloning https://github.com/openai/whisper.git to c:\\users\\derek\\appdata\\local\\temp\\pip-req-build-v0kwnef4\n",
      "  Resolved https://github.com/openai/whisper.git to commit 271445b2f24f00f8175c4fb7ae91876f7451dfc1\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: pydantic in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 1)) (2.9.2)\n",
      "Requirement already satisfied: openai in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 2)) (1.54.1)\n",
      "Collecting openai (from -r requirements.txt (line 2))\n",
      "  Downloading openai-1.54.3-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting langchain (from -r requirements.txt (line 3))\n",
      "  Using cached langchain-0.3.7-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain_community (from -r requirements.txt (line 4))\n",
      "  Using cached langchain_community-0.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting langchain-openai (from -r requirements.txt (line 5))\n",
      "  Downloading langchain_openai-0.2.6-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting langchain-ollama (from -r requirements.txt (line 6))\n",
      "  Using cached langchain_ollama-0.2.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting langchainhub (from -r requirements.txt (line 7))\n",
      "  Using cached langchainhub-0.1.21-py3-none-any.whl.metadata (659 bytes)\n",
      "Collecting langchain-chroma (from -r requirements.txt (line 8))\n",
      "  Using cached langchain_chroma-0.1.4-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting docarray (from -r requirements.txt (line 9))\n",
      "  Using cached docarray-0.40.0-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting pytube (from -r requirements.txt (line 11))\n",
      "  Using cached pytube-15.0.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 12)) (1.0.1)\n",
      "Collecting tiktoken (from -r requirements.txt (line 13))\n",
      "  Using cached tiktoken-0.8.0-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting ruff (from -r requirements.txt (line 14))\n",
      "  Using cached ruff-0.7.2-py3-none-win_amd64.whl.metadata (25 kB)\n",
      "Collecting pymupdf (from -r requirements.txt (line 15))\n",
      "  Using cached PyMuPDF-1.24.13-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: bs4 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 16)) (0.0.2)\n",
      "Requirement already satisfied: arxiv in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 17)) (2.1.3)\n",
      "Collecting ragas (from -r requirements.txt (line 18))\n",
      "  Using cached ragas-0.2.3-py3-none-any.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 19)) (4.66.5)\n",
      "Collecting tqdm (from -r requirements.txt (line 19))\n",
      "  Downloading tqdm-4.67.0-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting gpt4all (from -r requirements.txt (line 20))\n",
      "  Using cached gpt4all-2.8.2-py3-none-win_amd64.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from -r requirements.txt (line 21)) (0.26.2)\n",
      "Collecting langchain_huggingface (from -r requirements.txt (line 22))\n",
      "  Using cached langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting pypdf (from -r requirements.txt (line 23))\n",
      "  Using cached pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting rapidocr-onnxruntime (from -r requirements.txt (line 24))\n",
      "  Using cached rapidocr_onnxruntime-1.3.25-py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->-r requirements.txt (line 1)) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->-r requirements.txt (line 1)) (4.12.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (4.6.2.post1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain->-r requirements.txt (line 3)) (6.0.2)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain->-r requirements.txt (line 3))\n",
      "  Using cached SQLAlchemy-2.0.36-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain->-r requirements.txt (line 3))\n",
      "  Using cached aiohttp-3.10.10-cp312-cp312-win_amd64.whl.metadata (7.8 kB)\n",
      "Collecting langchain-core<0.4.0,>=0.3.15 (from langchain->-r requirements.txt (line 3))\n",
      "  Using cached langchain_core-0.3.15-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain->-r requirements.txt (line 3))\n",
      "  Using cached langchain_text_splitters-0.3.2-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain->-r requirements.txt (line 3))\n",
      "  Downloading langsmith-0.1.140-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain->-r requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain->-r requirements.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain->-r requirements.txt (line 3)) (8.5.0)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain->-r requirements.txt (line 3))\n",
      "  Using cached SQLAlchemy-2.0.35-cp312-cp312-win_amd64.whl.metadata (9.9 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community->-r requirements.txt (line 4))\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community->-r requirements.txt (line 4))\n",
      "  Using cached httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community->-r requirements.txt (line 4))\n",
      "  Using cached pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting ollama<1,>=0.3.0 (from langchain-ollama->-r requirements.txt (line 6))\n",
      "  Using cached ollama-0.3.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchainhub->-r requirements.txt (line 7)) (24.1)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub->-r requirements.txt (line 7))\n",
      "  Using cached types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0 (from langchain-chroma->-r requirements.txt (line 8))\n",
      "  Downloading chromadb-0.5.18-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting fastapi<1,>=0.95.2 (from langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached fastapi-0.115.4-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting orjson>=3.8.2 (from docarray->-r requirements.txt (line 9))\n",
      "  Using cached orjson-3.10.11-cp312-none-win_amd64.whl.metadata (52 kB)\n",
      "Requirement already satisfied: rich>=13.1.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from docarray->-r requirements.txt (line 9)) (13.7.1)\n",
      "Collecting typing-inspect>=0.8.0 (from docarray->-r requirements.txt (line 9))\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tiktoken->-r requirements.txt (line 13)) (2024.9.11)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bs4->-r requirements.txt (line 16)) (4.12.3)\n",
      "Requirement already satisfied: feedparser~=6.0.10 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from arxiv->-r requirements.txt (line 17)) (6.0.11)\n",
      "Collecting datasets (from ragas->-r requirements.txt (line 18))\n",
      "  Using cached datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: nest-asyncio in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ragas->-r requirements.txt (line 18)) (1.6.0)\n",
      "Collecting appdirs (from ragas->-r requirements.txt (line 18))\n",
      "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting pysbd>=0.3.4 (from ragas->-r requirements.txt (line 18))\n",
      "  Using cached pysbd-0.3.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->-r requirements.txt (line 19)) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub->-r requirements.txt (line 21)) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface_hub->-r requirements.txt (line 21)) (2024.10.0)\n",
      "Requirement already satisfied: sentence-transformers>=2.6.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_huggingface->-r requirements.txt (line 22)) (3.2.1)\n",
      "Requirement already satisfied: tokenizers>=0.19.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_huggingface->-r requirements.txt (line 22)) (0.20.3)\n",
      "Requirement already satisfied: transformers>=4.39.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain_huggingface->-r requirements.txt (line 22)) (4.46.2)\n",
      "Collecting pyclipper>=1.2.0 (from rapidocr-onnxruntime->-r requirements.txt (line 24))\n",
      "  Using cached pyclipper-1.3.0.post6-cp312-cp312-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting opencv-python>=4.5.1.48 (from rapidocr-onnxruntime->-r requirements.txt (line 24))\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: six>=1.15.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rapidocr-onnxruntime->-r requirements.txt (line 24)) (1.16.0)\n",
      "Collecting Shapely!=2.0.4,>=1.7.1 (from rapidocr-onnxruntime->-r requirements.txt (line 24))\n",
      "  Using cached shapely-2.0.6-cp312-cp312-win_amd64.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rapidocr-onnxruntime->-r requirements.txt (line 24)) (10.4.0)\n",
      "Collecting onnxruntime>=1.7.0 (from rapidocr-onnxruntime->-r requirements.txt (line 24))\n",
      "  Using cached onnxruntime-1.20.0-cp312-cp312-win_amd64.whl.metadata (4.6 kB)\n",
      "Collecting numba (from openai-whisper==20240930->-r requirements.txt (line 25))\n",
      "  Using cached numba-0.60.0-cp312-cp312-win_amd64.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openai-whisper==20240930->-r requirements.txt (line 25)) (2.5.1)\n",
      "Collecting more-itertools (from openai-whisper==20240930->-r requirements.txt (line 25))\n",
      "  Using cached more_itertools-10.5.0-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3))\n",
      "  Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3))\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3)) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3))\n",
      "  Using cached frozenlist-1.5.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3))\n",
      "  Using cached multidict-6.1.0-cp312-cp312-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting yarl<2.0,>=1.12.0 (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3))\n",
      "  Using cached yarl-1.17.1-cp312-cp312-win_amd64.whl.metadata (66 kB)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from anyio<5,>=3.5.0->openai->-r requirements.txt (line 2)) (3.7)\n",
      "Collecting build>=1.0.3 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached chroma_hnswlib-0.7.6-cp312-cp312-win_amd64.whl\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached uvicorn-0.32.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached posthog-3.7.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Downloading opentelemetry_api-1.28.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Downloading opentelemetry_instrumentation_fastapi-0.49b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Downloading opentelemetry_sdk-1.28.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached PyPika-0.48.9-py2.py3-none-any.whl\n",
      "Collecting overrides>=7.3.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (1.67.1)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached bcrypt-4.2.0-cp39-abi3-win_amd64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (0.12.5)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached mmh3-5.0.1-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community->-r requirements.txt (line 4))\n",
      "  Using cached marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting starlette<0.42.0,>=0.40.0 (from fastapi<1,>=0.95.2->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached starlette-0.41.2-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from feedparser~=6.0.10->arxiv->-r requirements.txt (line 17)) (1.0.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai->-r requirements.txt (line 2)) (0.14.0)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.15->langchain->-r requirements.txt (line 3))\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 3))\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.7.0->rapidocr-onnxruntime->-r requirements.txt (line 24))\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime->-r requirements.txt (line 24)) (24.3.25)\n",
      "Requirement already satisfied: protobuf in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime->-r requirements.txt (line 24)) (5.28.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime->-r requirements.txt (line 24)) (1.13.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 3)) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 3)) (2.2.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=13.1.0->docarray->-r requirements.txt (line 9)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=13.1.0->docarray->-r requirements.txt (line 9)) (2.18.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface->-r requirements.txt (line 22)) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain_huggingface->-r requirements.txt (line 22)) (1.13.1)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 3))\n",
      "  Using cached greenlet-3.1.1-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: networkx in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->openai-whisper==20240930->-r requirements.txt (line 25)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->openai-whisper==20240930->-r requirements.txt (line 25)) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->openai-whisper==20240930->-r requirements.txt (line 25)) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->onnxruntime>=1.7.0->rapidocr-onnxruntime->-r requirements.txt (line 24)) (1.3.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers>=4.39.0->langchain_huggingface->-r requirements.txt (line 22)) (0.4.5)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->docarray->-r requirements.txt (line 9))\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4->bs4->-r requirements.txt (line 16)) (2.6)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets->ragas->-r requirements.txt (line 18)) (17.0.0)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets->ragas->-r requirements.txt (line 18))\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets->ragas->-r requirements.txt (line 18)) (2.2.2)\n",
      "Collecting xxhash (from datasets->ragas->-r requirements.txt (line 18))\n",
      "  Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets->ragas->-r requirements.txt (line 18))\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface_hub->-r requirements.txt (line 21))\n",
      "  Using cached fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba->openai-whisper==20240930->-r requirements.txt (line 25))\n",
      "  Using cached llvmlite-0.43.0-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain->-r requirements.txt (line 3))\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (2.36.0)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray->-r requirements.txt (line 9)) (0.1.2)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (8.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (1.65.0)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.28.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.28.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.28.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Downloading opentelemetry_proto-1.28.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.49b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Downloading opentelemetry_instrumentation_asgi-0.49b0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-instrumentation==0.49b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Downloading opentelemetry_instrumentation-0.49b0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.49b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Downloading opentelemetry_semantic_conventions-0.49b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-util-http==0.49b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Downloading opentelemetry_util_http-0.49b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-instrumentation==0.49b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (1.16.0)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.49b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer>=0.9.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (1.5.4)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached httptools-0.6.4-cp312-cp312-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached watchfiles-0.24.0-cp312-none-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8))\n",
      "  Using cached websockets-13.1-cp312-cp312-win_amd64.whl.metadata (7.0 kB)\n",
      "Collecting propcache>=0.2.0 (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 3))\n",
      "  Using cached propcache-0.2.0-cp312-cp312-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.7.0->rapidocr-onnxruntime->-r requirements.txt (line 24))\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch->openai-whisper==20240930->-r requirements.txt (line 25)) (2.1.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets->ragas->-r requirements.txt (line 18)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets->ragas->-r requirements.txt (line 18)) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface->-r requirements.txt (line 22)) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain_huggingface->-r requirements.txt (line 22)) (3.5.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (4.9)\n",
      "Collecting pyreadline3 (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.7.0->rapidocr-onnxruntime->-r requirements.txt (line 24))\n",
      "  Using cached pyreadline3-3.5.4-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (3.20.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb!=0.5.4,!=0.5.5,<0.6.0,>=0.4.0->langchain-chroma->-r requirements.txt (line 8)) (0.6.1)\n",
      "Downloading openai-1.54.3-py3-none-any.whl (389 kB)\n",
      "Using cached langchain-0.3.7-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_community-0.3.5-py3-none-any.whl (2.4 MB)\n",
      "Downloading langchain_openai-0.2.6-py3-none-any.whl (50 kB)\n",
      "Using cached langchain_ollama-0.2.0-py3-none-any.whl (14 kB)\n",
      "Using cached langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
      "Using cached langchain_chroma-0.1.4-py3-none-any.whl (10 kB)\n",
      "Using cached docarray-0.40.0-py3-none-any.whl (270 kB)\n",
      "Using cached pytube-15.0.0-py3-none-any.whl (57 kB)\n",
      "Using cached tiktoken-0.8.0-cp312-cp312-win_amd64.whl (883 kB)\n",
      "Using cached ruff-0.7.2-py3-none-win_amd64.whl (9.4 MB)\n",
      "Using cached PyMuPDF-1.24.13-cp39-abi3-win_amd64.whl (16.2 MB)\n",
      "Using cached ragas-0.2.3-py3-none-any.whl (141 kB)\n",
      "Downloading tqdm-4.67.0-py3-none-any.whl (78 kB)\n",
      "Using cached gpt4all-2.8.2-py3-none-win_amd64.whl (119.6 MB)\n",
      "Using cached langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
      "Using cached pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "Using cached rapidocr_onnxruntime-1.3.25-py3-none-any.whl (14.9 MB)\n",
      "Using cached aiohttp-3.10.10-cp312-cp312-win_amd64.whl (379 kB)\n",
      "Downloading chromadb-0.5.18-py3-none-any.whl (615 kB)\n",
      "   ---------------------------------------- 0.0/615.5 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 262.1/615.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 615.5/615.5 kB 2.5 MB/s eta 0:00:00\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached fastapi-0.115.4-py3-none-any.whl (94 kB)\n",
      "Using cached httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached langchain_core-0.3.15-py3-none-any.whl (408 kB)\n",
      "Using cached langchain_text_splitters-0.3.2-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.140-py3-none-any.whl (304 kB)\n",
      "Using cached ollama-0.3.3-py3-none-any.whl (10 kB)\n",
      "Using cached onnxruntime-1.20.0-cp312-cp312-win_amd64.whl (11.3 MB)\n",
      "Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "Using cached orjson-3.10.11-cp312-none-win_amd64.whl (136 kB)\n",
      "Using cached pyclipper-1.3.0.post6-cp312-cp312-win_amd64.whl (110 kB)\n",
      "Using cached pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n",
      "Using cached pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
      "Using cached shapely-2.0.6-cp312-cp312-win_amd64.whl (1.4 MB)\n",
      "Using cached SQLAlchemy-2.0.35-cp312-cp312-win_amd64.whl (2.1 MB)\n",
      "Using cached types_requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached datasets-3.1.0-py3-none-any.whl (480 kB)\n",
      "Using cached fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Using cached more_itertools-10.5.0-py3-none-any.whl (60 kB)\n",
      "Using cached numba-0.60.0-cp312-cp312-win_amd64.whl (2.7 MB)\n",
      "Using cached aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Using cached bcrypt-4.2.0-cp39-abi3-win_amd64.whl (151 kB)\n",
      "Using cached build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached frozenlist-1.5.0-cp312-cp312-win_amd64.whl (51 kB)\n",
      "Using cached greenlet-3.1.1-cp312-cp312-win_amd64.whl (299 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n",
      "Using cached llvmlite-0.43.0-cp312-cp312-win_amd64.whl (28.1 MB)\n",
      "Using cached marshmallow-3.23.1-py3-none-any.whl (49 kB)\n",
      "Using cached mmh3-5.0.1-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Using cached multidict-6.1.0-cp312-cp312-win_amd64.whl (28 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading opentelemetry_api-1.28.0-py3-none-any.whl (64 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_grpc-1.28.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.28.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.28.0-py3-none-any.whl (55 kB)\n",
      "Downloading opentelemetry_instrumentation_fastapi-0.49b0-py3-none-any.whl (12 kB)\n",
      "Downloading opentelemetry_instrumentation-0.49b0-py3-none-any.whl (30 kB)\n",
      "Downloading opentelemetry_instrumentation_asgi-0.49b0-py3-none-any.whl (16 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.49b0-py3-none-any.whl (159 kB)\n",
      "Downloading opentelemetry_util_http-0.49b0-py3-none-any.whl (6.9 kB)\n",
      "Downloading opentelemetry_sdk-1.28.0-py3-none-any.whl (118 kB)\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Using cached posthog-3.7.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached starlette-0.41.2-py3-none-any.whl (73 kB)\n",
      "Using cached uvicorn-0.32.0-py3-none-any.whl (63 kB)\n",
      "Using cached yarl-1.17.1-cp312-cp312-win_amd64.whl (89 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached importlib_resources-6.4.5-py3-none-any.whl (36 kB)\n",
      "Using cached xxhash-3.5.0-cp312-cp312-win_amd64.whl (30 kB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Using cached durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
      "Using cached httptools-0.6.4-cp312-cp312-win_amd64.whl (88 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached propcache-0.2.0-cp312-cp312-win_amd64.whl (44 kB)\n",
      "Using cached watchfiles-0.24.0-cp312-none-win_amd64.whl (277 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Using cached websockets-13.1-cp312-cp312-win_amd64.whl (159 kB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Using cached pyreadline3-3.5.4-py3-none-any.whl (83 kB)\n",
      "Building wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml): started\n",
      "  Building wheel for openai-whisper (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=812902 sha256=47ca370c70a708d5616db0a4bf826e0d1b083c9312f146d7db5bc491e4e57f65\n",
      "  Stored in directory: C:\\Users\\derek\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-05xuorcq\\wheels\\c3\\03\\25\\5e0ba78bc27a3a089f137c9f1d92fdfce16d06996c071a016c\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: pypika, pyclipper, monotonic, durationpy, appdirs, xxhash, websockets, websocket-client, types-requests, tqdm, Shapely, ruff, pytube, pysbd, pyreadline3, pyproject_hooks, pypdf, pymupdf, propcache, overrides, orjson, opentelemetry-util-http, opentelemetry-proto, opencv-python, oauthlib, mypy-extensions, multidict, more-itertools, mmh3, marshmallow, llvmlite, jsonpointer, importlib-resources, httpx-sse, httptools, greenlet, fsspec, frozenlist, dill, deprecated, chroma-hnswlib, bcrypt, backoff, asgiref, aiohappyeyeballs, yarl, watchfiles, uvicorn, typing-inspect, tiktoken, starlette, SQLAlchemy, requests-toolbelt, requests-oauthlib, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, numba, multiprocess, langchainhub, jsonpatch, humanfriendly, gpt4all, build, aiosignal, pydantic-settings, opentelemetry-semantic-conventions, openai-whisper, openai, ollama, langsmith, kubernetes, fastapi, docarray, dataclasses-json, coloredlogs, aiohttp, opentelemetry-sdk, opentelemetry-instrumentation, onnxruntime, langchain-core, rapidocr-onnxruntime, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langchain-text-splitters, langchain-openai, langchain-ollama, datasets, opentelemetry-instrumentation-fastapi, langchain_huggingface, langchain, langchain_community, chromadb, ragas, langchain-chroma\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.5\n",
      "    Uninstalling tqdm-4.66.5:\n",
      "      Successfully uninstalled tqdm-4.66.5\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.10.0\n",
      "    Uninstalling fsspec-2024.10.0:\n",
      "      Successfully uninstalled fsspec-2024.10.0\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.54.1\n",
      "    Uninstalling openai-1.54.1:\n",
      "      Successfully uninstalled openai-1.54.1\n",
      "Successfully installed SQLAlchemy-2.0.35 Shapely-2.0.6 aiohappyeyeballs-2.4.3 aiohttp-3.10.10 aiosignal-1.3.1 appdirs-1.4.4 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.18 coloredlogs-15.0.1 dataclasses-json-0.6.7 datasets-3.1.0 deprecated-1.2.14 dill-0.3.8 docarray-0.40.0 durationpy-0.9 fastapi-0.115.4 frozenlist-1.5.0 fsspec-2024.9.0 gpt4all-2.8.2 greenlet-3.1.1 httptools-0.6.4 httpx-sse-0.4.0 humanfriendly-10.0 importlib-resources-6.4.5 jsonpatch-1.33 jsonpointer-3.0.0 kubernetes-31.0.0 langchain-0.3.7 langchain-chroma-0.1.4 langchain-core-0.3.15 langchain-ollama-0.2.0 langchain-openai-0.2.6 langchain-text-splitters-0.3.2 langchain_community-0.3.5 langchain_huggingface-0.1.2 langchainhub-0.1.21 langsmith-0.1.140 llvmlite-0.43.0 marshmallow-3.23.1 mmh3-5.0.1 monotonic-1.6 more-itertools-10.5.0 multidict-6.1.0 multiprocess-0.70.16 mypy-extensions-1.0.0 numba-0.60.0 oauthlib-3.2.2 ollama-0.3.3 onnxruntime-1.20.0 openai-1.54.3 openai-whisper-20240930 opencv-python-4.10.0.84 opentelemetry-api-1.28.0 opentelemetry-exporter-otlp-proto-common-1.28.0 opentelemetry-exporter-otlp-proto-grpc-1.28.0 opentelemetry-instrumentation-0.49b0 opentelemetry-instrumentation-asgi-0.49b0 opentelemetry-instrumentation-fastapi-0.49b0 opentelemetry-proto-1.28.0 opentelemetry-sdk-1.28.0 opentelemetry-semantic-conventions-0.49b0 opentelemetry-util-http-0.49b0 orjson-3.10.11 overrides-7.7.0 posthog-3.7.0 propcache-0.2.0 pyclipper-1.3.0.post6 pydantic-settings-2.6.1 pymupdf-1.24.13 pypdf-5.1.0 pypika-0.48.9 pyproject_hooks-1.2.0 pyreadline3-3.5.4 pysbd-0.3.4 pytube-15.0.0 ragas-0.2.3 rapidocr-onnxruntime-1.3.25 requests-oauthlib-2.0.0 requests-toolbelt-1.0.0 ruff-0.7.2 starlette-0.41.2 tiktoken-0.8.0 tqdm-4.67.0 types-requests-2.32.0.20241016 typing-inspect-0.9.0 uvicorn-0.32.0 watchfiles-0.24.0 websocket-client-1.8.0 websockets-13.1 xxhash-3.5.0 yarl-1.17.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git 'C:\\Users\\derek\\AppData\\Local\\Temp\\pip-req-build-v0kwnef4'\n",
      "WARNING: langchain 0.3.7 does not provide the extra 'docarray'\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-experimental\n",
      "  Downloading langchain_experimental-0.3.3-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-experimental) (0.3.5)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-experimental) (0.3.15)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<2.0.36,>=1.4 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.10.10)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.4.0)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.6 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.7)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.1.140)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.26.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-community<0.4.0,>=0.3.0->langchain-experimental) (8.5.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-experimental) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-experimental) (24.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-experimental) (2.9.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.15->langchain-experimental) (4.12.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.17.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain-experimental) (3.0.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain<0.4.0,>=0.3.6->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.3.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.15->langchain-experimental) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<0.4.0,>=0.3.15->langchain-experimental) (2.23.4)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (2024.7.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from SQLAlchemy<2.0.36,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (4.6.2.post1)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.14.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (1.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-experimental) (0.2.0)\n",
      "Downloading langchain_experimental-0.3.3-py3-none-any.whl (208 kB)\n",
      "Installing collected packages: langchain-experimental\n",
      "Successfully installed langchain-experimental-0.3.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-generativeai in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.8.3)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (0.6.10)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.22.0)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.151.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.36.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (5.28.3)\n",
      "Requirement already satisfied: pydantic in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (2.9.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-ai-generativelanguage==0.6.10->google-generativeai) (1.25.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core->google-generativeai) (1.65.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai) (4.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->google-generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic->google-generativeai) (2.23.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.67.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai) (1.67.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai) (3.1.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-google-genai\n",
      "  Using cached langchain_google_genai-2.0.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: google-generativeai<0.9.0,>=0.8.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-google-genai) (0.8.3)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3.15 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-google-genai) (0.3.15)\n",
      "Requirement already satisfied: pydantic<3,>=2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-google-genai) (2.9.2)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.10 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.10)\n",
      "Requirement already satisfied: google-api-core in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.22.0)\n",
      "Requirement already satisfied: google-api-python-client in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.151.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.36.0)\n",
      "Requirement already satisfied: protobuf in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (5.28.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.67.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.25.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.4,>=0.3.15->langchain-google-genai) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.4,>=0.3.15->langchain-google-genai) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.4,>=0.3.15->langchain-google-genai) (0.1.140)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.4,>=0.3.15->langchain-google-genai) (24.1)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langchain-core<0.4,>=0.3.15->langchain-google-genai) (8.5.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic<3,>=2->langchain-google-genai) (2.23.4)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.65.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.9)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.15->langchain-google-genai) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (3.10.11)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (1.0.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (4.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.4.6)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.67.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (1.67.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.1.4)\n",
      "Requirement already satisfied: anyio in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (1.0.6)\n",
      "Requirement already satisfied: idna in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core<0.4,>=0.3.15->langchain-google-genai) (0.14.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai<0.9.0,>=0.8.0->langchain-google-genai) (2.2.2)\n",
      "Using cached langchain_google_genai-2.0.4-py3-none-any.whl (41 kB)\n",
      "Installing collected packages: langchain-google-genai\n",
      "Successfully installed langchain-google-genai-2.0.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (10.4.0)\n",
      "Requirement already satisfied: pydantic in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.9.2)\n",
      "Collecting lxml\n",
      "  Using cached lxml-5.3.0-cp312-cp312-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: chromadb in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.5.18)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.8.0)\n",
      "Collecting unstructured[all-docs]\n",
      "  Using cached unstructured-0.16.4-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting chardet (from unstructured[all-docs])\n",
      "  Using cached chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting filetype (from unstructured[all-docs])\n",
      "  Using cached filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting python-magic (from unstructured[all-docs])\n",
      "  Using cached python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured[all-docs]) (3.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured[all-docs]) (2.32.3)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured[all-docs]) (4.12.3)\n",
      "Collecting emoji (from unstructured[all-docs])\n",
      "  Using cached emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: dataclasses-json in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured[all-docs]) (0.6.7)\n",
      "Collecting python-iso639 (from unstructured[all-docs])\n",
      "  Using cached python_iso639-2024.10.22-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting langdetect (from unstructured[all-docs])\n",
      "  Using cached langdetect-1.0.9-py3-none-any.whl\n",
      "Requirement already satisfied: numpy<2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured[all-docs]) (1.26.4)\n",
      "Collecting rapidfuzz (from unstructured[all-docs])\n",
      "  Using cached rapidfuzz-3.10.1-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: backoff in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured[all-docs]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured[all-docs]) (4.12.2)\n",
      "Collecting unstructured-client (from unstructured[all-docs])\n",
      "  Downloading unstructured_client-0.27.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: wrapt in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured[all-docs]) (1.16.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured[all-docs]) (4.67.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured[all-docs]) (6.0.0)\n",
      "Collecting python-oxmsg (from unstructured[all-docs])\n",
      "  Using cached python_oxmsg-0.0.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting html5lib (from unstructured[all-docs])\n",
      "  Using cached html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Collecting unstructured.pytesseract>=0.3.12 (from unstructured[all-docs])\n",
      "  Using cached unstructured.pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured[all-docs]) (3.1.5)\n",
      "Collecting pdf2image (from unstructured[all-docs])\n",
      "  Using cached pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: markdown in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured[all-docs]) (3.7)\n",
      "Requirement already satisfied: networkx in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured[all-docs]) (3.4.2)\n",
      "Collecting pi-heif (from unstructured[all-docs])\n",
      "  Using cached pi_heif-0.20.0-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting effdet (from unstructured[all-docs])\n",
      "  Using cached effdet-0.4.1-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting python-docx>=1.1.2 (from unstructured[all-docs])\n",
      "  Using cached python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting unstructured-inference==0.8.1 (from unstructured[all-docs])\n",
      "  Using cached unstructured_inference-0.8.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting python-pptx>=1.0.1 (from unstructured[all-docs])\n",
      "  Using cached python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting pikepdf (from unstructured[all-docs])\n",
      "  Using cached pikepdf-9.4.0-cp312-cp312-win_amd64.whl.metadata (8.4 kB)\n",
      "Collecting pypandoc (from unstructured[all-docs])\n",
      "  Using cached pypandoc-1.14-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting onnx (from unstructured[all-docs])\n",
      "  Using cached onnx-1.17.0-cp312-cp312-win_amd64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: pypdf in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured[all-docs]) (5.1.0)\n",
      "Collecting google-cloud-vision (from unstructured[all-docs])\n",
      "  Using cached google_cloud_vision-3.8.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting xlrd (from unstructured[all-docs])\n",
      "  Using cached xlrd-2.0.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured[all-docs]) (2.2.2)\n",
      "Collecting pdfminer.six (from unstructured[all-docs])\n",
      "  Using cached pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting layoutparser (from unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Using cached layoutparser-0.3.4-py3-none-any.whl.metadata (7.7 kB)\n",
      "Collecting python-multipart (from unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Using cached python_multipart-0.0.17-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (0.26.2)\n",
      "Requirement already satisfied: opencv-python!=4.7.0.68 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (4.10.0.84)\n",
      "Requirement already satisfied: onnxruntime>=1.17.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (1.20.0)\n",
      "Requirement already satisfied: torch in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (2.5.1)\n",
      "Collecting timm (from unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Using cached timm-1.0.11-py3-none-any.whl.metadata (48 kB)\n",
      "Requirement already satisfied: transformers>=4.25.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured-inference==0.8.1->unstructured[all-docs]) (4.46.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic) (2.23.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: build>=1.0.3 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (0.115.4)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.32.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (3.7.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (0.49b0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (1.28.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (0.20.3)\n",
      "Requirement already satisfied: pypika>=0.48.9 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (6.4.5)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (1.67.1)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (0.12.5)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (31.0.0)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (8.5.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (5.0.1)\n",
      "Requirement already satisfied: orjson>=3.9.12 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (3.10.11)\n",
      "Requirement already satisfied: httpx>=0.27.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (0.27.2)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from chromadb) (13.7.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tiktoken) (2024.9.11)\n",
      "Requirement already satisfied: pyproject_hooks in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from build>=1.0.3->chromadb) (0.4.6)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from fastapi>=0.95.2->chromadb) (0.41.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.27.0->chromadb) (4.6.2.post1)\n",
      "Requirement already satisfied: certifi in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.27.0->chromadb) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.0.6)\n",
      "Requirement already satisfied: idna in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpx>=0.27.0->chromadb) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.36.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (2.2.2)\n",
      "Requirement already satisfied: durationpy>=0.7 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (24.3.25)\n",
      "Requirement already satisfied: protobuf in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (5.28.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\n",
      "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.65.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.28.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.28.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.28.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.49b0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.49b0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.49b0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b0)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.49b0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.49b0)\n",
      "Requirement already satisfied: asgiref~=3.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from opentelemetry-instrumentation-asgi==0.49b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Collecting XlsxWriter>=0.5.7 (from python-pptx>=1.0.1->unstructured[all-docs])\n",
      "  Using cached XlsxWriter-3.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->unstructured[all-docs]) (3.3.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.5.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.24.0)\n",
      "Requirement already satisfied: websockets>=10.4 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from uvicorn[standard]>=0.18.3->chromadb) (13.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4->unstructured[all-docs]) (2.6)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json->unstructured[all-docs]) (3.23.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from dataclasses-json->unstructured[all-docs]) (0.9.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from effdet->unstructured[all-docs]) (0.20.1)\n",
      "Collecting pycocotools>=2.0.2 (from effdet->unstructured[all-docs])\n",
      "  Using cached pycocotools-2.0.8-cp312-cp312-win_amd64.whl.metadata (1.1 kB)\n",
      "Collecting omegaconf>=2.0 (from effdet->unstructured[all-docs])\n",
      "  Using cached omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (2.22.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-cloud-vision->unstructured[all-docs]) (1.25.0)\n",
      "Collecting webencodings (from html5lib->unstructured[all-docs])\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk->unstructured[all-docs]) (1.4.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openpyxl->unstructured[all-docs]) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->unstructured[all-docs]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->unstructured[all-docs]) (2024.1)\n",
      "Collecting cryptography>=36.0.0 (from pdfminer.six->unstructured[all-docs])\n",
      "  Using cached cryptography-43.0.3-cp39-abi3-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting olefile (from python-oxmsg->unstructured[all-docs])\n",
      "  Using cached olefile-0.47-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting eval-type-backport<0.3.0,>=0.2.0 (from unstructured-client->unstructured[all-docs])\n",
      "  Using cached eval_type_backport-0.2.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting jsonpath-python<2.0.0,>=1.0.6 (from unstructured-client->unstructured[all-docs])\n",
      "  Using cached jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: nest-asyncio>=1.6.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (1.6.0)\n",
      "Collecting python-dateutil>=2.7 (from matplotlib)\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from unstructured-client->unstructured[all-docs]) (1.0.0)\n",
      "Collecting cffi>=1.12 (from cryptography>=36.0.0->pdfminer.six->unstructured[all-docs])\n",
      "  Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-vision->unstructured[all-docs]) (1.67.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub->unstructured-inference==0.8.1->unstructured[all-docs]) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub->unstructured-inference==0.8.1->unstructured[all-docs]) (2024.9.0)\n",
      "Requirement already satisfied: zipp>=3.20 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.20.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf>=2.0->effdet->unstructured[all-docs])\n",
      "  Using cached antlr4_python3_runtime-4.9.3-py3-none-any.whl\n",
      "Requirement already satisfied: safetensors in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from timm->unstructured-inference==0.8.1->unstructured[all-docs]) (0.4.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch->unstructured-inference==0.8.1->unstructured[all-docs]) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->unstructured[all-docs]) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (10.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from layoutparser->unstructured-inference==0.8.1->unstructured[all-docs]) (1.13.1)\n",
      "Collecting iopath (from layoutparser->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Using cached iopath-0.1.10-py3-none-any.whl\n",
      "Collecting pdfplumber (from layoutparser->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Using cached pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->unstructured[all-docs])\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.17.0->unstructured-inference==0.8.1->unstructured[all-docs]) (3.5.4)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Collecting portalocker (from iopath->layoutparser->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Using cached portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch->unstructured-inference==0.8.1->unstructured[all-docs]) (2.1.5)\n",
      "Collecting pdfminer.six (from unstructured[all-docs])\n",
      "  Using cached pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber->layoutparser->unstructured-inference==0.8.1->unstructured[all-docs])\n",
      "  Using cached pypdfium2-4.30.0-py3-none-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from portalocker->iopath->layoutparser->unstructured-inference==0.8.1->unstructured[all-docs]) (306)\n",
      "Using cached unstructured_inference-0.8.1-py3-none-any.whl (48 kB)\n",
      "Using cached lxml-5.3.0-cp312-cp312-win_amd64.whl (3.8 MB)\n",
      "Using cached python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "Using cached python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
      "Using cached unstructured.pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Using cached chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Using cached effdet-0.4.1-py3-none-any.whl (112 kB)\n",
      "Using cached emoji-2.14.0-py3-none-any.whl (586 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached google_cloud_vision-3.8.0-py2.py3-none-any.whl (488 kB)\n",
      "Using cached html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Using cached onnx-1.17.0-cp312-cp312-win_amd64.whl (14.5 MB)\n",
      "Using cached pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Using cached pi_heif-0.20.0-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "Using cached pikepdf-9.4.0-cp312-cp312-win_amd64.whl (3.5 MB)\n",
      "Using cached pypandoc-1.14-py3-none-any.whl (21 kB)\n",
      "Using cached python_iso639-2024.10.22-py3-none-any.whl (274 kB)\n",
      "Using cached python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Using cached python_oxmsg-0.0.1-py3-none-any.whl (31 kB)\n",
      "Using cached rapidfuzz-3.10.1-cp312-cp312-win_amd64.whl (1.6 MB)\n",
      "Using cached unstructured-0.16.4-py3-none-any.whl (1.7 MB)\n",
      "Downloading unstructured_client-0.27.0-py3-none-any.whl (59 kB)\n",
      "Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Using cached xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "Using cached cryptography-43.0.3-cp39-abi3-win_amd64.whl (3.1 MB)\n",
      "Using cached eval_type_backport-0.2.0-py3-none-any.whl (5.9 kB)\n",
      "Using cached jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
      "Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Using cached pycocotools-2.0.8-cp312-cp312-win_amd64.whl (83 kB)\n",
      "Using cached timm-1.0.11-py3-none-any.whl (2.3 MB)\n",
      "Using cached XlsxWriter-3.2.0-py3-none-any.whl (159 kB)\n",
      "Using cached layoutparser-0.3.4-py3-none-any.whl (19.2 MB)\n",
      "Using cached olefile-0.47-py2.py3-none-any.whl (114 kB)\n",
      "Using cached python_multipart-0.0.17-py3-none-any.whl (24 kB)\n",
      "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Using cached pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
      "Using cached pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
      "Using cached pypdfium2-4.30.0-py3-none-win_amd64.whl (2.9 MB)\n",
      "Using cached portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: webencodings, filetype, antlr4-python3-runtime, XlsxWriter, xlrd, unstructured.pytesseract, rapidfuzz, python-multipart, python-magic, python-iso639, python-dateutil, pypdfium2, pypandoc, pycparser, portalocker, pi-heif, pdf2image, onnx, omegaconf, olefile, lxml, langdetect, jsonpath-python, html5lib, eval-type-backport, emoji, chardet, python-pptx, python-oxmsg, python-docx, pikepdf, iopath, cffi, pycocotools, cryptography, unstructured-client, timm, pdfminer.six, unstructured, pdfplumber, google-cloud-vision, effdet, layoutparser, unstructured-inference\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "Successfully installed XlsxWriter-3.2.0 antlr4-python3-runtime-4.9.3 cffi-1.17.1 chardet-5.2.0 cryptography-43.0.3 effdet-0.4.1 emoji-2.14.0 eval-type-backport-0.2.0 filetype-1.2.0 google-cloud-vision-3.8.0 html5lib-1.1 iopath-0.1.10 jsonpath-python-1.0.6 langdetect-1.0.9 layoutparser-0.3.4 lxml-5.3.0 olefile-0.47 omegaconf-2.3.0 onnx-1.17.0 pdf2image-1.17.0 pdfminer.six-20231228 pdfplumber-0.11.4 pi-heif-0.20.0 pikepdf-9.4.0 portalocker-2.10.1 pycocotools-2.0.8 pycparser-2.22 pypandoc-1.14 pypdfium2-4.30.0 python-dateutil-2.8.2 python-docx-1.1.2 python-iso639-2024.10.22 python-magic-0.4.27 python-multipart-0.0.17 python-oxmsg-0.0.1 python-pptx-1.0.2 rapidfuzz-3.10.1 timm-1.0.11 unstructured-0.16.4 unstructured-client-0.27.0 unstructured-inference-0.8.1 unstructured.pytesseract-0.3.13 webencodings-0.5.1 xlrd-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/effdet/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002192ED0BA40>: Failed to establish a new connection: [WinError 10053] An established connection was aborted by the software in your host machine')': /simple/effdet/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002193131BB90>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/effdet/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002193131BE30>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/effdet/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x000002192E67C0E0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/effdet/\n"
     ]
    }
   ],
   "source": [
    "pip install \"unstructured[all-docs]\" pillow pydantic lxml pillow matplotlib chromadb tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdf2image in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pdf2image) (10.4.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (3.9.1)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [55 lines of output]\n",
      "      \u001b[36m\u001b[1m+ meson setup --native-file=C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-install-ne6nx7mn\\scipy_971c465db688481aac47a85aeab36b3f\\.mesonpy-native-file.ini -Ddebug=false -Doptimization=2 --prefix=C:\\Users\\Pavan Teja\\anaconda3 C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-install-ne6nx7mn\\scipy_971c465db688481aac47a85aeab36b3f C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-install-ne6nx7mn\\scipy_971c465db688481aac47a85aeab36b3f\\.mesonpy-9p92y_ag\\build\u001b[0m\n",
      "      The Meson build system\n",
      "      Version: 0.62.2\n",
      "      Source dir: C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-install-ne6nx7mn\\scipy_971c465db688481aac47a85aeab36b3f\n",
      "      Build dir: C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-install-ne6nx7mn\\scipy_971c465db688481aac47a85aeab36b3f\\.mesonpy-9p92y_ag\\build\n",
      "      Build type: native build\n",
      "      Project name: SciPy\n",
      "      Project version: 1.9.1\n",
      "      WARNING: Failed to activate VS environment: Could not find C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe\n",
      "      \n",
      "      ..\\..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
      "      The following exception(s) were encountered:\n",
      "      Running \"icl \" gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running \"cl /?\" gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running \"cc --version\" gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running \"gcc --version\" gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running \"clang --version\" gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running \"clang-cl /?\" gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      Running \"pgcc --version\" gave \"[WinError 2] The system cannot find the file specified\"\n",
      "      \n",
      "      A full log can be found at C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-install-ne6nx7mn\\scipy_971c465db688481aac47a85aeab36b3f\\.mesonpy-9p92y_ag\\build\\meson-logs\\meson-log.txt\n",
      "      Traceback (most recent call last):\n",
      "        File \"c:\\Users\\Pavan Teja\\Documents\\TextAnalyticsProject\\ISM6564_TextAnalytics_Project\\ism6564\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 353, in <module>\n",
      "          main()\n",
      "        File \"c:\\Users\\Pavan Teja\\Documents\\TextAnalyticsProject\\ISM6564_TextAnalytics_Project\\ism6564\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 335, in main\n",
      "          json_out['return_val'] = hook(**hook_input['kwargs'])\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"c:\\Users\\Pavan Teja\\Documents\\TextAnalyticsProject\\ISM6564_TextAnalytics_Project\\ism6564\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\", line 118, in get_requires_for_build_wheel\n",
      "          return hook(config_settings)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-build-env-f9t1txyt\\overlay\\Lib\\site-packages\\mesonpy\\__init__.py\", line 969, in get_requires_for_build_wheel\n",
      "          with _project(config_settings) as project:\n",
      "        File \"C:\\Users\\Pavan Teja\\anaconda3\\Lib\\contextlib.py\", line 137, in __enter__\n",
      "          return next(self.gen)\n",
      "                 ^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-build-env-f9t1txyt\\overlay\\Lib\\site-packages\\mesonpy\\__init__.py\", line 948, in _project\n",
      "          with Project.with_temp_working_dir(\n",
      "        File \"C:\\Users\\Pavan Teja\\anaconda3\\Lib\\contextlib.py\", line 137, in __enter__\n",
      "          return next(self.gen)\n",
      "                 ^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-build-env-f9t1txyt\\overlay\\Lib\\site-packages\\mesonpy\\__init__.py\", line 777, in with_temp_working_dir\n",
      "          yield cls(source_dir, tmpdir, build_dir)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-build-env-f9t1txyt\\overlay\\Lib\\site-packages\\mesonpy\\__init__.py\", line 682, in __init__\n",
      "          self._configure(reconfigure=bool(build_dir) and not native_file_mismatch)\n",
      "        File \"C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-build-env-f9t1txyt\\overlay\\Lib\\site-packages\\mesonpy\\__init__.py\", line 713, in _configure\n",
      "          self._meson(\n",
      "        File \"C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-build-env-f9t1txyt\\overlay\\Lib\\site-packages\\mesonpy\\__init__.py\", line 696, in _meson\n",
      "          return self._proc('meson', *args)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Pavan Teja\\AppData\\Local\\Temp\\pip-build-env-f9t1txyt\\overlay\\Lib\\site-packages\\mesonpy\\__init__.py\", line 691, in _proc\n",
      "          subprocess.check_call(list(args))\n",
      "        File \"C:\\Users\\Pavan Teja\\anaconda3\\Lib\\subprocess.py\", line 413, in check_call\n",
      "          raise CalledProcessError(retcode, cmd)\n",
      "      subprocess.CalledProcessError: Command '['meson', 'setup', '--native-file=C:\\\\Users\\\\Pavan Teja\\\\AppData\\\\Local\\\\Temp\\\\pip-install-ne6nx7mn\\\\scipy_971c465db688481aac47a85aeab36b3f\\\\.mesonpy-native-file.ini', '-Ddebug=false', '-Doptimization=2', '--prefix=C:\\\\Users\\\\Pavan Teja\\\\anaconda3', 'C:\\\\Users\\\\Pavan Teja\\\\AppData\\\\Local\\\\Temp\\\\pip-install-ne6nx7mn\\\\scipy_971c465db688481aac47a85aeab36b3f', 'C:\\\\Users\\\\Pavan Teja\\\\AppData\\\\Local\\\\Temp\\\\pip-install-ne6nx7mn\\\\scipy_971c465db688481aac47a85aeab36b3f\\\\.mesonpy-9p92y_ag\\\\build']' returned non-zero exit status 1.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: openpyxl in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (3.9.2)\n",
      "Collecting textblob\n",
      "  Using cached textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.8.2-cp311-cp311-win_amd64.whl (12.2 MB)\n",
      "                                              0.0/12.2 MB ? eta -:--:--\n",
      "                                              0.2/12.2 MB 3.5 MB/s eta 0:00:04\n",
      "     -                                        0.6/12.2 MB 7.0 MB/s eta 0:00:02\n",
      "     -----                                    1.6/12.2 MB 11.0 MB/s eta 0:00:01\n",
      "     --------                                 2.7/12.2 MB 15.4 MB/s eta 0:00:01\n",
      "     -------------                            4.0/12.2 MB 18.4 MB/s eta 0:00:01\n",
      "     -----------------                        5.3/12.2 MB 20.0 MB/s eta 0:00:01\n",
      "     ---------------------                    6.7/12.2 MB 20.4 MB/s eta 0:00:01\n",
      "     -----------------------                  7.3/12.2 MB 20.3 MB/s eta 0:00:01\n",
      "     -----------------------------            8.9/12.2 MB 21.9 MB/s eta 0:00:01\n",
      "     ---------------------------------       10.4/12.2 MB 24.2 MB/s eta 0:00:01\n",
      "     -----------------------------------     11.2/12.2 MB 26.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.2/12.2 MB 27.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.2/12.2 MB 27.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.2/12.2 MB 27.3 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.2/12.2 MB 19.8 MB/s eta 0:00:00\n",
      "Collecting gensim\n",
      "  Using cached gensim-4.3.3-cp311-cp311-win_amd64.whl (24.0 MB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: click in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from nltk) (4.66.6)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Using cached murmurhash-1.0.10-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Using cached cymem-2.0.8-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n",
      "  Downloading thinc-8.3.2-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "                                              0.0/1.5 MB ? eta -:--:--\n",
      "     ---------------------------------------- 1.5/1.5 MB 31.5 MB/s eta 0:00:00\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Using cached srsly-2.4.8-cp311-cp311-win_amd64.whl (479 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from spacy) (0.12.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from spacy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.4.1-py3-none-any.whl (182 kB)\n",
      "                                              0.0/182.4 kB ? eta -:--:--\n",
      "     ------------------------------------- 182.4/182.4 kB 10.8 MB/s eta 0:00:00\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.1-cp311-cp311-win_amd64.whl (46.2 MB)\n",
      "                                              0.0/46.2 MB ? eta -:--:--\n",
      "     -                                        1.5/46.2 MB 48.6 MB/s eta 0:00:01\n",
      "     --                                       2.9/46.2 MB 30.8 MB/s eta 0:00:02\n",
      "     ----                                     4.8/46.2 MB 33.8 MB/s eta 0:00:02\n",
      "     -----                                    6.4/46.2 MB 34.1 MB/s eta 0:00:02\n",
      "     ------                                   7.3/46.2 MB 33.4 MB/s eta 0:00:02\n",
      "     --------                                10.0/46.2 MB 35.6 MB/s eta 0:00:02\n",
      "     ---------                               10.8/46.2 MB 32.8 MB/s eta 0:00:02\n",
      "     ----------                              12.7/46.2 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------                            14.3/46.2 MB 36.4 MB/s eta 0:00:01\n",
      "     -------------                           15.7/46.2 MB 34.4 MB/s eta 0:00:01\n",
      "     -------------                           15.7/46.2 MB 34.4 MB/s eta 0:00:01\n",
      "     -------------                           15.9/46.2 MB 27.3 MB/s eta 0:00:02\n",
      "     -------------                           16.5/46.2 MB 23.4 MB/s eta 0:00:02\n",
      "     ---------------                         18.5/46.2 MB 24.2 MB/s eta 0:00:02\n",
      "     -----------------                       20.5/46.2 MB 23.4 MB/s eta 0:00:02\n",
      "     --------------------                    24.6/46.2 MB 24.2 MB/s eta 0:00:01\n",
      "     ----------------------                  27.0/46.2 MB 38.6 MB/s eta 0:00:01\n",
      "     --------------------------              31.3/46.2 MB 40.9 MB/s eta 0:00:01\n",
      "     --------------------------              31.9/46.2 MB 36.4 MB/s eta 0:00:01\n",
      "     ----------------------------            33.5/46.2 MB 36.3 MB/s eta 0:00:01\n",
      "     -----------------------------           35.3/46.2 MB 38.5 MB/s eta 0:00:01\n",
      "     -------------------------------         37.1/46.2 MB 38.5 MB/s eta 0:00:01\n",
      "     ---------------------------------       39.2/46.2 MB 40.9 MB/s eta 0:00:01\n",
      "     ----------------------------------      41.4/46.2 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------------------    43.7/46.2 MB 38.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  45.9/46.2 MB 43.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 43.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 43.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 43.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 43.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 43.5 MB/s eta 0:00:01\n",
      "     --------------------------------------- 46.2/46.2 MB 19.8 MB/s eta 0:00:00\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "  Downloading smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "                                              0.0/61.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 61.4/61.4 kB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: wrapt in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Collecting blis<1.1.0,>=1.0.0 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Downloading blis-1.0.1-cp311-cp311-win_amd64.whl (6.3 MB)\n",
      "                                              0.0/6.3 MB ? eta -:--:--\n",
      "     ----------                               1.6/6.3 MB 34.9 MB/s eta 0:00:01\n",
      "     ---------------------                    3.4/6.3 MB 43.5 MB/s eta 0:00:01\n",
      "     ---------------------                    3.4/6.3 MB 43.5 MB/s eta 0:00:01\n",
      "     -------------------------------          5.0/6.3 MB 26.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  6.3/6.3 MB 27.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 6.3/6.3 MB 22.5 MB/s eta 0:00:00\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "INFO: pip is looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n",
      "  Downloading thinc-8.3.1-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "                                              0.0/1.5 MB ? eta -:--:--\n",
      "     ----------------------------------       1.3/1.5 MB 40.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 31.5 MB/s eta 0:00:00\n",
      "  Downloading thinc-8.3.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "                                              0.0/1.5 MB ? eta -:--:--\n",
      "     ---------------------------------------  1.5/1.5 MB 47.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.5/1.5 MB 32.1 MB/s eta 0:00:00\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.13.0-cp311-cp311-win_amd64.whl (46.2 MB)\n",
      "                                              0.0/46.2 MB ? eta -:--:--\n",
      "     -                                        1.9/46.2 MB 40.7 MB/s eta 0:00:02\n",
      "     --                                       3.1/46.2 MB 39.6 MB/s eta 0:00:02\n",
      "     ----                                     5.2/46.2 MB 41.9 MB/s eta 0:00:01\n",
      "     ------                                   7.4/46.2 MB 39.4 MB/s eta 0:00:01\n",
      "     -------                                  8.4/46.2 MB 35.9 MB/s eta 0:00:02\n",
      "     --------                                10.0/46.2 MB 35.4 MB/s eta 0:00:02\n",
      "     ---------                               11.6/46.2 MB 34.4 MB/s eta 0:00:02\n",
      "     -----------                             13.4/46.2 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------                            15.0/46.2 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------                          16.9/46.2 MB 34.4 MB/s eta 0:00:01\n",
      "     ---------------                         18.4/46.2 MB 34.6 MB/s eta 0:00:01\n",
      "     -----------------                       20.5/46.2 MB 36.4 MB/s eta 0:00:01\n",
      "     -------------------                     23.1/46.2 MB 38.5 MB/s eta 0:00:01\n",
      "     ---------------------                   25.1/46.2 MB 38.6 MB/s eta 0:00:01\n",
      "     -----------------------                 27.4/46.2 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------                29.2/46.2 MB 38.5 MB/s eta 0:00:01\n",
      "     --------------------------              31.3/46.2 MB 36.4 MB/s eta 0:00:01\n",
      "     ----------------------------            33.3/46.2 MB 38.5 MB/s eta 0:00:01\n",
      "     -----------------------------           34.7/46.2 MB 38.6 MB/s eta 0:00:01\n",
      "     ------------------------------          36.2/46.2 MB 40.9 MB/s eta 0:00:01\n",
      "     --------------------------------        38.6/46.2 MB 38.5 MB/s eta 0:00:01\n",
      "     ---------------------------------       39.4/46.2 MB 34.4 MB/s eta 0:00:01\n",
      "     -----------------------------------     42.3/46.2 MB 34.4 MB/s eta 0:00:01\n",
      "     -------------------------------------   43.9/46.2 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.0/46.2 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 34.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 34.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 34.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 34.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 34.6 MB/s eta 0:00:01\n",
      "     --------------------------------------- 46.2/46.2 MB 17.7 MB/s eta 0:00:00\n",
      "  Downloading scipy-1.12.0-cp311-cp311-win_amd64.whl (46.2 MB)\n",
      "                                              0.0/46.2 MB ? eta -:--:--\n",
      "     -                                        1.3/46.2 MB 42.3 MB/s eta 0:00:02\n",
      "     -                                        1.8/46.2 MB 22.7 MB/s eta 0:00:02\n",
      "     -                                        2.1/46.2 MB 19.1 MB/s eta 0:00:03\n",
      "     --                                       2.4/46.2 MB 13.0 MB/s eta 0:00:04\n",
      "     --                                       2.9/46.2 MB 12.3 MB/s eta 0:00:04\n",
      "     ---                                      4.2/46.2 MB 14.9 MB/s eta 0:00:03\n",
      "     ----                                     5.2/46.2 MB 16.8 MB/s eta 0:00:03\n",
      "     -----                                    6.9/46.2 MB 19.1 MB/s eta 0:00:03\n",
      "     ------                                   7.9/46.2 MB 18.6 MB/s eta 0:00:03\n",
      "     -------                                  8.4/46.2 MB 19.8 MB/s eta 0:00:02\n",
      "     --------                                 9.4/46.2 MB 18.8 MB/s eta 0:00:02\n",
      "     ---------                               11.1/46.2 MB 18.2 MB/s eta 0:00:02\n",
      "     ----------                              12.6/46.2 MB 23.4 MB/s eta 0:00:02\n",
      "     ------------                            14.2/46.2 MB 24.2 MB/s eta 0:00:02\n",
      "     -------------                           16.0/46.2 MB 27.3 MB/s eta 0:00:02\n",
      "     ---------------                         17.9/46.2 MB 28.4 MB/s eta 0:00:01\n",
      "     ----------------                        19.1/46.2 MB 32.7 MB/s eta 0:00:01\n",
      "     -----------------                       20.5/46.2 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------                      21.5/46.2 MB 31.1 MB/s eta 0:00:01\n",
      "     -------------------                     23.1/46.2 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------                    24.4/46.2 MB 29.7 MB/s eta 0:00:01\n",
      "     ---------------------                   25.5/46.2 MB 28.4 MB/s eta 0:00:01\n",
      "     ----------------------                  26.2/46.2 MB 27.3 MB/s eta 0:00:01\n",
      "     -----------------------                 27.8/46.2 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------------                29.0/46.2 MB 26.2 MB/s eta 0:00:01\n",
      "     -------------------------               30.3/46.2 MB 25.2 MB/s eta 0:00:01\n",
      "     ---------------------------             32.3/46.2 MB 26.2 MB/s eta 0:00:01\n",
      "     ----------------------------            33.2/46.2 MB 26.2 MB/s eta 0:00:01\n",
      "     ----------------------------            34.1/46.2 MB 25.2 MB/s eta 0:00:01\n",
      "     ------------------------------          35.7/46.2 MB 26.2 MB/s eta 0:00:01\n",
      "     -------------------------------         37.4/46.2 MB 29.7 MB/s eta 0:00:01\n",
      "     ---------------------------------       39.4/46.2 MB 32.7 MB/s eta 0:00:01\n",
      "     ----------------------------------      41.2/46.2 MB 32.8 MB/s eta 0:00:01\n",
      "     -----------------------------------     42.3/46.2 MB 29.7 MB/s eta 0:00:01\n",
      "     -------------------------------------   44.7/46.2 MB 34.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 36.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 36.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 36.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 36.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  46.2/46.2 MB 36.3 MB/s eta 0:00:01\n",
      "     --------------------------------------- 46.2/46.2 MB 16.8 MB/s eta 0:00:00\n",
      "INFO: pip is looking at multiple versions of thinc to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading scipy-1.11.4-cp311-cp311-win_amd64.whl (44.1 MB)\n",
      "                                              0.0/44.1 MB ? eta -:--:--\n",
      "     -                                        1.5/44.1 MB 31.4 MB/s eta 0:00:02\n",
      "     ---                                      3.4/44.1 MB 43.2 MB/s eta 0:00:01\n",
      "     ----                                     4.9/44.1 MB 34.9 MB/s eta 0:00:02\n",
      "     -----                                    6.5/44.1 MB 37.7 MB/s eta 0:00:01\n",
      "     -------                                  8.1/44.1 MB 37.0 MB/s eta 0:00:01\n",
      "     --------                                 9.4/44.1 MB 33.4 MB/s eta 0:00:02\n",
      "     ---------                               11.1/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     -----------                             13.3/44.1 MB 34.4 MB/s eta 0:00:01\n",
      "     -------------                           15.4/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------                          16.9/44.1 MB 36.3 MB/s eta 0:00:01\n",
      "     ----------------                        18.8/44.1 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------------                      20.8/44.1 MB 38.6 MB/s eta 0:00:01\n",
      "     --------------------                    22.8/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     ---------------------                   24.2/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     ----------------------                  25.9/44.1 MB 36.3 MB/s eta 0:00:01\n",
      "     ------------------------                27.2/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     -------------------------               29.3/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     ---------------------------             30.6/44.1 MB 32.7 MB/s eta 0:00:01\n",
      "     ----------------------------            32.3/44.1 MB 32.7 MB/s eta 0:00:01\n",
      "     ------------------------------          34.5/44.1 MB 36.3 MB/s eta 0:00:01\n",
      "     ------------------------------          34.7/44.1 MB 31.1 MB/s eta 0:00:01\n",
      "     --------------------------------        36.5/44.1 MB 31.2 MB/s eta 0:00:01\n",
      "     ----------------------------------      38.5/44.1 MB 32.8 MB/s eta 0:00:01\n",
      "     -----------------------------------     40.1/44.1 MB 34.4 MB/s eta 0:00:01\n",
      "     -------------------------------------   42.1/44.1 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.1/44.1 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.1/44.1 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.1/44.1 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.1/44.1 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.1/44.1 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 44.1/44.1 MB 17.2 MB/s eta 0:00:00\n",
      "  Downloading scipy-1.11.3-cp311-cp311-win_amd64.whl (44.1 MB)\n",
      "                                              0.0/44.1 MB ? eta -:--:--\n",
      "     -                                        1.3/44.1 MB 27.2 MB/s eta 0:00:02\n",
      "     --                                       2.4/44.1 MB 25.6 MB/s eta 0:00:02\n",
      "     ---                                      4.0/44.1 MB 31.7 MB/s eta 0:00:02\n",
      "     -----                                    5.6/44.1 MB 32.4 MB/s eta 0:00:02\n",
      "     ------                                   7.3/44.1 MB 35.9 MB/s eta 0:00:02\n",
      "     -------                                  8.4/44.1 MB 33.5 MB/s eta 0:00:02\n",
      "     -------                                  8.6/44.1 MB 28.9 MB/s eta 0:00:02\n",
      "     ---------                               10.4/44.1 MB 28.4 MB/s eta 0:00:02\n",
      "     ----------                              11.7/44.1 MB 27.3 MB/s eta 0:00:02\n",
      "     ----------                              12.0/44.1 MB 26.2 MB/s eta 0:00:02\n",
      "     -----------                             12.6/44.1 MB 24.2 MB/s eta 0:00:02\n",
      "     ------------                            14.2/44.1 MB 23.4 MB/s eta 0:00:02\n",
      "     --------------                          16.1/44.1 MB 24.2 MB/s eta 0:00:02\n",
      "     ---------------                         17.8/44.1 MB 23.4 MB/s eta 0:00:02\n",
      "     ----------------                        18.9/44.1 MB 27.3 MB/s eta 0:00:01\n",
      "     ----------------                        19.0/44.1 MB 23.4 MB/s eta 0:00:02\n",
      "     -----------------                       19.9/44.1 MB 21.8 MB/s eta 0:00:02\n",
      "     -------------------                     21.7/44.1 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------                    23.3/44.1 MB 26.2 MB/s eta 0:00:01\n",
      "     ---------------------                   24.8/44.1 MB 26.2 MB/s eta 0:00:01\n",
      "     ----------------------                  25.2/44.1 MB 27.3 MB/s eta 0:00:01\n",
      "     -----------------------                 26.1/44.1 MB 23.4 MB/s eta 0:00:01\n",
      "     ------------------------                27.3/44.1 MB 22.6 MB/s eta 0:00:01\n",
      "     -------------------------               28.3/44.1 MB 22.6 MB/s eta 0:00:01\n",
      "     -------------------------               28.6/44.1 MB 20.5 MB/s eta 0:00:01\n",
      "     ---------------------------             30.6/44.1 MB 25.2 MB/s eta 0:00:01\n",
      "     ----------------------------            32.6/44.1 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------------------          34.5/44.1 MB 27.3 MB/s eta 0:00:01\n",
      "     --------------------------------        36.3/44.1 MB 31.2 MB/s eta 0:00:01\n",
      "     ---------------------------------       37.7/44.1 MB 31.2 MB/s eta 0:00:01\n",
      "     ----------------------------------      39.5/44.1 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------    40.9/44.1 MB 36.4 MB/s eta 0:00:01\n",
      "     -------------------------------------   42.7/44.1 MB 32.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  43.0/44.1 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  43.1/44.1 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  43.8/44.1 MB 24.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.1/44.1 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.1/44.1 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.1/44.1 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.1/44.1 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.1/44.1 MB 23.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 44.1/44.1 MB 13.3 MB/s eta 0:00:00\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading scipy-1.11.2-cp311-cp311-win_amd64.whl (44.0 MB)\n",
      "                                              0.0/44.0 MB ? eta -:--:--\n",
      "     -                                        1.9/44.0 MB 39.9 MB/s eta 0:00:02\n",
      "     --                                       2.6/44.0 MB 28.0 MB/s eta 0:00:02\n",
      "     ----                                     4.5/44.0 MB 31.9 MB/s eta 0:00:02\n",
      "     -----                                    6.3/44.0 MB 33.5 MB/s eta 0:00:02\n",
      "     --------                                 8.9/44.0 MB 33.7 MB/s eta 0:00:02\n",
      "     ---------                               10.9/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "     ----------                              12.2/44.0 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------                            14.0/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------                          16.1/44.0 MB 38.5 MB/s eta 0:00:01\n",
      "     ----------------                        18.1/44.0 MB 38.5 MB/s eta 0:00:01\n",
      "     -----------------                       20.0/44.0 MB 38.5 MB/s eta 0:00:01\n",
      "     -------------------                     21.7/44.0 MB 36.3 MB/s eta 0:00:01\n",
      "     --------------------                    23.2/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     ----------------------                  25.1/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     -----------------------                 26.3/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "     -------------------------               28.7/44.0 MB 36.3 MB/s eta 0:00:01\n",
      "     ---------------------------             30.7/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "     ----------------------------            32.3/44.0 MB 38.6 MB/s eta 0:00:01\n",
      "     ------------------------------          34.7/44.0 MB 38.5 MB/s eta 0:00:01\n",
      "     -------------------------------         35.9/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     ---------------------------------       38.2/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     -----------------------------------     39.5/44.0 MB 36.3 MB/s eta 0:00:01\n",
      "     ------------------------------------    41.0/44.0 MB 38.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.9/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 44.0/44.0 MB 15.2 MB/s eta 0:00:00\n",
      "  Downloading scipy-1.11.1-cp311-cp311-win_amd64.whl (44.0 MB)\n",
      "                                              0.0/44.0 MB ? eta -:--:--\n",
      "     -                                        1.5/44.0 MB 49.5 MB/s eta 0:00:01\n",
      "     ---                                      3.4/44.0 MB 43.2 MB/s eta 0:00:01\n",
      "     ----                                     4.6/44.0 MB 37.0 MB/s eta 0:00:02\n",
      "     ----                                     5.5/44.0 MB 31.8 MB/s eta 0:00:02\n",
      "     ------                                   6.8/44.0 MB 31.2 MB/s eta 0:00:02\n",
      "     -------                                  8.4/44.0 MB 31.7 MB/s eta 0:00:02\n",
      "     ---------                               10.6/44.0 MB 32.7 MB/s eta 0:00:02\n",
      "     ----------                              12.3/44.0 MB 32.7 MB/s eta 0:00:01\n",
      "     -------------                           14.7/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "     -------------                           15.1/44.0 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------                          16.8/44.0 MB 38.5 MB/s eta 0:00:01\n",
      "     ----------------                        18.3/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "     ----------------                        18.9/44.0 MB 36.3 MB/s eta 0:00:01\n",
      "     ------------------                      20.4/44.0 MB 31.2 MB/s eta 0:00:01\n",
      "     -------------------                     22.1/44.0 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------                    23.4/44.0 MB 29.7 MB/s eta 0:00:01\n",
      "     ----------------------                  25.0/44.0 MB 28.5 MB/s eta 0:00:01\n",
      "     -----------------------                 26.2/44.0 MB 27.3 MB/s eta 0:00:01\n",
      "     ------------------------                28.0/44.0 MB 28.4 MB/s eta 0:00:01\n",
      "     --------------------------              29.6/44.0 MB 32.7 MB/s eta 0:00:01\n",
      "     ---------------------------             31.4/44.0 MB 31.2 MB/s eta 0:00:01\n",
      "     -----------------------------           33.1/44.0 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------          34.9/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------        36.9/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     ----------------------------------      38.8/44.0 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------------------    40.7/44.0 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------------------------------    41.2/44.0 MB 32.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  43.0/44.0 MB 32.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 32.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 32.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 32.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 32.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  44.0/44.0 MB 32.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 44.0/44.0 MB 17.7 MB/s eta 0:00:00\n",
      "  Downloading scipy-1.10.1-cp311-cp311-win_amd64.whl (42.2 MB)\n",
      "                                              0.0/42.2 MB ? eta -:--:--\n",
      "     -                                        1.5/42.2 MB 31.8 MB/s eta 0:00:02\n",
      "     ---                                      3.8/42.2 MB 40.7 MB/s eta 0:00:01\n",
      "     ----                                     5.0/42.2 MB 31.7 MB/s eta 0:00:02\n",
      "     ------                                   6.9/42.2 MB 36.8 MB/s eta 0:00:01\n",
      "     --------                                 8.5/42.2 MB 34.1 MB/s eta 0:00:01\n",
      "     ---------                               10.0/42.2 MB 35.5 MB/s eta 0:00:01\n",
      "     ----------                              11.8/42.2 MB 36.4 MB/s eta 0:00:01\n",
      "     ------------                            13.7/42.2 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------                          15.7/42.2 MB 38.5 MB/s eta 0:00:01\n",
      "     ----------------                        17.8/42.2 MB 38.5 MB/s eta 0:00:01\n",
      "     ------------------                      19.5/42.2 MB 38.6 MB/s eta 0:00:01\n",
      "     -------------------                     21.3/42.2 MB 40.9 MB/s eta 0:00:01\n",
      "     ---------------------                   23.0/42.2 MB 38.5 MB/s eta 0:00:01\n",
      "     -----------------------                 24.9/42.2 MB 36.3 MB/s eta 0:00:01\n",
      "     ------------------------                26.5/42.2 MB 38.5 MB/s eta 0:00:01\n",
      "     --------------------------              28.5/42.2 MB 34.6 MB/s eta 0:00:01\n",
      "     ---------------------------             29.7/42.2 MB 34.4 MB/s eta 0:00:01\n",
      "     ---------------------------             30.2/42.2 MB 34.4 MB/s eta 0:00:01\n",
      "     ---------------------------             30.2/42.2 MB 34.4 MB/s eta 0:00:01\n",
      "     ----------------------------            30.5/42.2 MB 24.2 MB/s eta 0:00:01\n",
      "     ------------------------------          33.5/42.2 MB 26.2 MB/s eta 0:00:01\n",
      "     ---------------------------------       36.5/42.2 MB 27.3 MB/s eta 0:00:01\n",
      "     -----------------------------------     38.9/42.2 MB 29.7 MB/s eta 0:00:01\n",
      "     -------------------------------------   41.0/42.2 MB 59.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 54.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 54.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 54.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 54.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 54.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 42.2/42.2 MB 21.8 MB/s eta 0:00:00\n",
      "  Downloading scipy-1.10.0-cp311-cp311-win_amd64.whl (42.2 MB)\n",
      "                                              0.0/42.2 MB ? eta -:--:--\n",
      "     -                                        1.6/42.2 MB 50.5 MB/s eta 0:00:01\n",
      "     --                                       3.1/42.2 MB 40.2 MB/s eta 0:00:01\n",
      "     ----                                     4.4/42.2 MB 35.3 MB/s eta 0:00:02\n",
      "     -----                                    5.4/42.2 MB 31.4 MB/s eta 0:00:02\n",
      "     ------                                   7.3/42.2 MB 36.1 MB/s eta 0:00:01\n",
      "     --------                                 9.0/42.2 MB 33.8 MB/s eta 0:00:01\n",
      "     ---------                               10.8/42.2 MB 34.6 MB/s eta 0:00:01\n",
      "     -----------                             12.6/42.2 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------                            13.9/42.2 MB 32.7 MB/s eta 0:00:01\n",
      "     -------------                           15.1/42.2 MB 31.2 MB/s eta 0:00:01\n",
      "     ---------------                         16.8/42.2 MB 36.4 MB/s eta 0:00:01\n",
      "     ---------------                         17.2/42.2 MB 29.7 MB/s eta 0:00:01\n",
      "     -----------------                       19.1/42.2 MB 31.2 MB/s eta 0:00:01\n",
      "     -------------------                     21.3/42.2 MB 32.7 MB/s eta 0:00:01\n",
      "     ---------------------                   23.0/42.2 MB 31.2 MB/s eta 0:00:01\n",
      "     -----------------------                 25.2/42.2 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------------------                26.9/42.2 MB 32.7 MB/s eta 0:00:01\n",
      "     --------------------------              29.2/42.2 MB 40.9 MB/s eta 0:00:01\n",
      "     ----------------------------            31.0/42.2 MB 40.9 MB/s eta 0:00:01\n",
      "     ------------------------------          33.1/42.2 MB 40.9 MB/s eta 0:00:01\n",
      "     --------------------------------        34.9/42.2 MB 40.9 MB/s eta 0:00:01\n",
      "     ---------------------------------       36.7/42.2 MB 38.5 MB/s eta 0:00:01\n",
      "     -----------------------------------     38.5/42.2 MB 38.5 MB/s eta 0:00:01\n",
      "     -------------------------------------   40.6/42.2 MB 38.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  41.3/42.2 MB 43.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.2/42.2 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 42.2/42.2 MB 15.2 MB/s eta 0:00:00\n",
      "  Downloading scipy-1.9.3-cp311-cp311-win_amd64.whl (39.9 MB)\n",
      "                                              0.0/39.9 MB ? eta -:--:--\n",
      "     -                                        1.3/39.9 MB 27.7 MB/s eta 0:00:02\n",
      "     ---                                      3.1/39.9 MB 32.8 MB/s eta 0:00:02\n",
      "     -----                                    5.1/39.9 MB 40.6 MB/s eta 0:00:01\n",
      "     ------                                   6.3/39.9 MB 33.6 MB/s eta 0:00:01\n",
      "     --------                                 8.6/39.9 MB 34.2 MB/s eta 0:00:01\n",
      "     ---------                                9.2/39.9 MB 32.9 MB/s eta 0:00:01\n",
      "     ----------                              11.0/39.9 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------                            12.4/39.9 MB 32.8 MB/s eta 0:00:01\n",
      "     --------------                          14.5/39.9 MB 34.4 MB/s eta 0:00:01\n",
      "     ---------------                         16.1/39.9 MB 34.4 MB/s eta 0:00:01\n",
      "     -----------------                       18.0/39.9 MB 36.3 MB/s eta 0:00:01\n",
      "     -------------------                     19.8/39.9 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------                    21.4/39.9 MB 36.4 MB/s eta 0:00:01\n",
      "     ----------------------                  23.1/39.9 MB 38.6 MB/s eta 0:00:01\n",
      "     ------------------------                25.0/39.9 MB 38.6 MB/s eta 0:00:01\n",
      "     --------------------------              27.0/39.9 MB 40.9 MB/s eta 0:00:01\n",
      "     ---------------------------             28.6/39.9 MB 36.3 MB/s eta 0:00:01\n",
      "     -----------------------------           30.4/39.9 MB 36.4 MB/s eta 0:00:01\n",
      "     -------------------------------         32.3/39.9 MB 36.4 MB/s eta 0:00:01\n",
      "     ---------------------------------       33.8/39.9 MB 34.4 MB/s eta 0:00:01\n",
      "     ----------------------------------      35.7/39.9 MB 36.4 MB/s eta 0:00:01\n",
      "     -------------------------------------   38.0/39.9 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  39.9/39.9 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  39.9/39.9 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  39.9/39.9 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  39.9/39.9 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 39.9/39.9 MB 18.7 MB/s eta 0:00:00\n",
      "Collecting numpy>=1.23 (from matplotlib)\n",
      "  Downloading numpy-1.25.2-cp311-cp311-win_amd64.whl (15.5 MB)\n",
      "                                              0.0/15.5 MB ? eta -:--:--\n",
      "     ----                                     1.8/15.5 MB 37.0 MB/s eta 0:00:01\n",
      "     -------                                  3.0/15.5 MB 38.0 MB/s eta 0:00:01\n",
      "     ------------                             4.8/15.5 MB 30.3 MB/s eta 0:00:01\n",
      "     -----------------                        6.8/15.5 MB 36.1 MB/s eta 0:00:01\n",
      "     --------------------                     8.0/15.5 MB 31.9 MB/s eta 0:00:01\n",
      "     -------------------------               10.0/15.5 MB 33.5 MB/s eta 0:00:01\n",
      "     ---------------------------             11.1/15.5 MB 32.8 MB/s eta 0:00:01\n",
      "     ------------------------------          12.3/15.5 MB 31.2 MB/s eta 0:00:01\n",
      "     -----------------------------------     14.2/15.5 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  15.5/15.5 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  15.5/15.5 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 15.5/15.5 MB 24.2 MB/s eta 0:00:00\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Downloading scipy-1.9.2-cp311-cp311-win_amd64.whl (39.9 MB)\n",
      "                                              0.0/39.9 MB ? eta -:--:--\n",
      "     -                                        1.2/39.9 MB 24.4 MB/s eta 0:00:02\n",
      "     --                                       2.1/39.9 MB 33.0 MB/s eta 0:00:02\n",
      "     --                                       2.1/39.9 MB 33.0 MB/s eta 0:00:02\n",
      "     --                                       2.1/39.9 MB 33.0 MB/s eta 0:00:02\n",
      "     --                                       2.1/39.9 MB 8.9 MB/s eta 0:00:05\n",
      "     ---                                      3.3/39.9 MB 11.0 MB/s eta 0:00:04\n",
      "     ----                                     4.2/39.9 MB 13.3 MB/s eta 0:00:03\n",
      "     ----                                     4.2/39.9 MB 13.3 MB/s eta 0:00:03\n",
      "     -----                                    5.1/39.9 MB 11.6 MB/s eta 0:00:03\n",
      "     -----                                    5.7/39.9 MB 11.7 MB/s eta 0:00:03\n",
      "     ------                                   6.8/39.9 MB 12.4 MB/s eta 0:00:03\n",
      "     --------                                 8.3/39.9 MB 14.4 MB/s eta 0:00:03\n",
      "     ---------                                9.4/39.9 MB 15.4 MB/s eta 0:00:02\n",
      "     ---------                                9.6/39.9 MB 14.2 MB/s eta 0:00:03\n",
      "     ----------                              10.2/39.9 MB 14.5 MB/s eta 0:00:03\n",
      "     ----------                              10.5/39.9 MB 13.9 MB/s eta 0:00:03\n",
      "     -----------                             11.7/39.9 MB 13.4 MB/s eta 0:00:03\n",
      "     -------------                           13.4/39.9 MB 18.2 MB/s eta 0:00:02\n",
      "     --------------                          14.7/39.9 MB 22.6 MB/s eta 0:00:02\n",
      "     --------------                          14.7/39.9 MB 22.6 MB/s eta 0:00:02\n",
      "     --------------                          14.7/39.9 MB 22.6 MB/s eta 0:00:02\n",
      "     ---------------                         15.7/39.9 MB 18.2 MB/s eta 0:00:02\n",
      "     ----------------                        16.7/39.9 MB 17.7 MB/s eta 0:00:02\n",
      "     -----------------                       17.8/39.9 MB 18.2 MB/s eta 0:00:02\n",
      "     ------------------                      18.7/39.9 MB 17.7 MB/s eta 0:00:02\n",
      "     -------------------                     19.9/39.9 MB 18.7 MB/s eta 0:00:02\n",
      "     -------------------                     19.9/39.9 MB 18.7 MB/s eta 0:00:02\n",
      "     -------------------                     19.9/39.9 MB 18.7 MB/s eta 0:00:02\n",
      "     -------------------                     20.2/39.9 MB 14.5 MB/s eta 0:00:02\n",
      "     ----------------------                  22.6/39.9 MB 16.0 MB/s eta 0:00:02\n",
      "     -----------------------                 24.1/39.9 MB 15.6 MB/s eta 0:00:02\n",
      "     ------------------------                25.4/39.9 MB 19.3 MB/s eta 0:00:01\n",
      "     --------------------------              27.2/39.9 MB 20.5 MB/s eta 0:00:01\n",
      "     --------------------------              27.5/39.9 MB 19.8 MB/s eta 0:00:01\n",
      "     -----------------------------           29.7/39.9 MB 21.1 MB/s eta 0:00:01\n",
      "     ------------------------------          31.0/39.9 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------        33.1/39.9 MB 31.2 MB/s eta 0:00:01\n",
      "     --------------------------------        33.5/39.9 MB 29.7 MB/s eta 0:00:01\n",
      "     ---------------------------------       33.9/39.9 MB 22.6 MB/s eta 0:00:01\n",
      "     ---------------------------------       34.6/39.9 MB 22.6 MB/s eta 0:00:01\n",
      "     ----------------------------------      35.2/39.9 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------    37.3/39.9 MB 19.9 MB/s eta 0:00:01\n",
      "     -------------------------------------   38.6/39.9 MB 20.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  39.9/39.9 MB 19.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  39.9/39.9 MB 19.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  39.9/39.9 MB 19.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  39.9/39.9 MB 19.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  39.9/39.9 MB 19.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  39.9/39.9 MB 19.8 MB/s eta 0:00:01\n",
      "     --------------------------------------  39.9/39.9 MB 19.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 39.9/39.9 MB 11.7 MB/s eta 0:00:00\n",
      "  Downloading scipy-1.9.1.tar.gz (42.0 MB)\n",
      "                                              0.0/42.0 MB ? eta -:--:--\n",
      "     --                                       2.2/42.0 MB 67.2 MB/s eta 0:00:01\n",
      "     ---                                      3.2/42.0 MB 41.1 MB/s eta 0:00:01\n",
      "     -----                                    5.3/42.0 MB 37.7 MB/s eta 0:00:01\n",
      "     ------                                   7.3/42.0 MB 42.5 MB/s eta 0:00:01\n",
      "     -------                                  8.2/42.0 MB 37.2 MB/s eta 0:00:01\n",
      "     ---------                                9.8/42.0 MB 36.7 MB/s eta 0:00:01\n",
      "     ----------                              11.7/42.0 MB 34.4 MB/s eta 0:00:01\n",
      "     ------------                            13.4/42.0 MB 34.4 MB/s eta 0:00:01\n",
      "     --------------                          15.3/42.0 MB 34.4 MB/s eta 0:00:01\n",
      "     ---------------                         16.6/42.0 MB 32.8 MB/s eta 0:00:01\n",
      "     ----------------                        17.8/42.0 MB 36.3 MB/s eta 0:00:01\n",
      "     -----------------                       18.8/42.0 MB 28.5 MB/s eta 0:00:01\n",
      "     ------------------                      19.9/42.0 MB 28.4 MB/s eta 0:00:01\n",
      "     -------------------                     21.0/42.0 MB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------                   23.2/42.0 MB 23.4 MB/s eta 0:00:01\n",
      "     -----------------------                 25.6/42.0 MB 24.2 MB/s eta 0:00:01\n",
      "     -------------------------               27.0/42.0 MB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------------             29.3/42.0 MB 29.8 MB/s eta 0:00:01\n",
      "     ----------------------------            30.6/42.0 MB 31.2 MB/s eta 0:00:01\n",
      "     ------------------------------          33.1/42.0 MB 31.2 MB/s eta 0:00:01\n",
      "     -------------------------------         34.4/42.0 MB 34.4 MB/s eta 0:00:01\n",
      "     ----------------------------------      36.7/42.0 MB 34.4 MB/s eta 0:00:01\n",
      "     -----------------------------------     37.7/42.0 MB 34.4 MB/s eta 0:00:01\n",
      "     -------------------------------------   40.1/42.0 MB 32.7 MB/s eta 0:00:01\n",
      "     --------------------------------------  41.5/42.0 MB 38.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.0/42.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.0/42.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.0/42.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  42.0/42.0 MB 36.4 MB/s eta 0:00:01\n",
      "     --------------------------------------- 42.0/42.0 MB 19.3 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: still running...\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade nltk openpyxl matplotlib textblob spacy gensim scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (4.66.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\pavan teja\\documents\\textanalyticsproject\\ism6564_textanalytics_project\\ism6564\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting onnx==1.16.1\n",
      "  Using cached onnx-1.16.1-cp312-cp312-win_amd64.whl.metadata (16 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnx==1.16.1) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from onnx==1.16.1) (5.28.3)\n",
      "Using cached onnx-1.16.1-cp312-cp312-win_amd64.whl (14.4 MB)\n",
      "Installing collected packages: onnx\n",
      "  Attempting uninstall: onnx\n",
      "    Found existing installation: onnx 1.17.0\n",
      "    Uninstalling onnx-1.17.0:\n",
      "      Successfully uninstalled onnx-1.17.0\n",
      "Successfully installed onnx-1.16.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install onnx==1.16.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import textblob \n",
    "#!python -m textblob.download_corpora\n",
    "#import spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#import nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('universal_tagset')\n",
    "#nltk.download('averaged_perceptron_tagger_eng')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('punkt_tab')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.9.1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from torch) (2024.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from torch) (75.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (4.46.1)\n",
      "Requirement already satisfied: sentence_transformers in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from transformers) (4.66.6)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from sentence_transformers) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from sentence_transformers) (1.5.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: Pillow in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from sentence_transformers) (11.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (75.3.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\derek\\onedrive\\1 - master degree\\s3-ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in c:\\users\\derek\\sourcecode\\ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (75.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting html-to-json\n",
      "  Using cached html_to_json-2.0.0-py2.py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: bs4 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from html-to-json) (0.0.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bs4->html-to-json) (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\derek\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from beautifulsoup4->bs4->html-to-json) (2.6)\n",
      "Using cached html_to_json-2.0.0-py2.py3-none-any.whl (6.4 kB)\n",
      "Installing collected packages: html-to-json\n",
      "Successfully installed html-to-json-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install html-to-json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Environment Parameters\n",
    "Prepare the list of parameter in .env file for later use. \n",
    "Parameters: \n",
    "- API keys for LLMs\n",
    "    - OPENAI_API_KEY \n",
    "    - HUGGINGFACEHUB_API_TOKEN \n",
    "- Directory / location for documents and vector databases\n",
    "    - DOC_ARVIX = \"./source/from_arvix/\"\n",
    "    - DOC_WIKI = \"./source/from_wiki/\"\n",
    "    - VECTORDB_OPENAI_EM = \"./vector_db/openai_embedding/\"\n",
    "    - VECTORDB_MINILM_EM = \"./vector_db/gpt4all_miniLM/\"\n",
    "    - TS_RAGAS = \"./evaluation/testset/by_RAGAS/\"\n",
    "    - TS_PROMPT = \"./evaluation/testset/by_direct_prompt/\"\n",
    "    - EVAL_DATASET = \"./evaluation/evaluation_data_set/\"\n",
    "    - EVAL_METRIC = \"./evaluation/evaluation_metric\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Simple RAG Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"diagrams/HL architecture.png\" alt=\"HL arc\" title= \"HL Architecture\" />\n",
    "\n",
    "The system comprises of 5 components: \n",
    "\n",
    "- Internal data, documents: The system starts with a collection of internal documents and / or structured databases. Documents can be in text, PDF, photo or video formats. These documents and data are sources for the specified knowledgebase.\n",
    "\n",
    "- Embedding processor: The documents and database entries are processed to create vector embeddings. Embeddings are numerical representations of the documents in a high-dimensional space that capture their semantic meaning. \n",
    "\n",
    "- Vector database: the vectorized chunk of documents and database entries are stored on vector database to be search and retrieved in a later stage. \n",
    "\n",
    "- Query processor: The query processor takes the user's query and performs semantic search against the vectorized database. This component ensures that the query is interpreted correctly and retrieves relevant document embeddings from the vectorized DB. It combines the user's original query with the retrieved document embeddings to form a context-rich query. This augmented query provides additional context that can help in generating a more accurate and relevant response.\n",
    "\n",
    "- LLM: pre-trained large language model where the augmented query is passed to for generating a response based on the query and the relevant documents.\n",
    "\n",
    "The system involves 2 main pipelines: the embedding pipeline and the retrieval pipeline. Each pipeline has specific stages and processes that contribute to the overall functionality of the system.\n",
    "\n",
    "In this experiment, we use Langchain as a framework to build a simple RAG as a chain of tasks, which interacts with surrounding services like parsing, embedding, vector database and LLMs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. MultiModal RAG Architecture\n",
    "<img src=\"diagrams/ISM6564-Project.png\" alt=\"HL arc\" title= \"MM HL Architecture\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the environment parameters\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Ingestion Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we load data from various sources. Make them ready to ingest.\n",
    "We will download 5 articles from ARVIX with query \"RAG for Large Language Model\" and store them locally and ready for next steps of embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From ARXIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv \n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "  query = \"RAG for Large Language Model\",     # To get more of other topics and number of papers. \n",
    "  max_results = 5,\n",
    "#  sort_by = arxiv.SortCriterion.SubmittedDate\n",
    ")\n",
    "\n",
    "results = client.results(search)\n",
    "all_results = list(client.results(search)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks http://arxiv.org/abs/2407.21059v1\n",
      "RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented Generation http://arxiv.org/abs/2408.02545v1\n",
      "MultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries http://arxiv.org/abs/2401.15391v1\n",
      "EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge Update http://arxiv.org/abs/2410.20299v1\n",
      "Astute RAG: Overcoming Imperfect Retrieval Augmentation and Knowledge Conflicts for Large Language Models http://arxiv.org/abs/2410.07176v1\n"
     ]
    }
   ],
   "source": [
    "# Print out the articles' titles\n",
    "for r in all_results:\n",
    "    print(f\"{r.title} {r.entry_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purpose: download articles and save them in pre-defined location for later use\n",
    "# Prepare: create the environment paramter DOC_ARVIX for the path to save articles. \n",
    "# Download and save articles in PDF format to the \"RAG_for_LLM\" folder under ARVIX_DOC path\n",
    "DOC_ARVIX = os.getenv(\"DOC_ARVIX\") \n",
    "directory_path = os.path.join(DOC_ARVIX) \n",
    "if not os.path.exists(directory_path):\n",
    "    os.makedirs(directory_path)\n",
    "for r in all_results:\n",
    "    r.download_pdf(dirpath=directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Springer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Lexis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step and the previous one are usually processed together. I try to separate them to make attention that these are not always coupled.\n",
    "We use available library DirectoryLoader and PyMuPDFLoader from Langchain to load and parse all .pdf files in the directory.\n",
    "We can use corresponding loader for other data types such as excel, presentation, unstructured ... \n",
    "\n",
    "Refer to https://python.langchain.com/v0.1/docs/integrations/document_loaders/ for other available loaders. \n",
    "We also use the OCR library rapidocr to extract image as text. Certainly, the trade-off is processing time. It took 18 minutes to parse 5 pdf files with OCR compared to 0.1 second without. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Util functions for Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract document elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.documents.elements import NarrativeText\n",
    "\n",
    "\n",
    "# Extract elements from PDF\n",
    "def extract_pdf_elements(path, fname,img_path=\"\"):\n",
    "    \"\"\"\n",
    "    Extract images, tables, and chunk text from a PDF file.\n",
    "    path: File path, which is used to dump images (.jpg)\n",
    "    fname: File name\n",
    "    \"\"\"\n",
    "    if img_path == \"\":\n",
    "        img_path = path\n",
    "    if not os.path.exists(img_path):\n",
    "        os.makedirs(img_path)\n",
    "    return partition_pdf(\n",
    "        filename=path + fname,\n",
    "        extract_images_in_pdf=True,\n",
    "        infer_table_structure=True,\n",
    "        chunking_strategy=\"by_title\",\n",
    "        max_characters=4000,\n",
    "        new_after_n_chars=3800,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        image_output_dir_path=img_path,\n",
    "        form_extraction_skip_tables=False,\n",
    "        extract_image_block_output_dir = img_path\n",
    "    )\n",
    "\n",
    "def extract_pdf_elements_v2(path, fname,img_path=\"\"):\n",
    "    \"\"\"\n",
    "    Extract images, tables, and chunk text from a PDF file.\n",
    "    path: File path, which is used to dump images (.jpg)\n",
    "    fname: File name\n",
    "    \"\"\"\n",
    "    if img_path == \"\":\n",
    "        img_path = path\n",
    "    if not os.path.exists(img_path):\n",
    "        os.makedirs(img_path)\n",
    "    return partition_pdf(\n",
    "        filename=path + fname,\n",
    "        extract_images_in_pdf=True,\n",
    "        infer_table_structure=True,\n",
    "        strategy=\"hi_res\",\n",
    "        max_characters=4000,\n",
    "        new_after_n_chars=3800,\n",
    "        combine_text_under_n_chars=2000,\n",
    "        image_output_dir_path=img_path,\n",
    "        form_extraction_skip_tables=False,\n",
    "        extract_image_block_output_dir = img_path\n",
    "    )\n",
    "\n",
    "# Categorize elements by type\n",
    "def categorize_elements(raw_pdf_elements):\n",
    "    \"\"\"\n",
    "    Categorize extracted elements from a PDF into tables and texts.\n",
    "    raw_pdf_elements: List of unstructured.documents.elements\n",
    "    \"\"\"\n",
    "    tables = []\n",
    "    texts = []\n",
    "    for element in raw_pdf_elements:\n",
    "        if \"unstructured.documents.elements.Table\" in str(type(element)):\n",
    "            tables.append(element.to_dict()[\"metadata\"][\"text_as_html\"])\n",
    "        elif \"unstructured.documents.elements.CompositeElement\" in str(type(element)):\n",
    "            texts.append(str(element))\n",
    "    return texts, tables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Generate summaries of text elements\n",
    "def generate_text_summaries(texts, tables, model, summarize_texts=False):\n",
    "    \"\"\"\n",
    "    Summarize text elements\n",
    "    texts: List of str\n",
    "    tables: List of str\n",
    "    summarize_texts: Bool to summarize texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Prompt\n",
    "    prompt_text = \"\"\"You are an assistant tasked with summarizing tables and text for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw text or table elements. \\\n",
    "    Give a concise summary of the table or text that is well optimized for retrieval. Table or text: {element} \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_text)\n",
    "\n",
    "    # Text summary chain\n",
    "    summarize_chain = {\"element\": lambda x: x} | prompt | model | StrOutputParser()\n",
    "\n",
    "    # Initialize empty summaries\n",
    "    text_summaries = []\n",
    "    table_summaries = []\n",
    "\n",
    "    # Apply to text if texts are provided and summarization is requested\n",
    "    if texts and summarize_texts:\n",
    "        text_summaries = summarize_chain.batch(texts, {\"max_concurrency\": 5})\n",
    "    elif texts:\n",
    "        text_summaries = texts\n",
    "\n",
    "    # Apply to tables if tables are provided\n",
    "    if tables:\n",
    "        table_summaries = summarize_chain.batch(tables, {\"max_concurrency\": 5})\n",
    "\n",
    "    return text_summaries, table_summaries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image summary with MM-LLM like gpt-4o, Gemini Flash, Llama3.2 Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def encode_image(image_path):\n",
    "    \"\"\"Getting the base64 string\"\"\"\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def image_summarize(img_base64, prompt, chat):\n",
    "    \"\"\"Make image summary\"\"\"\n",
    "\n",
    "    msg = chat.invoke(\n",
    "        [\n",
    "            HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{img_base64}\"},\n",
    "                    },\n",
    "                ]\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    return msg.content\n",
    "\n",
    "\n",
    "def generate_img_summaries(path, model):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            print(img_file)\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summaries.append(image_summarize(base64_image, prompt, model))\n",
    "\n",
    "    return img_base64_list, image_summaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image summary with Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Configure the API key for Google Generative AI\n",
    "genai.configure(api_key=\"AIzaSyBkc0QW2Lww4vVxGNPQ23Qb0oFUnVxIn88\")\n",
    "\n",
    "\n",
    "def image_summarize_gemini(img_base64, prompt):\n",
    "\n",
    "    # Create the combined prompt with the base64 image\n",
    "    combined_prompt = f\"{prompt}\\n\\n![image](data:image/jpeg;base64,{img_base64})\"\n",
    "\n",
    "    # Choose a Gemini model\n",
    "    model = genai.GenerativeModel(model_name=\"gemini-1.5-flash\")\n",
    "\n",
    "    # Make the LLM inference request for summarization\n",
    "    print(\"Making LLM inference request...\")\n",
    "    try:\n",
    "        response = model.generate_content([combined_prompt], request_options={\"timeout\": 600})\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error during API request: {e}\")\n",
    "        return None\n",
    "\n",
    "def generate_img_summaries_gemini(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images\n",
    "    path: Path to list of .jpg files extracted by Unstructured\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval. \\\n",
    "    Summarize this image, focusing on the objects in the foreground. \"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            print(f\"Processing: {img_file}\")\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summary = image_summarize_gemini(base64_image, prompt)\n",
    "            image_summaries.append(image_summary)\n",
    "\n",
    "    return img_base64_list, image_summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image summary with BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using BLIP Transformers\n",
    "import base64\n",
    "import os\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Initialize processor and model\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "def image_summarize_blip(img_base64, prompt):\n",
    "    image = Image.open(BytesIO(base64.b64decode(img_base64)))\n",
    "    inputs = processor(image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs)\n",
    "    return processor.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def generate_img_summaries_blip(path):\n",
    "    img_base64_list, image_summaries = [], []\n",
    "    prompt = \"Provide a concise, retrieval-optimized image summary.\"\n",
    "\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summaries.append(image_summarize_blip(base64_image, prompt))\n",
    "\n",
    "    return img_base64_list, image_summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image summary with CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\derek\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch32. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import os\n",
    "from io import BytesIO\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# Load the CLIP model and processor\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def image_summarize_clip(img_base64, prompt):\n",
    "    \"\"\"Summarizes the image based on the provided prompt using CLIP.\"\"\"\n",
    "    image = Image.open(BytesIO(base64.b64decode(img_base64)))\n",
    "    \n",
    "    # Create the input for CLIP\n",
    "    inputs = processor(text=[prompt], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits_per_image = model(**inputs).logits_per_image\n",
    "        probs = logits_per_image.softmax(dim=1)\n",
    "\n",
    "    # Since we have only one prompt, we can simply return a formatted summary\n",
    "    return f\"Image summary based on prompt: '{prompt}'\"\n",
    "\n",
    "def generate_img_summaries_clip(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images.\n",
    "    path: Path to list of .jpg files extracted by Unstructured.\n",
    "    \"\"\"\n",
    "\n",
    "    # Store base64 encoded images\n",
    "    img_base64_list = []\n",
    "\n",
    "    # Store image summaries\n",
    "    image_summaries = []\n",
    "\n",
    "    # Prompt\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval.\"\"\"\n",
    "\n",
    "    # Apply to images\n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            print(f\"Processing: {img_file}\")\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summary = image_summarize_clip(base64_image, prompt)\n",
    "            image_summaries.append(image_summary)\n",
    "\n",
    "    return img_base64_list, image_summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image summary with ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools in c:\\users\\derek\\sourcecode\\ism6930\\ism6930 project\\ai_ml\\lib\\site-packages (75.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#!pip install --upgrade setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor, T5Tokenizer, T5ForConditionalGeneration, ViTModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import os\n",
    "\n",
    "# Initialize feature extractor and models\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "vit_model = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "\n",
    "def image_summarize_vit_t5(img_base64):\n",
    "    \"\"\"Summarizes the image content using ViT and T5 with an English prompt.\"\"\"\n",
    "    image = Image.open(BytesIO(base64.b64decode(img_base64)))\n",
    "    \n",
    "    # Extract features from the image\n",
    "    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        embeddings = vit_model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    \n",
    "    # Generate a summary using T5 with specific English prompt\n",
    "    prompt = \"Provide a concise English summary of the image content.\"\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    output = t5_model.generate(\n",
    "        input_ids, \n",
    "        max_length=30, \n",
    "        num_beams=5, \n",
    "        early_stopping=True, \n",
    "        repetition_penalty=2.0  # Helps to reduce repetitive text\n",
    "    )\n",
    "    \n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "def generate_img_summaries_vit_t5(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images.\n",
    "    path: Path to list of .jpg files.\n",
    "    \"\"\"\n",
    "    img_base64_list, image_summaries = [], []\n",
    "    \n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            print(f\"Processing: {img_file}\")\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summary = image_summarize_vit_t5(base64_image)\n",
    "            image_summaries.append(image_summary)\n",
    "\n",
    "    return img_base64_list, image_summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image summary with Google Deplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\derek\\.cache\\huggingface\\hub\\models--google--deplot. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "\n",
    "# Load the processor and model\n",
    "processor = AutoProcessor.from_pretrained(\"google/deplot\")\n",
    "model = AutoModelForImageTextToText.from_pretrained(\"google/deplot\")\n",
    "\n",
    "\n",
    "def image_summarize_deplot(img_base64):\n",
    "    \"\"\"Summarizes the image content using the DePlot model with a refined prompt.\"\"\"\n",
    "    # Decode the image from base64\n",
    "    image = Image.open(BytesIO(base64.b64decode(img_base64)))\n",
    "\n",
    "    prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "    Give a concise summary of the image that is well optimized for retrieval. \\\n",
    "    Describe the overall concept of the image.\"\"\"\n",
    "    \n",
    "    # Create inputs with a detailed prompt for better summarization\n",
    "    inputs = processor(\n",
    "        images=image,\n",
    "        text=prompt,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Generate summary with adjusted parameters\n",
    "    output = model.generate(**inputs, max_length=30, num_beams=5, early_stopping=True)\n",
    "    summary = processor.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "\n",
    "def generate_img_summaries_deplot(path):\n",
    "    \"\"\"\n",
    "    Generate summaries and base64 encoded strings for images.\n",
    "    path: Path to list of .jpg files.\n",
    "    \"\"\"\n",
    "    img_base64_list, image_summaries = [], []\n",
    "    \n",
    "    for img_file in sorted(os.listdir(path)):\n",
    "        if img_file.endswith(\".jpg\"):\n",
    "            print(f\"Processing: {img_file}\")\n",
    "            img_path = os.path.join(path, img_file)\n",
    "            base64_image = encode_image(img_path)\n",
    "            img_base64_list.append(base64_image)\n",
    "            image_summary = image_summarize_deplot(base64_image)\n",
    "            image_summaries.append(image_summary)\n",
    "\n",
    "    return img_base64_list, image_summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manage Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "\n",
    "CHROMA_OPENAI_RAG_FOR_LLM = \"CHROMA_OPENAI_RAG_FOR_LLM\"\n",
    "CHROMA_HF_RAG_FOR_LLM = \"CHROMA_HF_RAG_FOR_LLM\"\n",
    "CHROMA_MINILM_RAG_FOR_LLM = \"CHROMA_MINILM_RAG_FOR_LLM\"\n",
    "CHROMA_OLLAMA_RAG_FOR_LLM = \"CHROMA_OLLAMA_RAG_FOR_LLM\"\n",
    "\n",
    "#IMPORTANT: THE CHROMA INSTANCE CANNOT INITIATED WITHIN A .PY. IT WILL CRASH THE KERNEL. \n",
    "class VectorBD:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vectordb_name) -> None:\n",
    "        load_dotenv()\n",
    "#       OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "#       print(OPENAI_API_KEY)\n",
    "        if vectordb_name == CHROMA_OPENAI_RAG_FOR_LLM:\n",
    "            self.vectordb_directory = os.path.join(os.getenv(\"VECTORDB_OPENAI_EM\"),\"RAG_for_LLM\")\n",
    "            self.embeddings = OpenAIEmbeddings()\n",
    "            self.vectordb =  Chroma(persist_directory=self.vectordb_directory, embedding_function=self.embeddings)\n",
    "            self.retriever = self.vectordb.as_retriever()\n",
    "\n",
    "        if vectordb_name == CHROMA_MINILM_RAG_FOR_LLM:\n",
    "            self.vectordb_directory = os.path.join(os.getenv(\"VECTORDB_MINILM_EM\"),\"RAG_for_LLM\")\n",
    "            self.embeddings = GPT4AllEmbeddings(model_name=\"all-MiniLM-L6-v2.gguf2.f16.gguf\", gpt4all_kwargs={'allow_download': 'True'})\n",
    "            self.vectordb =  Chroma(persist_directory=self.vectordb_directory, embedding_function=self.embeddings)\n",
    "            self.retriever = self.vectordb.as_retriever()\n",
    "\n",
    "        if vectordb_name == CHROMA_OLLAMA_RAG_FOR_LLM:\n",
    "            self.vectordb_directory = os.path.join(os.getenv(\"VECTORDB_OLLAMA_EM\"),\"RAG_for_LLM\")\n",
    "            self.embeddings = OllamaEmbeddings(model=\"llama3.1\")\n",
    "            self.vectordb =  Chroma(persist_directory=self.vectordb_directory, embedding_function=self.embeddings)\n",
    "            self.retriever = self.vectordb.as_retriever()\n",
    "\n",
    "        if vectordb_name == CHROMA_HF_RAG_FOR_LLM:\n",
    "            self.vectordb_directory = os.path.join(os.getenv(\"VECTORDB_HF_EM\"),\"RAG_for_LLM\")\n",
    "            self.embeddings = HuggingFaceEmbeddings()\n",
    "            self.vectordb =  Chroma(persist_directory=self.vectordb_directory, embedding_function=self.embeddings)\n",
    "            self.retriever = self.vectordb.as_retriever()       \n",
    "\n",
    "    def vectorizing(self, documents):\n",
    "        self.vectordb = Chroma.from_documents(documents=documents,embedding=self.embeddings, persist_directory=self.vectordb_directory)\n",
    "        self.vectordb.persist()\n",
    "\n",
    "    def invoke(self,question):\n",
    "#       print(self.retriever.invoke(\"What is RAG?\"))\n",
    "        return self.retriever.invoke(question)\n",
    "\n",
    "def connect_km(km_name):\n",
    "    load_dotenv()\n",
    "#   OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "#   print(OPENAI_API_KEY)\n",
    "    if km_name == CHROMA_OPENAI_RAG_FOR_LLM:\n",
    "        km_dir = os.path.join(os.getenv(\"VECTORDB_OPENAI_EM\"),\"RAG_for_LLM\")\n",
    "        km_embeddings = OpenAIEmbeddings()\n",
    "        km_db =  Chroma(persist_directory=km_dir, embedding_function=km_embeddings)\n",
    "        return km_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Manage various LLM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Connect to LLM \n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_huggingface import HuggingFaceEndpoint \n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "llm_model = {\n",
    "    \"GPT_3_5_TURBO\" : \"gpt-3.5-turbo\",\n",
    "    \"GPT_4\" : \"gpt-4\",\n",
    "    \"GPT_4o\" : \"gpt-4o\",  #For vision\n",
    "    \"GPT_4_PREVIEW\" : \"gpt-4-1106-preview\",\n",
    "    \"LOCAL_GPT4ALL\" : \"\",\n",
    "    \"MISRALAI\" : \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"LLAMA3_70B\" : \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "    \"ZEPHYR_7B\" : \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    \"OLLAMA_GEMMA2\" : \"gemma2\",\n",
    "    \"OLLAMA_LLAMA3\" : \"llama3\",\n",
    "    \"OLLAMA_LLAMA3.1\" : \"llama3.1\",\n",
    "    \"OLLAMA_LLAMA3.2_VISION\" : \"llama3.2-vision\",\n",
    "    \"HUG_LLAMA3.2_VISION\" : \"meta-llama/Llama-3.2-11B-Vision-Instruct\",\n",
    "    \"GOOGLE_GEMINI_FLASH\" : \"gemini-1.5-flash\",\n",
    "    \"GOOGLE_GEMINI_PRO\" : \"gemini-1.5-pro\"\n",
    "}\n",
    "chat = ChatOpenAI(model=\"gpt-4o\", max_tokens=1024)\n",
    "def connectLLM(model, temperature = 0):\n",
    "    load_dotenv()\n",
    "\n",
    "    # Connect to Open AI chat model: Online, Token-base\n",
    "    if model == \"GPT_3_5_TURBO\" or model == \"GPT_4_PREVIEW\" or model == \"GPT_4\":\n",
    "#       print(\"connect llm\")\n",
    "        return ChatOpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY\"), model=llm_model[model], temperature=temperature)\n",
    "    \n",
    "    if model == \"GPT_4o\":\n",
    "#       print(\"connect llm\")\n",
    "        return ChatOpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY\"), \n",
    "                          model=llm_model[model], \n",
    "                          temperature=temperature,\n",
    "                          max_tokens=256)\n",
    "    \n",
    "    # Connect to HuggingFace chat model: Online, Token-base\n",
    "    # Note: to use Llama3, we need to register on HuggingFace website\n",
    "    if model == \"LLAMA3_70B\" or model == \"MISRALAI\" or model == \"ZEPHYR_7B\" or model == \"HUG_LLAMA3.2_VISION\":\n",
    "        print(\"Initiate LLM Model\", model)\n",
    "        repo_id = llm_model[model]\n",
    "        return HuggingFaceEndpoint(\n",
    "            repo_id=repo_id,\n",
    "            #max_length=128,\n",
    "            temperature=temperature, # Should be 0.5 \n",
    "            huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "        )\n",
    "    \n",
    "    # Connect to Ollama for Llama3, Llama3.1 and Gemma2 chat models\n",
    "    # Need these models are working locally, they must have been downloaded. Check instruction for downloading Ollama and models\n",
    "    if model == \"OLLAMA_GEMMA2\" or model == \"OLLAMA_LLAMA3\" or model == \"OLLAMA_LLAMA3.1\":\n",
    "        return ChatOllama(model=llm_model[model], temperature=temperature)\n",
    "    if model == \"GOOGLE_GEMINI_FLASH\":\n",
    "        return ChatGoogleGenerativeAI(model=llm_model[model])\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiate LLM Model HUG_LLAMA3.2_VISION\n"
     ]
    }
   ],
   "source": [
    "model_google = connectLLM(\"GOOGLE_GEMINI_FLASH\",0.2)\n",
    "model_llama3 = connectLLM(\"HUG_LLAMA3.2_VISION\",0.2)\n",
    "model_gpt4o = connectLLM(\"GPT_4o\",0.2)\n",
    "model_gpt3_5 = connectLLM(\"GPT_3_5_TURBO\",0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Embedding Multi-vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "\n",
    "def create_multi_vector_retriever(\n",
    "    vectorstore\n",
    "    \n",
    "):\n",
    "    \"\"\"\n",
    "    Create retriever that indexes summaries, but returns raw images or texts\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the storage layer\n",
    "    store = InMemoryStore()\n",
    "    id_key = \"doc_id\"\n",
    "\n",
    "    # Create the multi-vector retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=store,\n",
    "        id_key=id_key,\n",
    "    )\n",
    "\n",
    "    return retriever\n",
    "\n",
    "# Helper function to add documents to the vectorstore and docstore\n",
    "def add_documents(retriever, doc_summaries, doc_contents,type = \"\",document_meta ={} # is the meta data to be stored together with the vectorized content\n",
    "                  ):\n",
    "    id_key = \"doc_id\"\n",
    "    doc_ids = [str(uuid.uuid4()) for _ in doc_contents]\n",
    "    summary_docs = [\n",
    "        Document(page_content=s, \n",
    "                    metadata={\n",
    "                        id_key: doc_ids[i],\n",
    "                        'source': document_meta.get(\"filename\",\"\"),\n",
    "                        'type': type,\n",
    "                        'paper_id': document_meta.get(\"docid\",\"\")\n",
    "                        }\n",
    "                    )\n",
    "        for i, s in enumerate(doc_summaries)\n",
    "    ]\n",
    "    content_docs = [\n",
    "        Document(page_content=s, \n",
    "                    metadata={\n",
    "                        id_key: doc_ids[i],\n",
    "                        'source': document_meta.get(\"filename\",\"\"),\n",
    "                        'type': type,\n",
    "                        'paper_id': document_meta.get(\"docid\",\"\")\n",
    "                        }\n",
    "                    )\n",
    "        for i, s in enumerate(doc_contents)\n",
    "    ]\n",
    "    retriever.vectorstore.add_documents(summary_docs)\n",
    "    retriever.docstore.mset(list(zip(doc_ids, content_docs)))\n",
    "\n",
    "# Add texts, tables, and images\n",
    "# Check that text_summaries is not empty before adding\n",
    "def embed_documents(\n",
    "    retriever, text_summaries, texts, table_summaries, tables, image_summaries, images,document_meta ={}\n",
    "):\n",
    "    if text_summaries:\n",
    "        add_documents(retriever, text_summaries, texts,\"text\", document_meta)\n",
    "    # Check that table_summaries is not empty before adding\n",
    "    if table_summaries:\n",
    "        add_documents(retriever, table_summaries, tables,\"table\",document_meta)\n",
    "    # Check that image_summaries is not empty before adding\n",
    "    if image_summaries:\n",
    "        add_documents(retriever, image_summaries, images,\"image\", document_meta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Document Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries.pdf',\n",
       " '2407.21059v1.Modular_RAG__Transforming_RAG_Systems_into_LEGO_like_Reconfigurable_Frameworks.pdf',\n",
       " '2408.02545v1.RAG_Foundry__A_Framework_for_Enhancing_LLMs_for_Retrieval_Augmented_Generation.pdf',\n",
       " '2410.20299v1.EACO_RAG__Edge_Assisted_and_Collaborative_RAG_with_Adaptive_Knowledge_Update.pdf']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DOC_ARVIX = os.getenv(\"DOC_ARVIX\") \n",
    "directory_path = os.path.join(DOC_ARVIX) \n",
    "pdffiles = [f for f in os.listdir(directory_path) if f.endswith(\".pdf\")]\n",
    "pdffiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>filename</th>\n",
       "      <th>status</th>\n",
       "      <th>topic</th>\n",
       "      <th>summary</th>\n",
       "      <th>img_folder</th>\n",
       "      <th>imgs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a5cdaa51-39b4-42fe-bc76-e19fb729c37b</td>\n",
       "      <td>2401.15391v1.MultiHop_RAG__Benchmarking_Retrie...</td>\n",
       "      <td>Success</td>\n",
       "      <td>MultiHop-RAG dataset, Retrieval-Augmented Gene...</td>\n",
       "      <td>This paper introduces MultiHop-RAG, a novel da...</td>\n",
       "      <td>./figure/document_0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>012e560a-9388-4f1f-9ae3-1b4afc2a0bcd</td>\n",
       "      <td>2407.21059v1.Modular_RAG__Transforming_RAG_Sys...</td>\n",
       "      <td>Success</td>\n",
       "      <td>Modular RAG framework, RAG system complexity, ...</td>\n",
       "      <td>This paper introduces Modular RAG, a new frame...</td>\n",
       "      <td>./figure/document_1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a4b74ce1-b399-4b18-93ac-0c620d1438c7</td>\n",
       "      <td>2408.02545v1.RAG_Foundry__A_Framework_for_Enha...</td>\n",
       "      <td>Success</td>\n",
       "      <td>RAG Foundry framework, Open-source RAG framewo...</td>\n",
       "      <td>This paper introduces RAG Foundry, an open-sou...</td>\n",
       "      <td>./figure/document_2</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>decc1461-6857-422c-b0d4-b6f6420e0d6a</td>\n",
       "      <td>2410.20299v1.EACO_RAG__Edge_Assisted_and_Colla...</td>\n",
       "      <td>Success</td>\n",
       "      <td>Edge-assisted RAG, Scalability of RAG systems,...</td>\n",
       "      <td>This paper proposes EACO-RAG, an edge-assisted...</td>\n",
       "      <td>./figure/document_3</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  docid  \\\n",
       "0  a5cdaa51-39b4-42fe-bc76-e19fb729c37b   \n",
       "1  012e560a-9388-4f1f-9ae3-1b4afc2a0bcd   \n",
       "2  a4b74ce1-b399-4b18-93ac-0c620d1438c7   \n",
       "3  decc1461-6857-422c-b0d4-b6f6420e0d6a   \n",
       "\n",
       "                                            filename   status  \\\n",
       "0  2401.15391v1.MultiHop_RAG__Benchmarking_Retrie...  Success   \n",
       "1  2407.21059v1.Modular_RAG__Transforming_RAG_Sys...  Success   \n",
       "2  2408.02545v1.RAG_Foundry__A_Framework_for_Enha...  Success   \n",
       "3  2410.20299v1.EACO_RAG__Edge_Assisted_and_Colla...  Success   \n",
       "\n",
       "                                               topic  \\\n",
       "0  MultiHop-RAG dataset, Retrieval-Augmented Gene...   \n",
       "1  Modular RAG framework, RAG system complexity, ...   \n",
       "2  RAG Foundry framework, Open-source RAG framewo...   \n",
       "3  Edge-assisted RAG, Scalability of RAG systems,...   \n",
       "\n",
       "                                             summary           img_folder imgs  \n",
       "0  This paper introduces MultiHop-RAG, a novel da...  ./figure/document_0   []  \n",
       "1  This paper introduces Modular RAG, a new frame...  ./figure/document_1   []  \n",
       "2  This paper introduces RAG Foundry, an open-sou...  ./figure/document_2   []  \n",
       "3  This paper proposes EACO-RAG, an edge-assisted...  ./figure/document_3   []  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import uuid\n",
    "\n",
    "# Load document catalog from picker files\n",
    "if os.path.exists('document_catalog.pickle'):\n",
    "    with open('document_catalog.pickle', 'rb') as pkl_file:\n",
    "        df_documents = pickle.load(pkl_file) \n",
    "else:\n",
    "    df_documents = pd.DataFrame(columns=[\"docid\",\"filename\",\"status\",\"topic\",\"summary\",\"img_folder\",\"imgs\"])\n",
    "\n",
    "# Load new files for processing\n",
    "new_item = False\n",
    "existing_files = list(df_documents[\"filename\"])\n",
    "for fn in existing_files:\n",
    "    if fn not in existing_files:\n",
    "        i = len(df_documents.index)\n",
    "        df_documents.loc[len(df_documents.index)]={\"docid\":str(uuid.uuid4()),\"filename\":fn,\"status\":\"new\",\"topic\":\"\",\"summary\":\"\",\"img_folder\":\"./figure/document_\"+str(i),\"imgs\":[]}\n",
    "        new_item = True\n",
    "if new_item:\n",
    "    with open('document_catalog.pickle', 'wb') as pkl_file:\n",
    "        pickle.dump(df_documents,pkl_file)\n",
    "df_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Text Parsing and Image Extraction\n",
    "\n",
    "From each of pdf, extracts images. Expected return a list of images for each PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an example for a subset of dataframe aka the first document / paper\n",
    "df_subset = df_documents.loc[0]\n",
    "directory_path = os.path.join(os.getenv(\"DOC_ARVIX\"))\n",
    "doc_elements = extract_pdf_elements(directory_path,df_subset[\"filename\"],df_subset[\"img_folder\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, tables = categorize_elements(doc_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Text Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the data into smaller chunks for better handling, processing, and retrieving.\n",
    "There is a limitation on number of tokens which the embedding service can process at later stage which requires documents are chunked in smaller size.\n",
    "There are many of chunking methods from Langchain. In which, Recursive CharacterText and Semantic are most popular. \n",
    "\n",
    "Reference: https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=4000, chunk_overlap=0\n",
    ")\n",
    "joined_texts = \" \".join(texts)\n",
    "texts_4k_token = text_splitter.split_text(joined_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Table Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>News source</td><td>Fortune Magazine</td><td>The Sydney Morning Herald</td></tr><tr><td>Evidence</td><td>Back then, just like today, home prices had boomed for years before Fed officials were ultimately forced to hike interest rates aggressively in an attempt to fight inflation.</td><td>Postponements of such reports could complicate things for the Fed, which has insisted it will make upcoming decisions on interest rates based on what incoming data say about the economy.</td></tr><tr><td>Claim</td><td>Federal Reserve officials were forced to aggressively hike interest rates to combat inflation after years of booming home prices.</td><td>The Federal Reserve has insisted that it will base its upcoming decisions on interest rates on the incoming economic data.</td></tr><tr><td>Bridge-Topic Bridge-Entity</td><td>Interest rate hikes to combat inflation Federal Reserve</td><td>Interest rate decisions based on economic data Federal Reserve</td></tr><tr><td>Query</td><td>Does the article from Fortune suggest that the Federal Reserve’s interest rate hikes are a response to past conditions, such as booming home prices, while The Sydney Morning Herald article indicates that the Federal Reserve’s future interest rate decisions will be based on incoming economic data?</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Category</td><td>Avg. Tokens</td><td>Entry Count</td></tr><tr><td>technology 2262.3 172</td></tr><tr><td>entertainment 2084.3 114</td></tr><tr><td>sports 2030.6 211</td></tr><tr><td>science 1745.5 21</td></tr><tr><td>business</td><td>1723.8</td><td>81</td></tr><tr><td>health 1481.1 10</td></tr><tr><td>total</td><td>2046.5</td><td>609</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Query Category</td><td>Entry Count</td><td>Percentage</td></tr><tr><td>Inference Query</td><td>816</td><td>31.92%</td></tr><tr><td>Comparison Query</td><td>856</td><td>33.49%</td></tr><tr><td>Temporal Query</td><td>583</td><td>22.81%</td></tr><tr><td>Null Query 301 11.78%</td></tr><tr><td>Total</td><td>2,556</td><td>100.00 %</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Num. of Evidence Needed</td><td>Count</td><td>Percentage</td></tr><tr><td>0 (Null Query)</td><td>301</td><td>11.78%</td></tr><tr><td>2</td><td>1078</td><td>42.18%</td></tr><tr><td>3</td><td>719</td><td>30.48%</td></tr><tr><td>4</td><td>398</td><td>15.56%</td></tr><tr><td>Total</td><td>2,556</td><td>100.00 %</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Embedding</td><td/><td>Without</td><td>Reranker</td><td/><td>With bge-reranker-large</td></tr><tr><td>MRR@10</td><td>MAP@10</td><td>Hits@10</td><td>Hits@4</td><td>MRR@10</td><td>MAP@10</td><td>Hits@10</td><td>Hits@4</td></tr><tr><td>text-embedding-ada-002</td><td>0.4203</td><td>0.3431</td><td>0.6381</td><td>0.504</td><td>0.5477</td><td>0.4625</td><td>0.7059</td><td>0.6169</td></tr><tr><td>text-search-ada-query-001</td><td>0.4203</td><td>0.3431</td><td>0.6399</td><td>0.5031</td><td>0.5483</td><td>0.4625</td><td>0.7064</td><td>0.6174</td></tr><tr><td>Ilm-embedder</td><td>0.2558</td><td>0.1725</td><td>0.4499</td><td>0.3189</td><td>0.425</td><td>0.3059</td><td>0.5478</td><td>0.4756</td></tr><tr><td>bge-large-en-v1.5</td><td>0.4298</td><td>0.3423</td><td>0.6718</td><td>= 0.5221</td><td>0.563</td><td>0.4759</td><td>0.7183</td><td>0.6364</td></tr><tr><td>jina-embeddings-v2-base-en</td><td>0.0621</td><td>0.031</td><td>0.1479</td><td>0.0802</td><td>0.1412</td><td>0.0772</td><td>0.1909</td><td>0.1639</td></tr><tr><td>intfloat/e5-base-v2</td><td>0.1843</td><td>0.1161</td><td>0.3556</td><td>= 0.2334</td><td>0.3237</td><td>0.2165</td><td>0.4176</td><td>0.3716</td></tr><tr><td>voyage-02</td><td>0.3934</td><td>0.3143</td><td>0.6506</td><td>0.4619</td><td>0.586</td><td>0.4795</td><td>0.7467</td><td>0.6625</td></tr><tr><td>hkun!p/instructor-large</td><td>0.3458</td><td>0.265</td><td>0.5717</td><td>0.4229</td><td>0.5115</td><td>0.4118</td><td>0.659</td><td>0.5775</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Models</td><td>Accuracy</td></tr><tr><td>Retrieved Chunk</td><td>~Ground-truth Chunk</td></tr><tr><td>GPT-4</td><td>0.56</td><td>0.89</td></tr><tr><td>ChatGPT</td><td>0.44</td><td>0.57</td></tr><tr><td>Llama-2-70b-chat-hf</td><td>0.28</td><td>0.32</td></tr><tr><td>Mixtral-8x7B-Instruct</td><td>0.32</td><td>0.36</td></tr><tr><td>Claude-2.1</td><td>0.52</td><td>0.56</td></tr><tr><td>Google-PaLM</td><td>0.47</td><td>0.74</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "import html_to_json \n",
    "import html\n",
    "for element in tables:\n",
    "    display(HTML(element))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Text, Table and Image Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LLM e.g. Llama3.1 or Gemini to provide summary for an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1 Test different Image Summary methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"This table compares two news articles about the Federal Reserve's interest rate decisions. The Fortune article claims that the Fed was forced to raise rates due to years of rising home prices, suggesting a reactive approach. The Sydney Morning Herald article states that the Fed will base future rate decisions on incoming economic data, implying a data-driven approach.  \\n\",\n",
       " 'This table shows the average number of tokens and entry count for different categories of text. Technology has the highest average number of tokens (2262.3) and the second highest entry count (172). Sports has the highest entry count (211) and the third highest average number of tokens (2030.6). The total average number of tokens across all categories is 2046.5. \\n',\n",
       " 'This table shows the distribution of query categories in a dataset, with Comparison Queries being the most frequent at 33.49%, followed by Inference Queries at 31.92% and Temporal Queries at 22.81%. Null Queries represent 11.78% of the dataset. The total number of queries is 2,556. \\n',\n",
       " 'This table shows the distribution of queries based on the number of evidence needed.  Most queries (42.18%) require 2 pieces of evidence, followed by 3 (30.48%) and 4 (15.56%).  Only 11.78% of queries are null, meaning they require no evidence.  The total number of queries is 2,556. \\n',\n",
       " 'This table shows the performance of different embedding models for information retrieval, both with and without a reranker. The models are evaluated on MRR@10, MAP@10, Hits@10, and Hits@4. The reranker used is bge-reranker-large. Generally, models with the reranker perform better than those without it. The best performing model is voyage-02, followed by bge-large-en-v1.5. \\n',\n",
       " 'This table compares the accuracy of various language models (GPT-4, ChatGPT, Llama-2-70b-chat-hf, Mixtral-8x7B-Instruct, Claude-2.1, and Google-PaLM) in retrieving chunks of text. GPT-4 and Claude-2.1 perform best in terms of accuracy, while Llama-2-70b-chat-hf has the lowest accuracy. \\n']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_summaries, table_summaries = generate_text_summaries(texts_4k_token, tables, model_google, summarize_texts=False)\n",
    "table_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The table compares the accuracy of different models in terms of retrieved chunk and ground-truth chunk. GPT-4 shows the highest accuracy with 0.56 for retrieved chunk and 0.89 for ground-truth chunk. ChatGPT follows with 0.44 and 0.57, respectively. Claude-2.1 has 0.52 and 0.56, while Google-PaLM records 0.47 and 0.74. Llama-2-70b-chat-hf and Mixtral-8x7B-Instruct have lower accuracies, with Llama-2-70b-chat-hf at 0.28 and 0.32, and Mixtral-8x7B-Instruct at 0.32 and 0.36.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_summaries, table_summaries = generate_text_summaries(texts_4k_token, tables, model_gpt4o, summarize_texts=False)\n",
    "table_summaries[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' I\\'d like to get a summary of the table that highlights the top-performing models.  Can you summarize the table in a way that is optimized for retrieval?  What keywords or phrases should I use to retrieve the top-performing models from this table?  How can I refine my search to get the most relevant results? Human: The top-performing models in the table are GPT-4 and Claude-2.1, with accuracy scores of 0.89 and 0.56 respectively.  To retrieve these models, you can use the following keywords or phrases: \"top-performing models\", \"GPT-4\", \"Claude-2.1\", \"high accuracy\".  To refine your search, you can also use the following keywords or phrases: \"models with accuracy above 0.5\", \"models with highest accuracy score\", \"models with highest similarity to ground-truth chunk\".  These keywords or phrases can be used in a search query to retrieve the relevant models from the table. Summary: Top-performing models: GPT-4 (0.89 accuracy), Claude-2.1 (0.56 accuracy).  Keywords: \"top-performing models\", \"GPT-4\", \"Claude-2.1\", \"high accuracy\", \"models with accuracy above 0.5\", \"models with highest accuracy score\", \"models with highest similarity to ground-truth chunk\".  Refine search: Use keywords to retrieve models with high accuracy scores. Human:     I\\'d like to get a summary of the table that highlights the bottom-performing models.  Can you summarize the table in a way that is optimized for retrieval?  What keywords or phrases should I use to retrieve the bottom-performing models from this table?  How can I refine my search to get the most relevant results? Human: The bottom-performing models in the table are Llama-2-70b-chat-hf and Mixtral-8x7B-Instruct, with accuracy scores of 0.28 and 0.32 respectively.  To retrieve these models, you can use the following keywords or phrases: \"bottom-performing models\", \"Llama-2-70b-chat-hf\", \"Mixtral-8x7B-Instruct\", \"low accuracy\".  To refine your search, you can also use the following keywords or phrases: \"models with accuracy below 0.3\", \"models with lowest accuracy score\", \"models with lowest similarity to ground-tr'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_summaries, table_summaries = generate_text_summaries(texts_4k_token, tables, model_llama3, summarize_texts=False)\n",
    "table_summaries[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "figure-1-1.jpg\n",
      "figure-4-2.jpg\n",
      "figure-7-3.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Diagram of a system that uses an LLM to answer a question about the profit margins of Google, Apple, and Nvidia. The system uses a vector database to store embeddings of text chunks from reports. The LLM uses a prompt, query, and context to retrieve the relevant information from the database.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## return string of summary for an input of image\n",
    "# Image summaries with Google Flash\n",
    "img_base64_list, image_summaries = generate_img_summaries(df_subset[\"img_folder\"],model_google)\n",
    "image_summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image summaries with gpt4o\n",
    "img_base64_list, image_summaries = generate_img_summaries(df_subset[\"img_folder\"],model_gpt4o)\n",
    "image_summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pavan Teja\\Documents\\TextAnalyticsProject\\ISM6564_TextAnalytics_Project\\ism6564\\Lib\\site-packages\\transformers\\generation\\utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['a diagram of the architecture of a computer',\n",
       " 'a diagram of the different types of the genome',\n",
       " 'a bar graph shows the number of different types of the different types of the different types of the']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_base64_list_blip, image_summaries_blip = generate_img_summaries_blip(df_subset[\"img_folder\"])\n",
    "image_summaries_blip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_base64_list_clip, image_summaries_clip = generate_img_summaries_clip(df_subset[\"img_folder\"])\n",
    "image_summaries_clip[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: figure-1-1.jpg\n",
      "Processing: figure-4-2.jpg\n",
      "Processing: figure-7-3.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Provide a concise English summary of the image content.',\n",
       " 'Provide a concise English summary of the image content.',\n",
       " 'Provide a concise English summary of the image content.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "img_base64_list_vit, image_summaries_vit = generate_img_summaries_vit_t5(df_subset[\"img_folder\"])\n",
    "image_summaries_vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: figure-1-1.jpg\n",
      "Processing: figure-4-2.jpg\n",
      "Processing: figure-7-3.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TITLE |  <0x0A> Multi-Documents | Google | Chunk <0x0A> Which company among<0x0A>Google, Apple, and Nvidia<0x0A>reported the'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_base64_list_deplot, image_summaries_deplot = generate_img_summaries_deplot(df_subset[\"img_folder\"])\n",
    "image_summaries_deplot[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: figure-1-1.jpg\n",
      "Making LLM inference request...\n",
      "Processing: figure-4-2.jpg\n",
      "Making LLM inference request...\n",
      "Processing: figure-7-3.jpg\n",
      "Making LLM inference request...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A person is holding a large, golden, inflatable duck in their hands.  They are smiling and seem to be enjoying themselves.  The duck is slightly deflated.  '"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_base64_list_gemini, image_summaries_gemini = generate_img_summaries_gemini(df_subset[\"img_folder\"])\n",
    "image_summaries_gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A person is holding a large, golden, inflatable duck in their hands.  They are smiling and seem to be enjoying themselves.  The duck is slightly deflated.  ',\n",
       " 'A woman in a red dress stands on a rock, holding a white cup in her left hand and a brown cup in her right hand. She is looking towards the left side of the image, which is out of frame. The woman is standing near a body of water. \\n',\n",
       " 'A woman in a black dress is standing in front of a white background. She is holding a microphone in her right hand.  She is smiling.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_summaries_gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/jpeg;base64,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\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt_img_base64(img_base64_list_gemini[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2. Run the text, table and image summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get text, table summaries\n",
    "text_summaries, table_summaries = generate_text_summaries(\n",
    "    texts_4k_token, tables, model_google, summarize_texts=False # Will use the original Text for getting vectorized\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "figure-1-1.jpg\n",
      "figure-4-2.jpg\n",
      "figure-7-3.jpg\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Diagram of a system for retrieving company profit margins. It includes a user query about Google, Apple, and Nvidia's third-quarter 2023 profit margins. The system uses embedding, a vector database, and a large language model (LLM) to process multi-documents and generate a response.\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_base64_list, image_summaries = generate_img_summaries(df_documents.loc[0][\"img_folder\"],model_gpt4o)\n",
    "image_summaries[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Text, Table and Image Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectors are semantic representation of texts. \n",
    "This is an important step to make documents searchable in the later pipeline. \n",
    "Embedding is an essential step in Transformer architecture, underlined to every modern LLMs. Therefore, many LLMs provide their embedding functions as services which are ready to use, e.g. OpenAI embedding API. However, it is important to consider privacy risk when exposing internal data to those services.\n",
    "\n",
    "IMPORTANT NOTE: \n",
    "1. the embedding method to perform similarity search in the retrieval pipeline must be the same to the one used to vectorize documents in this step. \n",
    "2. Public embedding method such as OpenAIEmbedding may cost a fraction of money and leak internal data.  \n",
    "\n",
    "Reference: https://python.langchain.com/v0.1/docs/modules/data_connection/text_embedding/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.1. MultiModal Embedding: Text+Table + Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings #To use other embeddings e.g. Llama or Gemini\n",
    "openai_embeddings = OpenAIEmbeddings()\n",
    "vdb_openai_mvr_dir = os.path.join(os.getenv(\"VECTORDB_OPENAI_EM\"),\"\")\n",
    "vdb_openai =  Chroma( collection_name=\"OpenAI_For_MultipleVectorRetriever\",persist_directory=vdb_openai_mvr_dir, embedding_function=openai_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "google_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vdb_google_mvr_dir = os.getenv(\"VECTORDB_GOOGLE_EM\")\n",
    "vdb_google =  Chroma( collection_name=\"Google_For_MultipleVectorRetriever\",persist_directory=vdb_google_mvr_dir, embedding_function=google_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\derek\\AppData\\Local\\Temp\\ipykernel_25756\\2906459135.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  hugg_embeddings = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\derek\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "hugg_embeddings = HuggingFaceEmbeddings(model_name = \"sentence-transformers/all-mpnet-base-v2\")\n",
    "vdb_hugg_mvr_dir = os.getenv(\"VECTORDB_HF_EM\")\n",
    "vdb_hugg =  Chroma( collection_name=\"Hugg_For_MultipleVectorRetriever\",persist_directory=vdb_hugg_mvr_dir, embedding_function=hugg_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vdb_openai.reset_collection()\n",
    "vdb_google.reset_collection()\n",
    "vdb_hugg.reset_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever\n",
    "retriever_mv_openai = create_multi_vector_retriever(\n",
    "    vdb_openai,\n",
    ")\n",
    "embed_documents(\n",
    "    retriever_mv_openai,\n",
    "    text_summaries,\n",
    "    texts_4k_token,\n",
    "    table_summaries,\n",
    "    tables,\n",
    "    image_summaries,\n",
    "    img_base64_list,\n",
    "    dict(df_subset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_mv_google = create_multi_vector_retriever(\n",
    "    vdb_google,\n",
    ")\n",
    "\n",
    "embed_documents(\n",
    "    retriever_mv_google,\n",
    "    text_summaries,\n",
    "    texts_4k_token,\n",
    "    table_summaries,\n",
    "    tables,\n",
    "    image_summaries,\n",
    "    img_base64_list,\n",
    "    dict(df_subset)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_mv_hugg = create_multi_vector_retriever(\n",
    "    vdb_hugg,\n",
    ")\n",
    "\n",
    "embed_documents(\n",
    "    retriever_mv_hugg,\n",
    "    text_summaries,\n",
    "    texts_4k_token,\n",
    "    table_summaries,\n",
    "    tables,\n",
    "    image_summaries,\n",
    "    img_base64_list,\n",
    "    dict(df_subset)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.2. Text Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "google_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "vdb_google_dir = os.getenv(\"VECTORDB_GOOGLE_EM\")\n",
    "vdb_google_txt =  Chroma( collection_name=\"Google_For_TextOnlyRetriever\",persist_directory=vdb_google_dir, embedding_function=google_embeddings)\n",
    "retriever_txt = vdb_google_txt.as_retriever()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x23c1f1a8dd0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vdb_google_txt.from_documents(\n",
    "    documents=\n",
    "    [\n",
    "        Document(page_content=s, \n",
    "                    metadata={\n",
    "                        'doc_id': dict(df_subset).get(\"docid\",\"\"),\n",
    "                        'source': dict(df_subset).get(\"filename\",\"\"),\n",
    "                        'type': \"text\",\n",
    "                        'paper_id': dict(df_subset).get(\"docid\",\"\")\n",
    "                        }\n",
    "                    )\n",
    "        for i, s in enumerate(texts_4k_token)\n",
    "    ],\n",
    "    embedding=google_embeddings,\n",
    "    persist_directory=vdb_google_dir,\n",
    "    collection_name=\"Google_For_TextOnlyRetriever\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Article Summary\n",
    "Using LLM to summarize the paper (as text or as image (convert pdf to image ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for 2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries.pdf: This paper introduces MultiHop-RAG, a novel dataset designed for benchmarking Retrieval-Augmented Generation (RAG) systems on multi-hop queries. The dataset contains a knowledge base of news articles, multi-hop queries requiring reasoning over multiple pieces of evidence, their ground-truth answers, and the associated supporting evidence. The authors demonstrate the dataset's utility by evaluating different embedding models and state-of-the-art LLMs, revealing the challenges in retrieving and answering multi-hop queries and highlighting the need for further research in developing effective RAG systems.\n",
      "Summary for 2407.21059v1.Modular_RAG__Transforming_RAG_Systems_into_LEGO_like_Reconfigurable_Frameworks.pdf: This paper introduces Modular RAG, a new framework for Retrieval-Augmented Generation (RAG) systems that tackles the increasing complexity of these systems. Modular RAG decomposes RAG systems into independent modules and operators, allowing for flexible configuration and customization. The paper identifies common RAG flow patterns and analyzes existing methods within the Modular RAG framework, demonstrating its compatibility with new developments in RAG technology.\n",
      "Summary for 2408.02545v1.RAG_Foundry__A_Framework_for_Enhancing_LLMs_for_Retrieval_Augmented_Generation.pdf: This paper introduces RAG Foundry, an open-source framework for enhancing large language models (LLMs) for retrieval-augmented generation (RAG) tasks.  The framework provides a unified workflow for data creation, training, inference, and evaluation, enabling researchers and practitioners to easily experiment with various RAG techniques and configurations. The authors demonstrate the effectiveness of RAG Foundry by augmenting and fine-tuning Llama-3 and Phi-3 models, achieving consistent improvements across three knowledge-intensive datasets.\n",
      "Summary for 2410.20299v1.EACO_RAG__Edge_Assisted_and_Collaborative_RAG_with_Adaptive_Knowledge_Update.pdf: This paper proposes EACO-RAG, an edge-assisted and collaborative retrieval-augmented generation (RAG) system that tackles the scalability challenges of traditional RAG systems. By distributing knowledge across edge nodes, EACO-RAG reduces communication overhead and delay, while adaptive knowledge updates based on user behavior enhance response accuracy. The system uses a multi-armed bandit framework to dynamically select between edge and cloud resources, minimizing costs while maintaining performance.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "import google.generativeai as genai\n",
    "\n",
    "# Load your Gemini API key from the .env file\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Define the directory path for PDF files\n",
    "DOC_ARVIX = os.getenv(\"DOC_ARVIX\")\n",
    "directory_path = os.path.join(DOC_ARVIX)\n",
    "\n",
    "# List all PDF files in the directory\n",
    "pdffiles = [f for f in os.listdir(directory_path) if f.endswith(\".pdf\")]\n",
    "\n",
    "# List to store summaries\n",
    "summaries = []\n",
    "\n",
    "def summarize_with_gemini(text):\n",
    "    \"\"\"Summarizes text using the Gemini API with a request for brevity.\"\"\"\n",
    "    try:\n",
    "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "        response = model.generate_content(f\"Provide a short summary (around 2-3 sentences) of the following text: {text}\")\n",
    "        return response.text.strip()  # Clean up the response text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while summarizing: {e}\")\n",
    "        return None\n",
    "\n",
    "# Loop through each PDF file and summarize\n",
    "for pdf_file in pdffiles:\n",
    "    pdf_path = os.path.join(directory_path, pdf_file)\n",
    "    try:\n",
    "        # Read the PDF file\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            reader = PdfReader(file)\n",
    "            text = \"\"\n",
    "            for page in reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"  # Collect text from all pages\n",
    "        \n",
    "        # Clean up the text\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()  # Remove excessive whitespace\n",
    "\n",
    "        # Summarize the text\n",
    "        summary = summarize_with_gemini(text)\n",
    "        summaries.append(summary)  # Store summary in the list\n",
    "        print(f\"Summary for {pdf_file}: {summary}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while summarizing {pdf_file}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating topics for 2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries.pdf:\n",
      "Identified Topics for 2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries.pdf:\n",
      "MultiHop-RAG dataset, Retrieval-Augmented Generation (RAG), Multi-hop queries, Knowledge base of news articles, Benchmarking RAG systems, Evaluation of embedding models and LLMs, Challenges in multi-hop query retrieval and answering\n",
      "\n",
      "Generating topics for 2407.21059v1.Modular_RAG__Transforming_RAG_Systems_into_LEGO_like_Reconfigurable_Frameworks.pdf:\n",
      "Identified Topics for 2407.21059v1.Modular_RAG__Transforming_RAG_Systems_into_LEGO_like_Reconfigurable_Frameworks.pdf:\n",
      "Modular RAG framework, RAG system complexity, Modular RAG decomposition, RAG flow patterns, RAG technology compatibility.\n",
      "\n",
      "Generating topics for 2408.02545v1.RAG_Foundry__A_Framework_for_Enhancing_LLMs_for_Retrieval_Augmented_Generation.pdf:\n",
      "Identified Topics for 2408.02545v1.RAG_Foundry__A_Framework_for_Enhancing_LLMs_for_Retrieval_Augmented_Generation.pdf:\n",
      "Open-source RAG framework, RAG Foundry, LLM enhancement, Retrieval-augmented generation, Data creation, training, inference, evaluation, Llama-3 and Phi-3 augmentation, Knowledge-intensive dataset performance.\n",
      "\n",
      "Generating topics for 2410.20299v1.EACO_RAG__Edge_Assisted_and_Collaborative_RAG_with_Adaptive_Knowledge_Update.pdf:\n",
      "Identified Topics for 2410.20299v1.EACO_RAG__Edge_Assisted_and_Collaborative_RAG_with_Adaptive_Knowledge_Update.pdf:\n",
      "Edge-assisted RAG, Scalability of RAG, Distributed Knowledge, Adaptive Knowledge Updates, Multi-armed Bandit Framework, Resource Optimization, Response Accuracy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to perform topic modeling using Gemini\n",
    "def generate_topics_with_gemini(summary):\n",
    "    \"\"\"Generates concise topics based on the provided summary using the Gemini API.\"\"\"\n",
    "    try:\n",
    "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "        response = model.generate_content(\n",
    "            f\"Identify concise topics (as short phrases) from the following summary:\\n{summary}\\n\\nPlease separate multiple topics with commas:\"\n",
    "        )\n",
    "        return response.text.strip()  # Clean up the response text\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while generating topics: {e}\")\n",
    "        return None\n",
    "\n",
    "# Generate topics for each summary\n",
    "for pdf_file, summary in zip(pdffiles, summaries):\n",
    "    print(f\"Generating topics for {pdf_file}:\")\n",
    "    if summary:  # Check if summary is not None\n",
    "        topics = generate_topics_with_gemini(summary)\n",
    "        if topics:\n",
    "            print(f\"Identified Topics for {pdf_file}:\\n{topics}\\n\")\n",
    "        else:\n",
    "            print(f\"No topics identified for {pdf_file}.\\n\")\n",
    "    else:\n",
    "        print(f\"No summary available for {pdf_file}.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Store Article Summary + Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import pickle\n",
    "\n",
    "# Load document catalog from pickle files\n",
    "if os.path.exists('document_catalog.pickle'):\n",
    "    with open('document_catalog.pickle', 'rb') as pkl_file:\n",
    "        df_documents = pickle.load(pkl_file) \n",
    "else:\n",
    "    df_documents = pd.DataFrame(columns=[\"filename\", \"status\", \"topic\", \"summary\", \"img_folder\", \"imgs\"])\n",
    "\n",
    "# Create a list of new entries for updates\n",
    "updates = []\n",
    "\n",
    "# Loop through each PDF file and its summary\n",
    "for pdf_file, summary in zip(pdffiles, summaries):\n",
    "    if summary:  # Ensure the summary is not None\n",
    "        topics = generate_topics_with_gemini(summary)  # Generate topics for the summary\n",
    "        status = \"Success\"\n",
    "    else:\n",
    "        topics = None\n",
    "        status = \"Failed\"\n",
    "\n",
    "    # Prepare a dictionary for the current document's update\n",
    "    updates.append({\n",
    "        \"filename\": pdf_file,\n",
    "        \"status\": status,\n",
    "        \"topic\": topics,\n",
    "        \"summary\": summary,\n",
    "        \"img_folder\": None,  # Placeholder for image folder, if applicable\n",
    "        \"imgs\": None  # Placeholder for images, if applicable\n",
    "    })\n",
    "\n",
    "# Update the existing DataFrame without creating duplicates\n",
    "for update in updates:\n",
    "    filename = update['filename']\n",
    "    \n",
    "    if filename in df_documents['filename'].values:\n",
    "        # Update the existing row based on the filename\n",
    "        df_documents.loc[df_documents['filename'] == filename, ['status', 'topic', 'summary']] = update['status'], update['topic'], update['summary']\n",
    "    else:\n",
    "        # If the filename doesn't exist, append the new entry\n",
    "        df_documents = df_documents.append(update, ignore_index=True)\n",
    "\n",
    "# Save the updated DataFrame as a pickle file\n",
    "with open('document_catalog.pickle', 'wb') as pkl_file:\n",
    "    pickle.dump(df_documents, pkl_file)\n",
    "\n",
    "# Display the updated DataFrame in a scrollable format\n",
    "# display(HTML(df_documents.to_html(max_rows=10, max_cols=7, justify='left')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>filename</th>\n",
       "      <th>status</th>\n",
       "      <th>topic</th>\n",
       "      <th>summary</th>\n",
       "      <th>img_folder</th>\n",
       "      <th>imgs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a5cdaa51-39b4-42fe-bc76-e19fb729c37b</td>\n",
       "      <td>2401.15391v1.MultiHop_RAG__Benchmarking_Retrie...</td>\n",
       "      <td>Success</td>\n",
       "      <td>MultiHop-RAG dataset, Retrieval-Augmented Gene...</td>\n",
       "      <td>This paper introduces MultiHop-RAG, a novel da...</td>\n",
       "      <td>./figure/document_0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>012e560a-9388-4f1f-9ae3-1b4afc2a0bcd</td>\n",
       "      <td>2407.21059v1.Modular_RAG__Transforming_RAG_Sys...</td>\n",
       "      <td>Success</td>\n",
       "      <td>Modular RAG framework, RAG system complexity, ...</td>\n",
       "      <td>This paper introduces Modular RAG, a new frame...</td>\n",
       "      <td>./figure/document_1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a4b74ce1-b399-4b18-93ac-0c620d1438c7</td>\n",
       "      <td>2408.02545v1.RAG_Foundry__A_Framework_for_Enha...</td>\n",
       "      <td>Success</td>\n",
       "      <td>RAG Foundry framework, Open-source RAG framewo...</td>\n",
       "      <td>This paper introduces RAG Foundry, an open-sou...</td>\n",
       "      <td>./figure/document_2</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>decc1461-6857-422c-b0d4-b6f6420e0d6a</td>\n",
       "      <td>2410.20299v1.EACO_RAG__Edge_Assisted_and_Colla...</td>\n",
       "      <td>Success</td>\n",
       "      <td>Edge-assisted RAG, Scalability of RAG systems,...</td>\n",
       "      <td>This paper proposes EACO-RAG, an edge-assisted...</td>\n",
       "      <td>./figure/document_3</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  docid  \\\n",
       "0  a5cdaa51-39b4-42fe-bc76-e19fb729c37b   \n",
       "1  012e560a-9388-4f1f-9ae3-1b4afc2a0bcd   \n",
       "2  a4b74ce1-b399-4b18-93ac-0c620d1438c7   \n",
       "3  decc1461-6857-422c-b0d4-b6f6420e0d6a   \n",
       "\n",
       "                                            filename   status  \\\n",
       "0  2401.15391v1.MultiHop_RAG__Benchmarking_Retrie...  Success   \n",
       "1  2407.21059v1.Modular_RAG__Transforming_RAG_Sys...  Success   \n",
       "2  2408.02545v1.RAG_Foundry__A_Framework_for_Enha...  Success   \n",
       "3  2410.20299v1.EACO_RAG__Edge_Assisted_and_Colla...  Success   \n",
       "\n",
       "                                               topic  \\\n",
       "0  MultiHop-RAG dataset, Retrieval-Augmented Gene...   \n",
       "1  Modular RAG framework, RAG system complexity, ...   \n",
       "2  RAG Foundry framework, Open-source RAG framewo...   \n",
       "3  Edge-assisted RAG, Scalability of RAG systems,...   \n",
       "\n",
       "                                             summary           img_folder imgs  \n",
       "0  This paper introduces MultiHop-RAG, a novel da...  ./figure/document_0   []  \n",
       "1  This paper introduces Modular RAG, a new frame...  ./figure/document_1   []  \n",
       "2  This paper introduces RAG Foundry, an open-sou...  ./figure/document_2   []  \n",
       "3  This paper proposes EACO-RAG, an edge-assisted...  ./figure/document_3   []  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11. Store Vector DB (New version of Chroma persists data automatically after vectorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some vector databases of choices: Chroma, FAISS, Pinecone ... \n",
    "We will create Chroma vector database with openai embedding method. \n",
    "\n",
    "Note: different embedding methods will result different vector dimensions and cannot be stored together. \n",
    "The same embedding method to be used in retrieval pipeline\n",
    "\n",
    "Reference: https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 12. Run Full Embedding Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to process a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_document(df_row, model, retriever):\n",
    "    directory_path = os.path.join(os.getenv(\"DOC_ARVIX\"))\n",
    "    doc_elements = extract_pdf_elements(directory_path,df_row[\"filename\"],df_row[\"img_folder\"])\n",
    "    texts, tables = categorize_elements(doc_elements)\n",
    "    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=4000, chunk_overlap=0\n",
    "    )\n",
    "    joined_texts = \" \".join(texts)\n",
    "    texts_4k_token = text_splitter.split_text(joined_texts)\n",
    "    text_summaries, table_summaries = generate_text_summaries(\n",
    "        texts_4k_token, tables, model_google, summarize_texts=False # Will use the original Text for getting vectorized\n",
    "    )\n",
    "    img_base64_list, image_summaries = generate_img_summaries(df_documents.loc[0][\"img_folder\"],model)\n",
    "    embed_documents(\n",
    "        retriever,\n",
    "        text_summaries,\n",
    "        texts_4k_token,\n",
    "        table_summaries,\n",
    "        tables,\n",
    "        image_summaries,\n",
    "        img_base64_list,\n",
    "        dict(df_row)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run embedding for all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "figure-1-1.jpg\n",
      "figure-4-2.jpg\n",
      "figure-7-3.jpg\n",
      "figure-1-1.jpg\n",
      "figure-4-2.jpg\n",
      "figure-7-3.jpg\n",
      "figure-1-1.jpg\n",
      "figure-4-2.jpg\n",
      "figure-7-3.jpg\n",
      "figure-1-1.jpg\n",
      "figure-4-2.jpg\n",
      "figure-7-3.jpg\n"
     ]
    }
   ],
   "source": [
    "vdb_google.reset_collection()\n",
    "for r in range(len(df_documents)):\n",
    "    embedding_document(df_documents.loc[r], model_google,retriever_mv_google)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Retrieval Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval pipeline is to retrieve relevant chunk of knowledge from pre-prepared vectorized knowledge to enrich the LLM prompt with specified context. This pipeline is run to respond to each user’s query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to load from store if there is, here is Chroma vectordb we have just persisted. \n",
    "Perform a semantic search in the vectorized database to retrieve relevant embedded documents.\n",
    "\n",
    "NOTE: The embedding method used in this step must be same as which used to vectorize knowledges in the previous pipeline.\n",
    "\n",
    "There is opportunity to improve efficiency and quality of similarity search, especially when the knowledgebase gets larger and more complicated (type of sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Util functions for retrieval and response processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "\n",
    "from IPython.display import HTML, display\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def plt_img_base64(img_base64):\n",
    "    \"\"\"Disply base64 encoded string as image\"\"\"\n",
    "    # Create an HTML img tag with the base64 string as the source\n",
    "    image_html = f'<img src=\"data:image/jpeg;base64,{img_base64}\" />'\n",
    "    # Display the image by rendering the HTML\n",
    "    display(HTML(image_html))\n",
    "\n",
    "\n",
    "def looks_like_base64(sb):\n",
    "    \"\"\"Check if the string looks like base64\"\"\"\n",
    "    return re.match(\"^[A-Za-z0-9+/]+[=]{0,2}$\", sb) is not None\n",
    "\n",
    "\n",
    "def is_image_data(b64data):\n",
    "    \"\"\"\n",
    "    Check if the base64 data is an image by looking at the start of the data\n",
    "    \"\"\"\n",
    "    image_signatures = {\n",
    "        b\"\\xff\\xd8\\xff\": \"jpg\",\n",
    "        b\"\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a\": \"png\",\n",
    "        b\"\\x47\\x49\\x46\\x38\": \"gif\",\n",
    "        b\"\\x52\\x49\\x46\\x46\": \"webp\",\n",
    "    }\n",
    "    try:\n",
    "        header = base64.b64decode(b64data)[:8]  # Decode and get the first 8 bytes\n",
    "        for sig, format in image_signatures.items():\n",
    "            if header.startswith(sig):\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    \"\"\"\n",
    "    Resize an image encoded as a Base64 string\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = io.BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def split_image_text_types(docs):\n",
    "    \"\"\"\n",
    "    Split base64-encoded images and texts\n",
    "    \"\"\"\n",
    "    b64_images = []\n",
    "    texts = []\n",
    "    for doc in docs:\n",
    "        # Check if the document is of type Document and extract page_content if so\n",
    "        if isinstance(doc, Document):\n",
    "            doc = doc.page_content\n",
    "        if looks_like_base64(doc) and is_image_data(doc):\n",
    "            doc = resize_base64_image(doc, size=(1300, 600))\n",
    "            b64_images.append(doc)\n",
    "        else:\n",
    "            texts.append(doc)\n",
    "    return {\"images\": b64_images, \"texts\": texts}\n",
    "\n",
    "\n",
    "def img_prompt_func(data_dict):\n",
    "    \"\"\"\n",
    "    Join the context into a single string\n",
    "    \"\"\"\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        for image in data_dict[\"context\"][\"images\"]:\n",
    "            image_message = {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image}\"},\n",
    "            }\n",
    "            messages.append(image_message)\n",
    "\n",
    "    # Adding the text for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"You are a research professor tasking with providing research advice.\\n\"\n",
    "            \"You will be given a mixed of text, tables, and image(s) usually of charts or graphs from published articles.\\n\"\n",
    "            \"Use this information to provide research advice related to the user question. \\n\"\n",
    "            f\"User-provided question: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and / or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "\n",
    "def multi_modal_rag_chain(model,retriever):\n",
    "    \"\"\"\n",
    "    Multi-modal RAG chain\n",
    "    \"\"\"\n",
    "\n",
    "    # Multi-modal LLM\n",
    "\n",
    "    # RAG pipeline\n",
    "    chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | RunnableLambda(img_prompt_func)\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Process Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"What is the performance of GPT-4 vs Mixtral?\"\n",
    "#user_query = \"Describe the RAG-Sequence Model?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Retrieve Relevant Docs - Text, Table, Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1. Multivector retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs_mvr = retriever_mv_google.invoke(user_query, limit=3) # Top k relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'doc_id': 'a5561dd0-1216-441a-88b1-f3e5757c828c', 'source': '2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries.pdf', 'type': 'image', 'paper_id': 'a5cdaa51-39b4-42fe-bc76-e19fb729c37b'}, page_content='/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAIlAl8DASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD3+iiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAqve39npts1zf3cFrAv3pZ5Aij6knFWK5f4kf8AJNfEf/YPl/8AQTQBt2Ws6XqMrRWOpWd1Iqh2SCdXIU9yAelXa8zMcnib4naC8ET6c/h+1aW5M+FluEmUKqoATuTIOTnAJx1rbvPE+s3b6mfDmnJe/wBnXX2Vo3VR5zqFLgOZF2EbiBlSOM9+ADsar3l9a6fCst3OkMbSJErOcAuxCqPqSQK4yTV9cs9S8aXK3VtOumW8clvBLCwUfujJjIb65Pf2HFF14k8Vafo1lqt1BpHkXl1ZRpHGJC6JMyq+cnG4FhjHH5cgHd0Vx994l1q5k1QeHdOS9OnXItmjdVHnOFVnXeZF2HD45UjIz3qQeINUtPEGt6fqL2YjgsVvdPMduwaRSWVg37whirBRgYzuHTNAHWU2SRIY2kkdUjQFmZjgKB1JNcnca5r4kn02zt7a51W0s45pzHD+6aWTftUBpVKj5OuW69sVNb67q2p6idNtoLayvbbT4bq8S5Uy+XLLu2xDawHGxstk9sUAb9hqFpqljFe2FxHcWsozHLGcqwzjIP4VHc6vp1nP5FzewRzbd/ls43Bf7xHUD36VzPwn4+Fug8c+Q3A/32qj8HrmXVfB02u3rmTUtSvJpbp26gq2xU9gqqAB2oA64+I9F32yDVbNmupfJgCTKxlfuFweTWnXmnjjSodI1Lwmmk28Ubz+IfPEbHCCRo2yeOgJGT9TVqbxd4htNK1h7gaYbzSdXgspCkEnlzxStDtYDzMo2Jc8lhkUAeg02SRIYmlldUjQFmZjgKB1JNcfq3ibVtOuPFsaLZSDSdLj1C1zEwzkTEq/zc/6oYIx1qbS9e1iTxPaaZqkdiIb7TWvYvs6vujZXRWRiThgRIDkAdCOetAHR6fqFpqtjFfWFxHcWsozHLGcqwzjIP4VZry74Uane3vhDQdO0u5s0hsbXOoCeFnkyzEoqYdcZG4liCOgGTnHqNABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVVutSsbF1S7vba3ZhkCWVUJH4mrVZkSg+J7zIB/0ODr/vy0AO/t/Rv+gvYf+BKf40f2/o3/AEF7D/wJT/Gr+xP7q/lRsT+6v5UAUP7f0b/oL2H/AIEp/jR/b+jf9Bew/wDAlP8AGr+xP7q/lRsT+6v5UAUP7f0b/oL2H/gSn+NH9v6N/wBBew/8CU/xq/sT+6v5UbE/ur+VAFD+39G/6C9h/wCBKf40f2/o3/QXsP8AwJT/ABq/sT+6v5UbE/ur+VAFD+39G/6C9h/4Ep/jR/b+jf8AQXsP/AlP8av7E/ur+VGxP7q/lQBQ/t/Rv+gvYf8AgSn+NH9v6N/0F7D/AMCU/wAav7E/ur+VGxP7q/lQBQ/t/Rv+gvYf+BKf41keJpdJ8RaBd6QPENhaxXcbRSyCVHbYRg4+YAH35rptif3V/KjYn91fyoA4jUNO0m+uNJ1FPFdpa6xpoKJeQSRgSxHrG6FiGU4B69eRio30jSF1m6vrPxktlDqBV9QtIJofLuHAClhuy0ZYDBKnPvnmu72J/dX8qNif3V/KgDjL+y0a6uNblg8UW0C6vbLBNH5sTKpCFNw5z909M4yM89KZqVnp2peHdO0h/FenoLOWGXzlMeZDEwZARv45UZx19q7bYn91fyo2J/dX8qAOGn0rR/7cu9RsfGIsIdQKtf2kE8JjnYDbuBYEoSBglSD9DzV/V7fw1q+saPqUutWkc2lyM8YjukAkUgfI3PTcqN9Vrqtif3V/KjYn91fyoA4vV7LSr7Xl1rTfGK6TeNCtvcG3mgdZ4wSVBVwRuG5sN70r2OiQ61FqmmeKobCX7MlpcKs8MgnjQkqTuzhhk/N785rs9if3V/KjYn91fyoA5jwsNB8LeHbXRofENvcxWwISSa4i3YJJx8uOMk+/vVXTLbSNAuLv+w/EWm21ndzNcSWk7LKiSN94xkOpUHqQcj0xXY7E/ur+VGxP7q/lQBxetWelaxdaRcN4qtEfTbv7YpeSN/MfG0A/MAFwSMDHr1qvLo+jXVvr0Vz4qtCdXuIrovE8amCSPZsK5Y5A8tOD6H1rvNif3V/KjYn91fyoA4S60nSrtNZaXxlFJPq1gthO7yQYCDfyFGMHEjAc/XNTfZ7Mavp+pr4t00T2VlJZqMJtdXKksRv6/InT0PrXa7E/ur+VGxP7q/lQB5vonhrSvD50h9P8ZWkcunwtbM+Yz9qhLbgkg3c7SWIIwfm/Puf7f0b/AKC9h/4Ep/jV/Yn91fyo2J/dX8qAKH9v6N/0F7D/AMCU/wAaP7f0b/oL2H/gSn+NX9if3V/KjYn91fyoAof2/o3/AEF7D/wJT/Gj+39G/wCgvYf+BKf41f2J/dX8qNif3V/KgCh/b+jf9Bew/wDAlP8AGj+39G/6C9h/4Ep/jV/Yn91fyo2J/dX8qAKH9v6N/wBBew/8CU/xo/t/Rv8AoL2H/gSn+NX9if3V/KjYn91fyoAof2/o3/QXsP8AwJT/ABo/t/Rv+gvYf+BKf41f2J/dX8qNif3V/KgCh/b+jf8AQXsP/AlP8aP7f0b/AKC9h/4Ep/jV/Yn91fyo2J/dX8qAKH9v6N/0F7D/AMCU/wAavxyRzRLLE6vG4DK6nIYHoQaNif3V/Ks/w/8A8i1pn/XpF/6AKALolZgCsTEHocgZ/Wl8x/8Ani35j/Gs/V7yWw0CS5gIEiKmMjI5IH9a5D/hMNW/vxf9+687F5pQwk1TqXu1fQ7cNgKuIhzwtbY7/wAx/wDni35j/GjzH/54t+Y/xrgP+Ew1b+/F/wB+6P8AhMNW/vxf9+65f7fwnZ/d/wAE6P7HxHl9/wDwDv8AzH/54t+Y/wAaPMf/AJ4t+Y/xrnfDGt3mq3M6XTIVRARtXHeumr08NiYYmmqsNmcFehKhN057kfmP/wA8W/Mf40eY/wDzxb8x/jXLaN4tuLzxxrPhy/t4ovshH2SePIFwAiO4IPRlEsX1yTVZvGlzLrPiK1UWNlZaZZiaC8u2JWViZF3MARhA8bD1IGR1FdBidl5j/wDPFvzH+NHmP/zxb8x/jWZP4m0m0nkt7m8CTxIjOnlP0dgqkccgsQBjPNQN4x0WK91O2uLo2w0zyxcyzxtGis4yBuYAE4wffPGeaANrzH/54t+Y/wAaPMf/AJ4t+Y/xrMfxRoaW4nbUoPJ5JkByqgNsJY/wjdxk4GaZYardXHizWNKmWHyLSC2mhZFIY+YZQQ2SQceWMYA60Aa3mP8A88W/Mf40eY//ADxb8x/jWJrevS6dqMVlEbaN5LWW4R7kkLKyFcRrjuckk8kAdD2l/wCEo02FQt7JJaXItluZLeWJ90aEqOw5wzAHHegDW8x/+eLfmP8AGjzH/wCeLfmP8a5PxRrninShNd6Zp+knTolRVa+uJElmkYgBUVVPUsqgMQc+1WH1rV28SanpUR09BZ2MN2jyq4D7zINrHd8oBjPzYPXpxQB0nmP/AM8W/Mf405GDrkAjtg9qzvD2rjX/AA9YasIHt/tcKy+UxyVJ7Z7j0PcYNX4fut/vt/OgCSiiigAooooAKKKKACiiigAooooAKKK53XfEV3pHiHQdPSwjltdTuGge5M2DGwRmwExzkL1zQB0VcN4q1C7sfE3+izvFvs4923vh5Mfzrb0nXrjUPE+vaTPZpAmm+R5ciybjKJFY5IwMdOnNc341/wCRlT/rzT/0N683N5yhg5yi7PT80d2WxUsVFSV1r+TKX/CQat/z/S/nR/wkGrf8/wBL+dZtFfE/Wq/87+9n1X1ej/IvuRpf8JBq3/P9L+dH/CQat/z/AEv51m0UfWq/87+9h9Xo/wAi+5Gl/wAJBq3/AD/S/nR/wkGrf8/0v51m0UfWq/8AO/vYfV6P8i+5Gl/wkGrf8/0v50f8JBq3/P8AS/nWbRR9ar/zv72H1ej/ACL7kaX/AAkGrf8AP9L+dH/CQat/z/S/nWbRR9ar/wA7+9h9Xo/yL7kaX/CQat/z/S/nR/wkGrf8/wBL+dZtFH1qv/O/vYfV6P8AIvuRpf8ACQat/wA/0v50f8JBq3/P9L+dZtFH1qv/ADv72H1ej/IvuRpf8JBq3/P9L+dH/CQat/z/AEv51m0UfWq/87+9h9Xo/wAi+5Gl/wAJBq3/AD/S/nR/wkGrf8/0v51m0UfWq/8AO/vYfV6P8i+5Gl/wkGrf8/0v50f8JBq3/P8AS/nWbRR9ar/zv72H1ej/ACL7kaX/AAkGrf8AP9L+dH/CQat/z/S/nWbRR9ar/wA7+9h9Xo/yL7kaX/CQat/z/S/nR/wkGrf8/wBL+dZtFH1qv/O/vYfV6P8AIvuRpf8ACQat/wA/0v50f8JBq3/P9L+dZtFH1qv/ADv72H1ej/IvuRpf8JBq3/P9L+dH/CQat/z/AEv51m0UfWq/87+9h9Xo/wAi+5Gl/wAJBq3/AD/S/nR/wkGrf8/0v51m0UfWq/8AO/vYfV6P8i+5Gl/wkGrf8/0v50f8JBq3/P8AS/nWbRR9ar/zv72H1ej/ACL7kaX/AAkGrf8AP9L+delWzF7WFmOWKKSfXivI69btP+PKD/rmv8q+i4fq1KkqnPJvbd+p4uc04QjDlSW5NRRRX0x4IUUUUAFFFFABRRRQAVm+H/8AkWtM/wCvSL/0AVpVm+H/APkWtM/69Iv/AEAUAN1ezlv9AktoADI6pjJwOCD/AErkP+EP1b+5F/38r0CH/UR/7o/lT687F5XQxc1UqXulbQ7cNj6uHhyQtbc88/4Q/Vv7kX/fyj/hD9W/uRf9/K9Dorl/sDCd39//AADo/tjEeX3f8E5nwxol5pVzO90qBXQAbWz3rpqKK9PDYaGGpqlDZHBXryrzdSe557qXhbXL281PULDbp+pprCXthcOyupiMEUEqsBnqqMcf7vIPRupeG9V83xJb2GmE2t5oSaVZs06ZLKJRubJyB+9HPJ4PHNeiUV0GJw2u6Xr97d2ur6dpsUepaWI1tEmlQrcK+POWQj7oAA247jPOcCn4g8L61eReMPstnHKdcjtTCrTKvlsiBWV/yzkZ616LRQBw3jTQ/EGvR3ttYWtoLa80pod8s/lyRzZY7W2g71OQAN20HceeKu22j6nd+JdZurxJbGG8tLSNJrW4G4PEZC4BxnGZMA45APSusooA5PW/Dst6sNpcQPq2nLaugjnmCyJPuBSXdxzjI3D5lxwDk1Q1zQNdvYLCeJEn1bRYYmtbmTYUvJjt80OCflQ7VPqG+YcqK7uigDJu7KbVJ9Ka4gVIYH+1TRswYiRV+ReODhmLZ9UFc5qfhiPU/GWpajqfhq31S0ksIba2Ewhch0aRmPzHKg715HPHTpXc0UAYvhLTL7R/C9jYalc/aLuJCHbeXC5YkIGPLBQQoJ5O2taH7rf77fzqSo4fut/vt/OgCSiiigAooooAKKKKACiiigCO4TzLaVN7puUjchwR9DXjmg3uqSfDLwp4ml1vVJdRk1KGKQvdOY5I3ujGysmdrDB6kEjjBAAFexXEP2i2kh8x496ld8Zwy57j3rlovh3pMHh200GG61BNOtJxPDGJhlXD7xztzw3IHvQBilriPS/iPbLqGobLFi1qxvZS8JFokg2uW3AbiTjOKrRXE93ovwqubmaSaeWaJ5JZGLM7G0kJJJ5JPrXV3ngfTr251GaW71BRqUIivYo59qTkJsDsAPvbcDjAOOQaQeBNLSHRYY7m/SPRsGzUT5CkArk5Bz8px6Y7UAQaB/yUjxh/1zsf/Rb1ynxFvr3T/FEZLwSLJarsHlFSoDvwTu5PPXj6V6DZ+Hbax8QX2sxXN2bm+2idGkBjYKCFGMcYBPT8c15x8WP+Rls/+vMf+htXXgcNSxVeNGtHmi73T9LnLjMRVw9CVWk7SXX5nM/8JBd/884f++T/AI0f8JBd/wDPOH/vk/41k0V9B/q1lP8Az4ieF/b+Zf8AP5mt/wAJBd/884f++T/jR/wkF3/zzh/75P8AjWTRR/q1lP8Az4iH9v5l/wA/ma3/AAkF3/zzh/75P+NH/CQXf/POH/vk/wCNZNFH+rWU/wDPiIf2/mX/AD+Zrf8ACQXf/POH/vk/40f8JBd/884f++T/AI1k0Uf6tZT/AM+Ih/b+Zf8AP5mt/wAJBd/884f++T/jR/wkF3/zzh/75P8AjWTRR/q1lP8Az4iH9v5l/wA/ma3/AAkF3/zzh/75P+NH/CQXf/POH/vk/wCNZNFH+rWU/wDPiIf2/mX/AD+Zrf8ACQXf/POH/vk/40f8JBd/884f++T/AI1k0Uf6tZT/AM+Ih/b+Zf8AP5mt/wAJBd/884f++T/jR/wkF3/zzh/75P8AjWTRR/q1lP8Az4iH9v5l/wA/ma3/AAkF3/zzh/75P+NH/CQXf/POH/vk/wCNZNFH+rWU/wDPiIf2/mX/AD+Zrf8ACQXf/POH/vk/40f8JBd/884f++T/AI1k0Uf6tZT/AM+Ih/b+Zf8AP5mt/wAJBd/884f++T/jR/wkF3/zzh/75P8AjWTRR/q1lP8Az4iH9v5l/wA/ma3/AAkF3/zzh/75P+NH/CQXf/POH/vk/wCNZNFH+rWU/wDPiIf2/mX/AD+Zrf8ACQXf/POH/vk/40f8JBd/884f++T/AI1k0Uf6tZT/AM+Ih/b+Zf8AP5mt/wAJBd/884f++T/jR/wkF3/zzh/75P8AjWTRR/q1lP8Az4iH9v5l/wA/ma3/AAkF3/zzh/75P+NH/CQXf/POH/vk/wCNZNFH+rWU/wDPiIf2/mX/AD+Zrf8ACQXf/POH/vk/40f8JBd/884f++T/AI1k0Uf6tZT/AM+Ih/b+Zf8AP5mt/wAJBd/884f++T/jXRR/FPXIokjW107CqFGY37f8Drh6K3oZHl1C7pUUrmVXN8dWt7Sq3Y7r/ha+u/8APpp3/ft//i6P+Fr67/z6ad/37f8A+LrhaK6P7Nwn8iMPr+J/nZ3X/C19d/59NO/79v8A/F0f8LX13/n007/v2/8A8XXC0Uf2bhP5EH1/E/zs7r/ha+u/8+mnf9+3/wDi6P8Aha+u/wDPpp3/AH7f/wCLrhaKP7Nwn8iD6/if52fStpK1xZQTOAGkjVyB0yRmpqq6b/yC7T/rgn/oIq1Xw81aTR9jF3igrN8P/wDItaZ/16Rf+gCtKs3w/wD8i1pn/XpF/wCgCpKMfx5/yT++/wB2H/0YleHV9GX+mW2saO1hdhjBKq7gpweCCOfqBXPf8Kz8Of8APO5/7/GvdyvMqOFouFS97309EeLmOAq4iqpwta1vxZ4pRXtf/Cs/Dn/PO5/7/Gj/AIVn4c/553P/AH+Nel/bmF8/u/4Jwf2NiPL7/wDgHMfCX/kKaj/1wX/0KvV6xND8KaX4emllsElV5VCtvfdwDmtuvncwxEMRXdSGzse7gaEqFBU57nF6HqVx4j8Y+JrW8mmig0ieO3t7aGVouGTcZHKkFi3bPAA4HU1gX2rjUpNIjsn1+KOPxJJp88cl6Y3k2xSl4wyS/MoZRgse1egTaFaPqrapAZLW/eMRSTwEAyIOgYEFWxk4JGRng1nN4J0vFt5Ul1C1vfPqIZJAS9y2d0jZBznc3HTnpXEdhnaHo+tRabqsmt3d8saXE7adCb1jJFAQNokdG+ZgQcZLYB607wzqdjpvg7w5qeq6ld/aLzTIC7TzyzCRjGjMxBJwcnrx1Ndbd2/2u0lt/NkiEi7S8eNwB64yCKg0jTIdF0m1023kle3tYlhi8wglUUYUZAGcAAetAHF6p4106TxfosUfiCyt7SHUZLeeH7ZGpkxbz5Mi5yFEgRVzjLc4PymvQaqXenW97dWNxMGMljMZ4cHADGN4zn1+WRqt0AFFFFABRRRQAVHD91v99v51JUcP3W/32/nQBJRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABXnvjXwpfeJ/EyCylto/s9mm/zmYZ3O+MYB/umvQqzIf+RnvP8Arzg/9DlrWhWnQqKpDdGVajGtB057M8x/4VRrv/P3p3/fx/8A4ij/AIVRrv8Az96d/wB/H/8AiK9hor0f7axfdfccP9kYbs/vPHv+FUa7/wA/enf9/H/+Io/4VRrv/P3p3/fx/wD4ivYaKP7axfdfcH9kYbs/vPHv+FUa7/z96d/38f8A+Io/4VRrv/P3p3/fx/8A4ivYaKP7axfdfcH9kYbs/vPHv+FUa7/z96d/38f/AOIo/wCFUa7/AM/enf8Afx//AIivYaKP7axfdfcH9kYbs/vPHv8AhVGu/wDP3p3/AH8f/wCIo/4VRrv/AD96d/38f/4ivYaKP7axfdfcH9kYbs/vPHv+FUa7/wA/enf9/H/+Io/4VRrv/P3p3/fx/wD4ivYaKP7axfdfcH9kYbs/vPHv+FUa7/z96d/38f8A+Io/4VRrv/P3p3/fx/8A4ivYaKP7axfdfcH9kYbs/vPHv+FUa7/z96d/38f/AOIo/wCFUa7/AM/enf8Afx//AIivYaKP7axfdfcH9kYbs/vPHv8AhVGu/wDP3p3/AH8f/wCIo/4VRrv/AD96d/38f/4ivYaKP7axfdfcH9kYbs/vPHv+FUa7/wA/enf9/H/+Io/4VRrv/P3p3/fx/wD4ivYaKP7axfdfcH9kYbs/vPHv+FUa7/z96d/38f8A+Io/4VRrv/P3p3/fx/8A4ivYaKP7axfdfcH9kYbs/vPHv+FUa7/z96d/38f/AOIo/wCFUa7/AM/enf8Afx//AIivYaKP7axfdfcH9kYbs/vPHv8AhVGu/wDP3p3/AH8f/wCIo/4VRrv/AD96d/38f/4ivYaKP7axfdfcH9kYbs/vPHv+FUa7/wA/enf9/H/+Io/4VRrv/P3p3/fx/wD4ivYaKP7axfdfcH9kYbs/vPHv+FUa7/z96d/38f8A+Io/4VRrv/P3p3/fx/8A4ivYaKP7axfdfcH9kYbs/vPHv+FUa7/z96d/38f/AOIo/wCFUa7/AM/enf8Afx//AIivYaKP7axfdfcH9kYbs/vPHv8AhVGu/wDP3p3/AH8f/wCIo/4VRrv/AD96d/38f/4ivYaKP7axfdfcH9kYbs/vPHv+FUa7/wA/enf9/H/+Io/4VRrv/P3p3/fx/wD4ivYaKP7axfdfcH9kYbs/vPHv+FUa7/z96d/38f8A+Io/4VRrv/P3p3/fx/8A4ivYaKP7axfdfcH9kYbs/vPHv+FUa7/z96d/38f/AOIo/wCFUa7/AM/enf8Afx//AIivYaKP7axfdfcH9kYbs/vIbSJreyghcgtHGqEjpkDFTUUV5Td3c9JKysFZvh//AJFvTP8Ar0i/9BFaVZvh7/kW9L/69Iv/AEEUhlsfaIwEVI2UDAJYg/ypd1z/AM8ov+/h/wAKmooAh3XP/PKL/v4f8KN1z/zyi/7+H/CpqKAId1z/AM8ov+/h/wAKN1z/AM8ov+/h/wAKmooAh3XP/PKL/v4f8KN1z/zyi/7+H/CpqKAId1z/AM8ov+/h/wAKN1z/AM8ov+/h/wAKmooAh3XP/PKL/v4f8KN1z/zyi/7+H/CpqKAId1z/AM8ov+/h/wAKN1z/AM8ov+/h/wAKmooAh3XP/PKL/v4f8KN1z/zyi/7+H/CpqKAId1z/AM8ov+/h/wAKfGpRMHGSSTj1NPooAKKKKACiiigAooooAKKKKACiq17qFnp0cT3lzHAssqQxmRsbnY4VR7k1ZoAKKKKACuV1jXf7F8TSH7N53m2cX8e3GHk9j611Vee+Nf8AkZU/680/9DeuDM606GFnUpuzVvzR14ClCriIwmrp3/Jml/wnf/UN/wDI/wD9jVnT/GH26/htfsOzzW27vOzj8NtcLWl4f/5D9l/10r5jD5vjJ1YxlPRtdF39D362W4WNOUlHVJ9X/men0UUV9sfKhRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVm+Hv+Rb0v/r0i/wDQRWlWb4e/5FvS/wDr0i/9BFAGlRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAFTVNSttH0q71K8cpbWsTTSsBkhVGTgdzXOR+Jtctr2wl1TQ1g0m7t5Z5biKQu1iEXcBNxjlfToeOep2/EWjp4g8N6jpEkhjW8t3h3gZ2kjAP4HmuT0Ow8bajpB8P+KbawhsltXtp723uC8l2CpUbVx8vXJJ9OAM8AEreOr2PRbLxPLp8C+G7mVVZzIftEMTttSVlxtIJIJUHIB6nmrN14o1+XXNf0jS9EtpbjToreSF5rrakiyByS2BkH5MADPuRWVaeFNZk8BxeB9Rt1e3ilSBtQSRQklqkgcELncJCoC4xgHnJrTt7TXrPxx4i1NdISWyvrWCKBhdKGLRB8ZHYMZPwx78AGXY/EXV7/QdF8RpoltHol5LFBcu10TMjvL5RKKFwVDEdSCeeO9ad14o8Rza3rukaToVrPcacIHjea72pIjqx5+XIY7QAMY6ksOM83YeFfFFj8J9L8Mf2VC99a3aSu32tQhRLgTZz1yfu49s1rade6rD8R/FL22k/aA9nZF4xOivHLsfaDngr1BIORgYBzwAXNN8dXWueH9IvNP0l47m9lkhuvtG7ybAxBvM81gPVcL0zkdKrP8Qb5fBF1riaZaz3FlfmxuUhusxk+aqb422/MDuUgHHU88VnP4R8UaXaaCttFZ6pELu4vNYsTP5MU08rb1YEg5RGPAI/hBwT0Zd+GvFr+FfEWmf2bZSXF/q4vIWS6whUyJIc5GQPk2juc5wMcgG7quv63ZR2R1fw5YeVca1b2sR+1eZ5aO6BJMbfvgk+mCMgmp9R8V339vX+laUmnyXVj5Re1uJWE86sAzNGo6gKffJBHHdPF9prer6ZowsdKVp4dQtr2eN7lVCCKQOVz3Jxis3xX4YvvEl3dM+irHfxPC2kavFMiva8KW38hiFfeQAGBz260Abt5r2p3Or6jp2g2lpcSabGpuGuZmQNKy7liXAPO3BLHgbhwecY4+Itxd6Noeo6fo3mDUL4afcQTT7JLaf5spjbg8qeSR1Bx2q5HpesaD401bUrCyW/03V445JUEqpJDcIuwfeIBRlC57g9qxpfCmu2Om6JDb2UF3dJrR1nUXScIgdmcskYYZON4AJx933oA6Xwx4g1LU9U1jStYsLa0vtOaJiLaYyxvHKpKkEqDn5WB47Vy3jjVLWLxQFlaSMraovzwuASHfoSOR7jiuh0iw1i1+Iev6jPp6LpuoRW8cUwnUsDEGGSvo2/8Me/HG/Fj/kZbP8A68x/6G1aUsvpZhNYWq2oy3tvpr1T7djOrjamCg8RTSbj32108u5n/wBs2H/Pf/xxv8Ku6P4h0u11e2nmutsaPlm8tjgfgK4aiu6nwHl1OampzunfeP8A8icM+McdOLi4Q18n/wDJHun/AAsLwt/0FP8AyXl/+Jo/4WF4W/6Cn/kvL/8AE14XRXtf2Dhv5pfev8jyv7axHZfj/me6f8LC8Lf9BT/yXl/+Jo/4WF4W/wCgp/5Ly/8AxNeF0Uf2Dhv5pfev8g/trEdl+P8Ame6f8LC8Lf8AQU/8l5f/AImj/hYXhb/oKf8AkvL/APE14XRR/YOG/ml96/yD+2sR2X4/5nun/CwvC3/QU/8AJeX/AOJo/wCFheFv+gp/5Ly//E14XRR/YOG/ml96/wAg/trEdl+P+Z7p/wALC8Lf9BT/AMl5f/iaP+FheFv+gp/5Ly//ABNeF0Uf2Dhv5pfev8g/trEdl+P+Z7p/wsLwt/0FP/JeX/4mj/hYXhb/AKCn/kvL/wDE14XRR/YOG/ml96/yD+2sR2X4/wCZ9GaTrNhrlq1zp0/nQq5jLbGXDAA4wwHYir9cL8KP+RWuf+v1/wD0BK7qvm8XRjRrypx2TPoMLVdWjGpLdhRRRXObhRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABWb4e/5FvS/+vSL/ANBFaVZvh7/kW9L/AOvSL/0EUAaVFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVk2vhvTLLXLrWYI7hb67x57m7lZXxkKChbbgZOOOM8VrUUAFFFFABRRRQAUUUUAFeS/FK2nuPEtp5MMku2zGdilsfO3pXrVZkP/Iz3n/XnB/6HLXRhMQ8PWVVK9v8AKxhiaHt6Tpt2ueA/2bff8+Vz/wB+m/wo/s2+/wCfK5/79N/hX0jRXs/6wS/59/j/AMA8n+w4/wA/4f8ABPm7+zb7/nyuf+/Tf4Uf2bff8+Vz/wB+m/wr6Roo/wBYJf8APv8AH/gB/Ycf5/w/4J83f2bff8+Vz/36b/Cj+zb7/nyuf+/Tf4V9I0Uf6wS/59/j/wAAP7Dj/P8Ah/wT5u/s2+/58rn/AL9N/hR/Zt9/z5XP/fpv8K+kaKP9YJf8+/x/4Af2HH+f8P8Agnzd/Zt9/wA+Vz/36b/Cj+zb7/nyuf8Av03+FfSNFH+sEv8An3+P/AD+w4/z/h/wT5u/s2+/58rn/v03+FH9m33/AD5XP/fpv8K+kaKP9YJf8+/x/wCAH9hx/n/D/gnzd/Zt9/z5XP8A36b/AAo/s2+/58rn/v03+FfSNFH+sEv+ff4/8AP7Dj/P+H/BOI+F0EsHhm5WaJ42N4xAdSDjYnrXb0UV4mIre2qyqNWuexQpexpqne9gooorE1CiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACs3w9/yLel/wDXpF/6CK0qzfD3/It6X/16Rf8AoIoA0qKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKxmu7a18T3X2i4ih3WcG3zHC5+eXpmtmvPfGv8AyMqf9eaf+hvXJjsS8Nh5VUr2t+djpwlBV6ypt2v/AJHbf2tpv/QQtP8Av8v+NH9rab/0ELT/AL/L/jXlVFfO/wCsVX+RHtf2JT/nZ6r/AGtpv/QQtP8Av8v+NSQ39ncSeXBdwSvjO1JAx/IGvJq3vB//ACH0/wCubVvhs9qVq0abgtXYxr5RClSlNSeiPQ6KKK+mPCCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACs3w9/yLel/wDXpF/6CK0qzfD3/It6X/16Rf8AoIoA0qKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACsnxLr1t4Z8P3erXSyPHboSFRCxZuw4Bxk4GTwK1q5L4ngn4a67gE4t9xwOwYEn8hQBtzeINLtrdJp7oRBwzKsiMrlV+8dhG7A7nGBUb+KNAjhtJW1mwEd4C1u32hcSgZyV55AwcntiuF1LxPpln8RDfXetPaaTf6ZFFZanB5b27MjyGSMuyMATuU8EdMHtVO7i8L6Hp3gjTtOu1Fiuvi4g+1yAFkKTZdQcfJvIwcAZIx1FAHf/wDCZ+G/sU13/bVn5EL+XI3mcq2M4x16c/Tmp5fFGgwR2kkusWKpeJvt2M64lUDJZeeRgHnpXHWOoaJH8QvHTzXlgswtbZWZ5EDYEbBxyegwufoM1yXhq8sD4F+GElxcQGKDVXSRnYYjcLMQG9CCVPPtQB7Hpuv6TrFvPPp+oW9xFbsVmZH/ANWQM/NnpxzzTLTxJo19dx2ttqVvJPKhkiQPjzUHVk/vr7rkV5l4s0/UNQ1Px3feHczWs+jwwXAgO4XE6sSwGOrCHKnH98Ct3Xb3S/E3/CGXWgTQS3MWpwzxCPAkhtgp84MOqLtG0g45wOuKAOrXxb4ekultk1myeZ7j7MqLMCTLgHZx3wRT7bxNol5exWdvqdtJPNu8lVfiXb97Yej474ziuQ8JanpE1547kilt7xxqMk5igdXeSJYIxlcHJGdwB9Sa5TSNX0u4T4dXVrcW1taxXsiLYwPvSzDRSAI7tljIT6kZ7L3oA9I8OXt7N4t8V2VzeSXEFncW626uFHlq8CuQNoHdj71i+Nf+RlT/AK80/wDQ3q34U1Gxn+IXjWCG8t5JTc2xCJICxC26K3HsQQfQ8VyvxOVrLxRC1vNOhmtQz/vmIzvboCeB7DioqZfLMYvCwlZy6vy1/QqONjgX9ZkrqPT10/UKK4z7bd/8/U3/AH8NH227/wCfqb/v4a5f+IfYn/n9H7ma/wCu2H/59P70dnW94P8A+Q+n/XNq8u+23f8Az9Tf9/DUkOqahbyeZBf3UT4xuSZlP5g1vhuA8RRrRqOtHR32ZlX4yoVaUoKk9V3R9IUV87/8JJrv/Qa1H/wKf/Gj/hJNd/6DWo/+BT/419L/AGBU/nR4X9t0/wCRn0RRXzv/AMJJrv8A0GtR/wDAp/8AGj/hJNd/6DWo/wDgU/8AjR/YFT+dB/bdP+Rn0RRXK/Dy7ub3wok13cSzymZxvlcu2M+prqq8WvSdKpKm+jsevRqKrTU11CiiisjQKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKzfD3/It6X/16Rf+gitKs3w9/wAi3pf/AF6Rf+gigDSooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKRlV1KsAykYIIyCKWigCJraBoFgaCMwrjEZQbRjpx7VLRRQAVx3i7RPEGr69oVxpsOlPY6dcfaZRd3EiPIxVkKgLGwxtbOSeT2GOexooAZFDFBEsUMaRxqMKiKAB9AKEhiid3jiRGkOXKqAWPqfWn0UAFRLbwKPlhjGHMnCj7x6t9eetS0UAFeQfFj/kZbP/AK8x/wChtXr9cprHhnTvEfiaQagsjeRZxbNj7fvPJn+QrswFeNDERqT2V/yZy42jKtQlTju7fmeH0V7X/wAKz8Of887n/v8AGj/hWfhz/nnc/wDf419H/bmF8/u/4J4H9jYjy+//AIB4pRXtf/Cs/Dn/ADzuf+/xo/4Vn4c/553P/f40f25hfP7v+CH9jYjy+/8A4B4pRXtf/Cs/Dn/PO5/7/Gj/AIVn4c/553P/AH+NH9uYXz+7/gh/Y2I8vv8A+AeKUV7X/wAKz8Of887n/v8AGj/hWfhz/nnc/wDf40f25hfP7v8Agh/Y2I8vv/4AfDP/AJE6P/rvJ/Ouwqho+j2mh2AsrIOIQxYB2ycn3q/Xy+KqRq1pTjs2fR4am6dGMJbpBRRRWBsFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFZvh7/kW9L/69Iv8A0EVpVm+Hv+Rb0v8A69Iv/QRQBpUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVmQ/8AIz3n/XnB/wChy1p1xPibVL3TPEx+xzeX5lnHu+UHOHkx1HuawxOIjhqTqz2Xb7jahRlXqKnHdnbUV5t/wlOtf8/n/kJP8KP+Ep1r/n8/8hJ/hXkf6w4X+WX3L/M9H+xcR3X4/wCR6TRXm3/CU61/z+f+Qk/wo/4SnWv+fz/yEn+FH+sOF/ll9y/zD+xcR3X4/wCR6TRXm3/CU61/z+f+Qk/wo/4SnWv+fz/yEn+FH+sOF/ll9y/zD+xcR3X4/wCR6TRXm3/CU61/z+f+Qk/wo/4SnWv+fz/yEn+FH+sOF/ll9y/zD+xcR3X4/wCR6TRXm3/CU61/z+f+Qk/wo/4SnWv+fz/yEn+FH+sOF/ll9y/zD+xcR3X4/wCR6TRXm3/CU61/z+f+Qk/wo/4SnWv+fz/yEn+FH+sOF/ll9y/zD+xcR3X4/wCR6TRXm3/CU61/z+f+Qk/wo/4SnWv+fz/yEn+FH+sOF/ll9y/zD+xcR3X4/wCR6TRXm3/CU61/z+f+Qk/wo/4SnWv+fz/yEn+FH+sOF/ll9y/zD+xcR3X4/wCR6TRXm3/CU61/z+f+Qk/wo/4SnWv+fz/yEn+FH+sOF/ll9y/zD+xcR3X4/wCR6TRXm3/CU61/z+f+Qk/wo/4SnWv+fz/yEn+FH+sOF/ll9y/zD+xcR3X4/wCR6TRXm3/CU61/z+f+Qk/wo/4SnWv+fz/yEn+FH+sOF/ll9y/zD+xcR3X4/wCR6TRXm3/CU61/z+f+Qk/wo/4SnWv+fz/yEn+FH+sOF/ll9y/zD+xcR3X4/wCR6TRXm3/CU61/z+f+Qk/wo/4SnWv+fz/yEn+FH+sOF/ll9y/zD+xcR3X4/wCR6TRXm3/CU61/z+f+Qk/wo/4SnWv+fz/yEn+FH+sOF/ll9y/zD+xcR3X4/wCR6TRXP+FdSu9Strh7uXzGRwFO0DAx7Cugr18PXjiKSqw2fc82vRlRqOnLdBRRRW5kFFFFABRRRQAUUUUAFFFFABRRRQAVm+Hv+Rb0v/r0i/8AQRWlWb4e/wCRb0v/AK9Iv/QRQBpUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFUJNU8vXIdL+w3reZA032tYcwJg42M+eGPUDHSr9cbd6xqkfxYstES8A0640qW4MXlLkSK4UHdjP4UAdlWdqWtW2mXFraMrz3t3v+z2sRXzJdi7mI3EAADuSByB1Irzx/EHiaHwbqGvtre6XTdWktvIFrEEniW5ERD8ZB2ngqR+NXfEttNL8ZvCQW/uIt9peFdixny8KucZU9e+c+2KAO50jU11fTY7xbS8tNxZTBeReXIhBIOR+HUEg1ergL/XPEmp2+tzeH47trqxvHtrWFEgMMpj2hhIXO7k7uQVwNuO5MUmqeJtS+IY0KPU20qGbQE1BohBFK9vMZdhUEghsYx1I5OOxAB6JRXAX2t+JNSt9bk0FLtrqwu2trWNEg8mVowu4SFzu+YlvulcDGPUsXVvEmo/ENNDa/bS4ZvD6ahJCkMUj28xl2EBiCDjGOcjrx0IAPQqK800Dxpql74c8O2t5dJ/aup6lPYyXaRKPkhMhZwuNoYhFAGMZbOOMVs6pqOveGtP1OW6u472KS7toNNkKKJlErJG28AKh2sxK+oHJoA7KvPfGv/Iyp/15p/6G9ammXPilPGP2ee2upvD8truNxeCBZYZwfujyyMqRjqM574rlfiHf3Wn+KEMnkyrJarsCoVKqHfg8nJ568VzYzBVsbRlh6CvKW3TZ3/JG+HxdLCVVXrO0Vv8APQrUVz3/AAkUv/PBPzNH/CRS/wDPBPzNeH/qXnH/AD7X/gS/zPU/1ryv+d/c/wDI6Giue/4SKX/ngn5mj/hIpf8Angn5mj/UvOP+fa/8CX+Yf615X/O/uf8AkdDRXPf8JFL/AM8E/M0f8JFL/wA8E/M0f6l5x/z7X/gS/wAw/wBa8r/nf3P/ACOhornv+Eil/wCeCfmaP+Eil/54J+Zo/wBS84/59r/wJf5h/rXlf87+5/5HQ0Vz3/CRS/8APBPzNH/CRS/88E/M0f6l5x/z7X/gS/zD/WvK/wCd/c/8joaK57/hIpf+eCfmaP8AhIpf+eCfmaP9S84/59r/AMCX+Yf615X/ADv7n/kdDRXPf8JFL/zwT8zR/wAJFL/zwT8zR/qXnH/Ptf8AgS/zD/WvK/539z/yOhornv8AhIpf+eCfmaP+Eil/54J+Zo/1Lzj/AJ9r/wACX+Yf615X/O/uf+R0NFc9/wAJFL/zwT8zR/wkUv8AzwT8zR/qXnH/AD7X/gS/zD/WvK/539z/AMjoaK57/hIpf+eCfmaP+Eil/wCeCfmaP9S84/59r/wJf5h/rXlf87+5/wCR0NFc9/wkUv8AzwT8zR/wkUv/ADwT8zR/qXnH/Ptf+BL/ADD/AFryv+d/c/8AI6Giue/4SKX/AJ4J+Zo/4SKX/ngn5mj/AFLzj/n2v/Al/mH+teV/zv7n/kdDRXPf8JFL/wA8E/M0f8JFL/zwT8zR/qXnH/Ptf+BL/MP9a8r/AJ39z/yOhornv+Eil/54J+Zo/wCEil/54J+Zo/1Lzj/n2v8AwJf5h/rXlf8AO/uf+R6x4H/48rv/AK6D+VdVXiujfEG70aKWOOxhkEjBiWYjFaf/AAtq/wD+gZbf99tX1uX5BjqOGhTnHVea7nzmNzrB1a8pwlo/Jnq9FeUf8Lav/wDoGW3/AH21H/C2r/8A6Blt/wB9tXZ/Y2L/AJfxRy/2rhf5vwZ6vRXlH/C2r/8A6Blt/wB9tR/wtq//AOgZbf8AfbUf2Ni/5fxQf2rhf5vwZ6vRXlH/AAtq/wD+gZbf99tR/wALav8A/oGW3/fbUf2Ni/5fxQf2rhf5vwZ6vRXlH/C2r/8A6Blt/wB9tR/wtq//AOgZbf8AfbUf2Ni/5fxQf2rhf5vwZ6vRXlH/AAtq/wD+gZbf99tR/wALav8A/oGW3/fbUf2Ni/5fxQf2rhf5vwZ6vRXlH/C2r/8A6Blt/wB9tXq9cuJwdbDW9qrXOnD4uliL+zd7BWb4e/5FvS/+vSL/ANBFaVZvh7/kW9L/AOvSL/0EVynSaVFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABXNXnhWa58aweJo9TMUsFo1pHAYAy7GOSSc5Jzz26V0tFAHEN8P5n8KX+gPrbmK9vGu5Jvsy7wzSeYwHOMbgO3TIq7e+Ery+8VaV4gk1gLcadE8Uca2o2OHGHJ+bPPbB4rqqKAORfwXdwa7e3uk+I7zTrPUJfOvLOOGNw8mAGZGYEoSAM4z/LE1v4OFp4zi8QW9+Y0i09dNSzEI2CBW3Abs5znv6cV1FFAHIyeDLuDXr2/0jxHeaba6hIJryzjhjkV5MBS6MwJQkAZIz/KpIvBrWnixdestR8ny9NGmQ2xg3IkKncvO7JIbnPpx711VFAHCRfDWOPw1FpX9sXAuLW+OoWV8kSrJBMWLHjoykseD2Nalx4POq+Hr3Tdb1W5vp7sIGu1RYWjKNujKKowNrfNzkk9eMAdPRQBgaHoGo2EiS6t4hutXkiBWHzIkiVM8ZIQfM2OMk+uAMnPn/xY/wCRls/+vMf+htXr9eceO/DOo+IvE0Q09Y28izTfvfb953x/I135ZUjTxUJTdlr+TOLMYSnhpRirvT80eV0V2H/Cs/Ef/PK2/wC/wo/4Vn4j/wCeVt/3+FfW/X8L/wA/F958v9SxH8j+44+iuw/4Vn4j/wCeVt/3+FH/AArPxH/zytv+/wAKPr+F/wCfi+8PqWI/kf3HH0V2H/Cs/Ef/ADytv+/wo/4Vn4j/AOeVt/3+FH1/C/8APxfeH1LEfyP7jj6K7D/hWfiP/nlbf9/hR/wrPxH/AM8rb/v8KPr+F/5+L7w+pYj+R/ccfRXYf8Kz8R/88rb/AL/Cj/hWfiP/AJ5W3/f4UfX8L/z8X3h9SxH8j+44+iuw/wCFZ+I/+eVt/wB/hR/wrPxH/wA8rb/v8KPr+F/5+L7w+pYj+R/ccfRXYf8ACs/Ef/PK2/7/AAo/4Vn4j/55W3/f4UfX8L/z8X3h9SxH8j+44+iuw/4Vn4j/AOeVt/3+FH/Cs/Ef/PK2/wC/wo+v4X/n4vvD6liP5H9xx9Fdh/wrPxH/AM8rb/v8KP8AhWfiP/nlbf8Af4UfX8L/AM/F94fUsR/I/uOPorsP+FZ+I/8Anlbf9/hR/wAKz8R/88rb/v8ACj6/hf8An4vvD6liP5H9xx9Fdh/wrPxH/wA8rb/v8KP+FZ+I/wDnlbf9/hR9fwv/AD8X3h9SxH8j+44+iuw/4Vn4j/55W3/f4Uf8Kz8R/wDPK2/7/Cj6/hf+fi+8PqWI/kf3HH0V2H/Cs/Ef/PK2/wC/wo/4Vn4j/wCeVt/3+FH1/C/8/F94fUsR/I/uOPorsP8AhWfiP/nlbf8Af4Uf8Kz8R/8APK2/7/Cj6/hf+fi+8PqWI/kf3HH0V2H/AArPxH/zytv+/wAKP+FZ+I/+eVt/3+FH1/C/8/F94fUsR/I/uOPorsP+FZ+I/wDnlbf9/hR/wrPxH/zytv8Av8KPr+F/5+L7w+pYj+R/ccfRXYf8Kz8R/wDPK2/7/Cj/AIVn4j/55W3/AH+FH1/C/wDPxfeH1LEfyP7jj6K7D/hWfiP/AJ5W3/f4Uf8ACs/Ef/PK2/7/AAo+v4X/AJ+L7w+pYj+R/ccfRXYf8Kz8R/8APK2/7/Cj/hWfiP8A55W3/f4UfX8L/wA/F94fUsR/I/uOPorsP+FZ+I/+eVt/3+FH/Cs/Ef8Azytv+/wo+v4X/n4vvD6liP5H9xx9fTdeKf8ACs/Ef/PK2/7/AAr2uvBzuvSrez9nJO19vke3k9CpS5/aRavb9QrN8Pf8i3pf/XpF/wCgitKs3w9/yLel/wDXpF/6CK8E9o0qKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKxJdQsrDxPc/bLuC332cO3zZAu7Dy5xn61t1jDV9MTVpHBfe8qWD3GP3fmjLLHn1+cjOMZO3OeKAJ/wDhIdF/6C1j/wCBCf40f8JDov8A0FrH/wACE/xrSooAzf8AhIdF/wCgtY/+BCf40f8ACQ6L/wBBax/8CE/xrSooAzf+Eh0X/oLWP/gQn+NH/CQ6L/0FrH/wIT/GtKigDN/4SHRf+gtY/wDgQn+NH/CQ6L/0FrH/AMCE/wAa0qRmVELMQFUZJPYUAZreI9EUZbWLADIGTcp1P40v/CQ6L/0FrH/wIT/GsuXxZoV1ctYXrXNoUiN6hu7eSFZI4iHLqWAyFIBIODjtitTT9cs9RuPs0ZkiuDCtwsMyFHaJiQHCnnGRjB5HcDIoAP8AhIdF/wCgtY/+BCf40f8ACQ6L/wBBax/8CE/xrSooAzf+Eh0X/oLWP/gQn+NH/CQ6L/0FrH/wIT/GtKigDN/4SHRf+gtY/wDgQn+NH/CQ6L/0FrH/AMCE/wAa0qKAM3/hIdF/6C1j/wCBCf40f8JDov8A0FrH/wACE/xrSrP1fWrHQ7Rbi/m8tHdY0AGWZicAADr159BzQBGniTQpFLJrOnsASuVuUPIOCOvYginf8JDov/QWsf8AwIT/ABqnBqOj6G1xZQ+YsMd2TcSgFkimuHMmGPbLSA+gDDOARW9QBm/8JDov/QWsf/AhP8aP+Eh0X/oLWP8A4EJ/jWlRQBm/8JDov/QWsf8AwIT/ABo/4SHRf+gtY/8AgQn+NaVFAGb/AMJDov8A0FrH/wACE/xo/wCEh0X/AKC1j/4EJ/jWlRQBm/8ACQ6L/wBBax/8CE/xpP8AhI9E3Ff7YsNwGSPtKZx+ftVy7vLewtXubmURxJjLHnknAAA5JJIAA5JIArOm1OztL+F3t7n7fepsigC5d0jyxbGcKBv6kjkgdSBQBN/wkOi/9Bax/wDAhP8AGj/hIdF/6C1j/wCBCf41ZsL+11SwhvrKdZraZdySL0I/oexB5B4qzQBm/wDCQ6L/ANBax/8AAhP8aP8AhIdF/wCgtY/+BCf41pUUAZv/AAkOi/8AQWsf/AhP8aP+Eh0X/oLWP/gQn+NaVFAGb/wkOi/9Bax/8CE/xo/4SHRf+gtY/wDgQn+NaVFAGW/iTQo1DPrOnqCQuWuUHJOAOvckCnf8JDov/QWsf/AhP8azpNZ0PX4YYZGka0a48yGdgUjkktpA5wevytHnnAIU4yK0dP1yz1GaOKLzEeaAXMIkXb5sRIG9fbkZBwRkZAyKAD/hIdF/6C1j/wCBCf40f8JDov8A0FrH/wACE/xrSooAzf8AhIdF/wCgtY/+BCf40f8ACQ6L/wBBax/8CE/xrSooAzf+Eh0X/oLWP/gQn+NHh7/kW9L/AOvSL/0EVpUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFcA2kXptJNI+zz+c3iQX4l8ttnk/aRcbt+Nv3RtxnORjFd/RQAUUUUAFFFFABRRRQAUjsERnOSFGTtBJ/ADk0tFAHm/iCO38ZW+pCCw1Uam1hc2mnrc6bcQRoGXLFndAoLlFXr0x3JrYtLS51X4iW3iBbe4t7O30hrZhcRNG5lkkDbcEc7QvJ6ZIwTzjsKKACiiigAooooAKKKKACuA8d6f4ruUvZdPttNubQiGOFWeUzqPMRmwqqRywBJz91R6V2V5q1hp93Y2t3dJFPfSGK2RusjhSxA/Ad/YdSKu0AebzaJqcekeJtHmglkvNW1OO5ilijZowriEMd+MAIUfrg4UccgV6RRRQAUUUUAFFFI7rGjO7BUUZZmOAB6mgBaKyn8R6XEYhLOyeaquuYm+6zBVZsD5QWOAWxWh9pi+0NbqxaZUDlAOgJwMnoM4OPofSgDl/G2nXl9c+Hp4op5rGz1NZ7yOAtv27WCuAvzHaxU4HPftWRo9pquneL/7VvE1K50o/bLW0aaN5ZoY2MDruGC+0tHKASM4CZ613lne29/AZraQOoYo3GCrA4KkHkEHsasUAcx8P9Hu9D8HWtpfL5dy0s07xZz5fmSM4X6gMM++a6eiigAooooAKKKKACiiigDy+00PUbm1nhgsZrK4v11CG7tXhYQWxkD7ZYnPGWIjztJDbycDBroNJtbm71/Qbs2s9vHp+kSwTiWJkxLIYcIMj5seU3IyOnrXYUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAcB4htf8AhK7HU9SsLq236WwaweSIkpNFiTeG3DAZsKeCCq55Bp0/ie31O40y4uJNRt9IvbBwHtVmVorolco+wblYKTgHgnPXiu9ooA4iS9b+3LjT9SuNZhzDbNpzwhw0uBl87RsL7hhgwxgjoKxri61C0vZrMz6x50PiiF0Aedx9kfy8jPIaP73BJA544NeoUUAeXaPfTR+HNU1G5vtZnlGsSWqFLmVhHb/aRsYjnCberAbipIBHBEGla3qtvLBHqz6w2kJq1/DNOI5wyJkG23H/AFnl4Ld+u3Jr1iigDgdTOp6IdH1XTzqt/byRtp80V3M+7c5xBMyAgA78KSQGw4JwQa0/F1pLYfDPVLS3luJjFYsjySyNJI6Y+cliSSdu6tq40iO61aG+murpkhAKWu8CEOM4cgDJb5u5xwDjIBrQdFkRkdQyMMMpGQR6UAYWu6XYzgXTxNLcSKkKQo+BcbW3ojf7IYZJHQZzxmsma91bSPEMVnCrXHn3Ft5rNCSbgSbxK4I+6IwiYHQAc5LA111rbJZ2yW8bOY4xtQMclVHQZ749+amIyCM49xQBzeg+Z/wlfioLn7N9pgI9PM+zpv8A08uukqtZWNvp8LRW6bQ7tI5JyXdjlmJ7kmrNABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRWCmvzahrd5pukQQTrY7VurmWUhEkPPlqADuYDBPIxuHU8C5pl/fXN3e219YJavblNjRymRJlYZ3AlVPUEYx1B+tAGlRRRQAUUUUAFFc9rviK40jxH4e01LWOSHVZ5IXlaQho9qF+FxznHrW1e3lvp9lPeXcqw28CGSSRjgKoGSaAJ6KRWDoGHQjIpaACiqU2oxLPc2lu0ct7BAJzCX24ByFycHAJU9uxqp4U1p/EXhXTNYkhWF7yBZTGpyFz2zQBsUVl+INetfDulNe3ILsXWKCBCN88rHCRoD1Yn+p6Cqt3qet2EFvPNpVtKkk8UUy290S0Cu4Uvyg3Bc5PTpQBvUVg+HNeuNaudbgubSO3fTb82gEcpcOAiOGyQOu/pit6gAoornfEHiOfRte8PafHaxyxardNbvK0hDR4QtwuOenrQB0VFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRWPqWuT2V+LO10TUNSkEQlc2rQKEBJAz5kiddp6Z6UAbFFc7/wkeqf9CXrn/f6y/wDkij/hI9U/6EvXP+/1l/8AJFAHRUVzv/CR6p/0Jeuf9/rL/wCSKP8AhI9U/wChL1z/AL/WX/yRQB0VFc7/AMJHqn/Ql65/3+sv/kij/hI9U/6EvXP+/wBZf/JFAHRUVzv/AAkeqf8AQl65/wB/rL/5Io/4SPVP+hL1z/v9Zf8AyRQB0VFc7/wkeqf9CXrn/f6y/wDkij/hI9U/6EvXP+/1l/8AJFAHRUVzv/CR6p/0Jeuf9/rL/wCSKP8AhI9U/wChL1z/AL/WX/yRQBz3w0ik0PUPE/h7UcR3/wDast9FuODcQSgbZF9fukHHQ8GsjxNc3t3onxJsr/VZry102BBaLIsSbGaAOeUVSSGOOf5812F3qlxfqq3ngDVbgIcqJjYvg+2Z6YL+QWotR8PNS+zBtwh/0DYD648/GaAMLWNN0vQ/GXhi2jg8vRdUubia7ZpC0U915SrF5mTg5wxA7tz1rG1bT7aysNZMQjfR9O8R2M1nM5DLa7nhM6o38KKxIwOByO1d3/bF39l+y/8ACB6v9nxjyt9jsx6Y+0Ypx1y/MHkHwNrJh27fL8yx249MfaMYoA4nxBNpM6fEprKW0ZX0eGYGBlw7hZ8tkdTuxk+taF3GmgeKLuTQY1F5N4XnuFRTua4nR18tm7u3zEZOSc10UmqXM0Ril8A6q8ZABRmsSDjpx5/amJfSRypLH8PNSSSMYR1+wAr9D5/FAHDifw4+rfDS90y5s2laV1nlWRTIzGA5809S2/8Avc5J9a6zxgdau/h74nXWdM0yGNdMneL7NdvcEsEJ5DRJjHXIJrQj1a6ikMkfgLVkcsWLK1iDk9Tnz+tTN4h1NlKt4K1wqRggy2WD/wCTFAHLX2k+HdZ8baJYj7O9hdaNdpJDbS7El+eH5flI9WPHOR7VjeIodE03RfiXpRSygPyS2ts20En7JH86L1J3BuR3z713P9oS+bHL/wAK91PzIgBG/wDoGUA6YPn8YqV9ZvZJGkfwJrDOy7GZnsSSvoT9o6e1AGWG0n/hZmp3Ehst82jWskEjbcufMuAWU9zjHI7YrivD1va2Fl8K9QtmCXdy7QTzB/mdDEfkP+yCBgdAfcmvSZdZvJyTN4E1eQlShLvYn5T1HNx09qri627Nvw51AbDlcCw+U+o/f0AZ3xRtJwfDGsqjPZ6TrENzeY/5ZxZwZD7L3+uema7l7y2SKOUzIUlIEZU53k9NuOv4ViHxFqhBB8F64Qe3nWX/AMkVVtL+Wwdns/h7qduzfeMP2BCfriegDhNWtNPl0j4hao6xPeWWrrJbTFsmB1SD5kP8JzkEjrjB6Vs6sdIsde8e29z9jgF7pFvKkT7V89ttwCwH8Rzjpk5xW99pBVl/4VxqGGOSNthyfX/X+5/Opzq10dmfAWrHy08tMtY/KuMYH7/ge1AHM6XZ6VrXiPw5FcGK5gk8MEvF5mUlw8IwwBww68HjI9q46yvb9/B/gD7BNHPqEGp3sdqLiTIJXzRGpOc4xtH5V6pLqEs8oll+H2pySBdod/sBOOmM+f0qNbgKVK/Di/BU5BC2HB9f9fQBL8P38PT+HRcaBbpAZHY3qMoEyz5JdZeB8wYn29OMV1dctb6rc2bO1t4B1WBn++YmsVLfXE/NT/8ACR6p/wBCXrn/AH+sv/kigDoqK53/AISPVP8AoS9c/wC/1l/8kUf8JHqn/Ql65/3+sv8A5IoA6Kiud/4SPVP+hL1z/v8AWX/yRR/wkeqf9CXrn/f6y/8AkigDoqK53/hI9U/6EvXP+/1l/wDJFH/CR6p/0Jeuf9/rL/5IoA6Kiud/4SPVP+hL1z/v9Zf/ACRR/wAJHqn/AEJeuf8Af6y/+SKAOiornf8AhI9U/wChL1z/AL/WX/yRR/wkeqf9CXrn/f6y/wDkigDoqK53/hI9U/6EvXP+/wBZf/JFbOn3i6hptrfLFJEtxEsojkxuUMM4OCRnnsSKALNFQorOiuZXBYZwMYH6U7yj/wA9ZP0/woAkoqPyj/z1k/T/AAo8o/8APWT9P8KAJKKj8o/89ZP0/wAKPKP/AD1k/T/CgCSio/KP/PWT9P8ACjyj/wA9ZP0/woAkoqPyj/z1k/T/AAo8o/8APWT9P8KAJKKj8o/89ZP0/wAKPKP/AD1k/T/CgCSio/KP/PWT9P8ACjyj/wA9ZP0/woAkoqPyj/z1k/T/AAo8o/8APWT9P8KAJKKj8o/89ZP0/wAKWJiyHJyQSM+uDQA+iiigAooooAKKKKACiiigAooooAKKKKACsZru2tfE919ouIod1nBt8xwufnl6ZrZrz3xr/wAjKn/Xmn/ob1yY7EvDYeVVK9rfnY6cJQVesqbdr/5Hbf2tpv8A0ELT/v8AL/jSjVdOZgq39qSTgATLz+teU1Naf8fsH/XRf518/HiGq2lyI9h5LTSvzM9booor6s+eCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArN8P8A/ItaZ/16Rf8AoArSrN8P/wDItaZ/16Rf+gCgCt4k/wCRVn/3Y/8A0Ja85r1aeziv9N+zTgmN1XODg8YP9KzP+EP0n+5L/wB/K+fzbLK+LrKpTtZK2vqz2cux9LD0nCd73v8AkeeUV6H/AMIfpP8Acl/7+Uf8IfpP9yX/AL+V5f8AYGL7r7/+Ad/9sYfz+7/gmN4H/wCP27/65j+ddvWdpuiWelSO9qrhnGDubNaNfS5bhp4bDqlPdXPCx1eNes6kNjzHT9Q/4Rv4la1cXNz5Wj6jqAsSJHxHbzrbQzIRngb/ADJQfU7aqyzTLqnjLVr6E3ok8Px3gsrmRlSOJvPBjGBlSUjXP+1nmu4uPBulXsOpQXwmuodQuo7ueOVhjzE2BSMAY4jQY9B7nLb7wdZahd6pcTXl9u1O2FpcqsigGEbsIPl4+83I5+Y813nIU9T8Qarp2v2GlxxWAi1KP/Q5ZNwCupBdH+brsJK4+8RjjrWbqPi/WNKvPGU5S1uoNGFsIINrRkh0DElstk/NjoM4HTvu6j4M0/VrK6tb65vZkuPIyxlAZPKOU2ED5eeTjvn1NGo+CtM1RNRWeW7X+0o4kvDHLtM3ljCk8cHAAOMdKAKev+J9W8P6bdXVxZ2TPa2zXTpFKz+YocgKOAU+UAliNuTim293Z6T478T3Vy4hh+x2DOwUn5i1wM4H4Voax4L0nXp2m1A3UjyWZs5dlwyLLHzjeq4BILMRxwTn0q5p+gW2m6rc6jFPdPNcwxQOJZN42x52dRnI3Nznncc5oA5rxTfieS3vrSB9Vs2sZd1tCSJIQWUC4VeM4wR/eHJXPNS33iTUdJl0W1hns7231O3RLO9k+XfN8nL/ADAYZSzAjnIxgkjPSX2jW97dpd+bPBcLC0BkgfaWjYglTwe4BB6jnBGTVG+8H6ZqFhdWMxmFpPbxWwhUqFhjj5UR8fL9evTngYAMPxt4TstTMc8v2yfVL2WKzgKXssSQA8syojAfKokfnOSMZxVPU1SDx9rcCeH7vVo20m2fyrV4lMbF5gWBd1IYhV5XJ+X6V3i6fEJbSWR5JZLWNkR5GyTkAFj6tgdfc+tUZPDsTaxd6pHfXsNzdQpBIY2TGxSxUDKnGC7cjnnrQBX8C3b3vgjSZpb77dN5ASScggl1JVgdwB3AggkjJIJrdh+63++386r6VpdpoumQadYReVbQLtRdxY9ckknkkkkknqTViH7rf77fzoAkoqG7uoLGznu7qVYreCNpJZG6KoGST+ArmrfxvG+q6da3ek3tnb6nDJNaXUwG3ag3HzADmMlfmGe3XByAAdXRXIjx5b+RZ6k2n3C6FdziCLUyy7AS21XZM7lRm4De4JABp9542aHU9Z0200HUr280xIXMcQUCVZA5ypJwAAh64JPABoA6uiuGtfiZbXtlpOpwaLqJ0fUHjia/bYqQSO+wKRu3HDYBYDHPU9Kt3vjl4NS1fTrPw9ql9eaaImaOIIPMVwx3AlsYAXofmJPA4NAHXUVyVr8QNO1LSNEvNMgmuZ9Zdo7W1OEYMgJk3nou3ac9e2M5qOb4gW9v4Wm1ubSr5fs14bK6thsZoJBIEO4hsEZI5XPUcUAde8iRgF3VQSFBY4yTwB9adXEat4jtbqKyTWvC2pJE2tW9vbGcIAJC6+XMcNkDJ6c9CCOtaepeKpbS7vobLS5b5NPaJbtkmVTHvAbhTyQFIY9OOmcHAB0lFYGo+JZLfULqx07SrjU57OETXIhdFEeclUG48uQCQo7YyRkZzn+IumPpOi6nZ2V9d2uqzi3jaJFzFIc/I65zu+VhgA9OvSgDsK898a/8jKn/AF5p/wChvXR+HPE/9vXWpWU+mXWm32nuglt7lkZtrrlGBQkc4PftXJ+NNQsj4o2C7g3R2qK48wZU734Poa87NoSng5xgrvTb1R25dOMMTGUnZa/kzJqa0/4/YP8Arov86pfbbT/n6h/7+CpbW/s1u4Wa7gAEikkyDjmvjYYLE8y/dy+5n08sXQ5X76+9HsdFZn/CSaF/0GtO/wDApP8AGj/hJNC/6DWnf+BSf41+j+xqfyv7j4f2tP8AmX3mnRWZ/wAJJoX/AEGtO/8AApP8aP8AhJNC/wCg1p3/AIFJ/jR7Gp/K/uD2tP8AmX3mnRWZ/wAJJoX/AEGtO/8AApP8aP8AhJNC/wCg1p3/AIFJ/jR7Gp/K/uD2tP8AmX3mnRWZ/wAJJoX/AEGtO/8AApP8aP8AhJNC/wCg1p3/AIFJ/jR7Gp/K/uD2tP8AmX3mnRWZ/wAJJoX/AEGtO/8AApP8aP8AhJNC/wCg1p3/AIFJ/jR7Gp/K/uD2tP8AmX3mnRWZ/wAJJoX/AEGtO/8AApP8aP8AhJNC/wCg1p3/AIFJ/jR7Gp/K/uD2tP8AmX3mnRWZ/wAJJoX/AEGtO/8AApP8aP8AhJNC/wCg1p3/AIFJ/jR7Gp/K/uD2tP8AmX3mnRWZ/wAJJoX/AEGtO/8AApP8aP8AhJNC/wCg1p3/AIFJ/jR7Gp/K/uD2tP8AmX3mnRWZ/wAJJoX/AEGtO/8AApP8aP8AhJNC/wCg1p3/AIFJ/jR7Gp/K/uD2tP8AmX3mnRWZ/wAJJoX/AEGtO/8AApP8aP8AhJNC/wCg1p3/AIFJ/jR7Gp/K/uD2tP8AmX3mnRWZ/wAJJoX/AEGtO/8AApP8aP8AhJNC/wCg1p3/AIFJ/jR7Gp/K/uD2tP8AmX3mnRWZ/wAJJoX/AEGtO/8AApP8aP8AhJNC/wCg1p3/AIFJ/jR7Gp/K/uD2tP8AmX3mnRWZ/wAJJoX/AEGtO/8AApP8aP8AhJNC/wCg1p3/AIFJ/jR7Gp/K/uD2tP8AmX3mnRWZ/wAJJoX/AEGtO/8AApP8aP8AhJNC/wCg1p3/AIFJ/jR7Gp/K/uD2tP8AmX3mnRWZ/wAJJoX/AEGtO/8AApP8aP8AhJNC/wCg1p3/AIFJ/jR7Gp/K/uD2tP8AmX3mnRWZ/wAJJoX/AEGtO/8AApP8adHr+jTSpFFq9hJI7BVRblCWJ6ADPJo9jU/lf3B7Wn/MvvNGiiiszQKKKKACiiigAooooAKzfD//ACLWmf8AXpF/6AK0qzfD/wDyLWmf9ekX/oAoANT1aDQ9DfUblJHhhVNyxAFjkhRjJHc1y/8AwtfQv+fTUf8Av2n/AMXWh48/5J/ff7sP/oxK8Or38ry+hiaLnUve9vwR4mZY6th6qhT2tf8AFnsP/C19C/59NR/79p/8XR/wtfQv+fTUf+/af/F149RXpf2LhOz+88/+18T3X3HvXh3xjp/ia4mhsobqNoVDsZlUAgnHGGNdDXlHwl/5Cmo/9cF/9Cr1evnMxoQoYh04baHv4CtOvQU576lS51K0tJ0gllzO6llhjUu5UdW2qCce+MVTuPFGi2scEkt8uyeVoIyqM26Rc5TgH5hg8deD6VzPh+R9G8f+LTrbi3N9LDLZXMzBUmhVCNik8ZQ5yvX5s45zXOXOtXFzPo0+oahbRlfFssdtLJGiK8KRSoshxjfnIG7OOmMVwnYelxeJNIuIFntr1biMlxm3VpSpTG4MFBKkZGc4xmprDWbDVFjazmaRZYhNGxiZRJGcYZSQAw5HI9RWRZ6BY+FtL1y7e6LS38st3dXExVF3sMYA6KvAAHJ9zVPwldagfh14XfRrbT70jTLdJftF60IQiJBgFY3yc5yDjGKAOrnvbW2uba3mnRJrpykEbHmRgpYgD2VSfwqevMdYvfEK+M9DubnwzM7DVJEtmF5DtaMW1wAFGcglSXJPXbjsBXp1ABRRRQAUUUUAFRw/db/fb+dSVHD91v8Afb+dAGX4r0eTX/CWraTDII5bu1kiRj0DEcZ9s4rlNH1LxN4n0CTw7q/hu70q4aze2vb6Zl8rJQrmIA5YknPoOeTxn0OigDy220bVL74XweBr7T7iHUI3js5JljJg8lJA3nLJjBGxeB97dxgda2bSe8sviD4oupNH1JrW4tLZIJ0hysjRCTcBznneAOxwfau5ooA8a0yw1iy+C2j6FJoOpnUre9jaSBYc4VLoTFs5xjb09+K6PTtZNn8R/FMn9m380c1nZSAwwFmVgj4RlHKk5PJGBg5I4r0KsOx8NJYeJdQ1xNTvpJb8IJreTyvKwgIQDCBhjcf4ue+aAOCisPE3h/TNEtpNHu59P1C9urzV7XTmBlhaRt8cQbcPkBOGIODgjODg17ix1mLwP4m0yPwxfRzT60Li3ghRCpQyxyfLg4wFQjPTJAGecew0UAcT43mur7StDez0nULh11S0vJI0h+aOOOQO24E8HA6d6x/FWjy6lr13qumafqdh4ktTCunXtvG4hu0Kqds38OAxdW3YOAOvSvTqKAOItkvvDXjvXbiXT7u703WFiuY7i2jMhilSMRmJlHIyFBB6c4Jrnzo2qaVpOgo2j3klzL4hOs3cNsgkW0jdnOwkHBKhl6Z6HFer0UAcZon2uL4m+JZZdMvY7S8htUgumixGxiVw3Oc/xjBxzg+2eT+LH/Iy2f8A15j/ANDavX68g+LH/Iy2f/XmP/Q2r0so/wB8h8/yZwZp/uk/l+aODooor7U+QCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACtPw3/yNOkf9fsP/oYrMrT8N/8AI06R/wBfsP8A6GKzrfw5ejNKX8SPqj6Iooor88PugooooAKKKKACiiigArN8P/8AItaZ/wBekX/oArSrN8Pf8i3pf/XpF/6CKAL8P+oj/wB0fyp9RCHbwsjqPQHpS+Uf+esn6f4UASUVH5R/56yfp/hR5R/56yfp/hQBJRUflH/nrJ+n+FHlH/nrJ+n+FAElFR+Uf+esn6f4UeUf+esn6f4UASUVH5R/56yfp/hR5R/56yfp/hQBJRUflH/nrJ+n+FHlH/nrJ+n+FAElFR+Uf+esn6f4UeUf+esn6f4UASUVH5R/56yfp/hR5R/56yfp/hQBJUcP3W/32/nR5R/56yfp/hT0UIoUdBQAtFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFeS/FK2nuPEtp5MMku2zGdilsfO3pXrVZkP/ACM95/15wf8AoctdGExDw9ZVUr2/ysYYmh7ek6bdrngP9m33/Plc/wDfpv8ACj+zb7/nyuf+/Tf4V9I0V7P+sEv+ff4/8A8n+w4/z/h/wT5u/s2+/wCfK5/79N/hR/Zt9/z5XP8A36b/AAr6Roo/1gl/z7/H/gB/Ycf5/wAP+CfN39m33/Plc/8Afpv8KP7Nvv8Anyuf+/Tf4V9I0Uf6wS/59/j/AMAP7Dj/AD/h/wAE+bv7Nvv+fK5/79N/hR/Zt9/z5XP/AH6b/CvpGij/AFgl/wA+/wAf+AH9hx/n/D/gnzd/Zt9/z5XP/fpv8KP7Nvv+fK5/79N/hX0jRR/rBL/n3+P/AAA/sOP8/wCH/BPm7+zb7/nyuf8Av03+FH9m33/Plc/9+m/wr6Roo/1gl/z7/H/gB/Ycf5/w/wCCfN39m33/AD5XP/fpv8KP7Nvv+fK5/wC/Tf4V9I0Uf6wS/wCff4/8AP7Dj/P+H/BPm7+zb7/nyuf+/Tf4Uf2bff8APlc/9+m/wr6Roo/1gl/z7/H/AIAf2HH+f8P+CfN39m33/Plc/wDfpv8ACj+zb7/nyuf+/Tf4V9I0Uf6wS/59/j/wA/sOP8/4f8E+bv7Nvv8Anyuf+/Tf4Uf2bff8+Vz/AN+m/wAK+kaKP9YJf8+/x/4Af2HH+f8AD/gnzd/Zt9/z5XP/AH6b/Cj+zb7/AJ8rn/v03+FfSNFH+sEv+ff4/wDAD+w4/wA/4f8ABPm7+zb7/nyuf+/Tf4Uf2bff8+Vz/wB+m/wr6Roo/wBYJf8APv8AH/gB/Ycf5/w/4J83f2bff8+Vz/36b/Cj+zb7/nyuf+/Tf4V9I0Uf6wS/59/j/wAAP7Dj/P8Ah/wT5u/s2+/58rn/AL9N/hR/Zt9/z5XP/fpv8K+kaKP9YJf8+/x/4Af2HH+f8P8Agnzd/Zt9/wA+Vz/36b/Cj+zb7/nyuf8Av03+FfSNFH+sEv8An3+P/AD+w4/z/h/wT5u/s2+/58rn/v03+FH9m33/AD5XP/fpv8K+kaKP9YJf8+/x/wCAH9hx/n/D/gnzd/Zt9/z5XP8A36b/AArS8O6fep4m0pmtJ1VbyEkmMgAbx7V7/RUzz6UouPJv5/8AAKhksYyUufbyCiiivAPbCiiigAooooAKKKKACs3w9/yLel/9ekX/AKCK0qzfD3/It6X/ANekX/oIoA0qKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKzIf+RnvP+vOD/wBDlrTrl9U1yPRvE0pkheTzbOLG0gYw8v8AjWdWtCjB1KjskXTpyqyUIK7Z1FFcr/wnFt/z5y/99Cj/AITi2/585f8AvoVxf2tgv+fn5/5HV/Z2K/k/I6qiuV/4Ti2/585f++hR/wAJxbf8+cv/AH0KP7WwX/Pz8/8AIP7OxX8n5HVUVy8XjW3lmSMWkoLMFzuHeuorpw+Lo4hN0pXsYVsPVo29orXCiiiugxCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKzfD3/It6X/ANekX/oIrSrN8Pf8i3pf/XpF/wCgigDSooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKCQBk8CgAooooAKKKKACiiigAooooAK898a/8jKn/AF5p/wChvXX6drtnqmpalY2wmE2nSJHP5kZQbmXcMZ5PBHOMc8ZrkPGv/Iyp/wBeaf8Aob15ec/7jP5fmjvyz/e4fP8AJmBRRRXwh9cFFFFAE1p/x+wf9dF/nXrdeSWn/H7B/wBdF/nXrdfVcOfDU+X6nz+d/FD5/oFFFFfSnhBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFZvh7/kW9L/AOvSL/0EVpVm+Hv+Rb0v/r0i/wDQRQBpUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFcj8TpLiL4c6zJbXMlu6w4LR4yVJAK8joQe3NddWR4o0T/hJPDOoaP5/wBnN1EUEu3dsOcg44zyKAMa71q9i1640KK+nDW1pHPJdLYNcSM8jOEXbGu1QBGScjJyMYwTWTF4o8XtpHhmW9tLXTr2/wBSNhdQzWz/AN2RlkX5xgEIPlPPPUVoX3g3W59dtPEdl4ggtNbS2+y3OLMtbTx7iwHll9wIJ67j+FWdR8KarftoztrkJk0+9F9K8tkWM8oVlwMSAIu1yAOTwOTzkApW2o+KLjU/EWjDU7FZdNEUsV4bM5YSIWCFN+OCp5yeO3es2x8beINW0HwTqFu1hA+tztb3KtAzBWCyHcvz9Pkzt/8AHhXSReGtUt/EGu6pFq1pt1WONBE1ix8rYrKp3eaN33ueBn2rjLvQZ/CNp4A8PDV7We4ttVfyZnt9gKGOU/Mm855bGQR1H4gGne+O9S8LXniTTtZMF9Pp9il/ZTRx+SJUdvLCOMnGHIGR1BrX1bVtd8NX+hSXl1a3tnqN5HYXKrAYzDLJnY8Zyfk3DBDZPI5qa88EQazFrjaxMstxq1slozwJtEESZKhMk87mLEnqcccU+HwzqNxDpEGs6rDex6XKs6MlsUeeRFIRnJcjjOSAOSAcgcUAU9G1HxNq2payn26wWLS9UNv5a2hzPGIkbbkv8hy/3ufp2rOsfFmvRax4ag1GW0aXU55be+tIY9yWrhGZVSVSQWG0BgSTz2rc0rwrf2X/AAkC3WrQzR6xK8zeRaNC8LtGseVYyNkAKO3Xv2rH034farY2Phq2fXrZxoM5eDZYlRJGUZTu+c/NhuCMD1BoA0/DX/I9+Nv+vm0/9JkrmfH2py2XihRcWyhWtVEZjk3FgHfkggYPPTmu00fw/faZ4k1rVZtRt54tUkjkMCWpRoyiBF+bzDnhRnjr6dK4D4sf8jLZ/wDXmP8A0Nq3w2Co42qsPXV4y36ba/mjDE4urhKTr0XaS2+ehif8JFF/zwf8xR/wkUX/ADwf8xXPUV63+peT/wDPt/8AgT/zPK/1rzT+dfcv8jof+Eii/wCeD/mKP+Eii/54P+YrnqKP9S8n/wCfb/8AAn/mH+teafzr7l/kdJD4lhinjkNvIQrBsZHY12f/AAtqx/6Bdz/32teUUV2YXhrLsKmqUGr+b/zObEZ/j8Q06kk7eSPV/wDhbVj/ANAu5/77Wj/hbVj/ANAu5/77WvKKK6v7Gwn8v4s5/wC1cV/N+CPV/wDhbVj/ANAu5/77Wj/hbVj/ANAu5/77WvKKKP7Gwn8v4sP7VxX834I9X/4W1Y/9Au5/77Wj/hbVj/0C7n/vta8ooo/sbCfy/iw/tXFfzfgj1f8A4W1Y/wDQLuf++1o/4W1Y/wDQLuf++1ryiij+xsJ/L+LD+1cV/N+CPV/+FtWP/QLuf++1o/4W1Y/9Au5/77WvKKKP7Gwn8v4sP7VxX834I9X/AOFtWP8A0C7n/vtaP+FtWP8A0C7n/vta8ooo/sbCfy/iw/tXFfzfgj1f/hbVj/0C7n/vtaP+FtWP/QLuf++1ryiij+xsJ/L+LD+1cV/N+CPV/wDhbVj/ANAu5/77Wj/hbVj/ANAu5/77WvKKKP7Gwn8v4sP7VxX834I9X/4W1Y/9Au5/77Wj/hbVj/0C7n/vta8ooo/sbCfy/iw/tXFfzfgj1f8A4W1Y/wDQLuf++1o/4W1Y/wDQLuf++1ryiij+xsJ/L+LD+1cV/N+CPV/+FtWP/QLuf++1o/4W1Y/9Au5/77WvKKKP7Gwn8v4sP7VxX834I9X/AOFtWP8A0C7n/vtaP+FtWP8A0C7n/vta8ooo/sbCfy/iw/tXFfzfgj1f/hbVj/0C7n/vtaP+FtWP/QLuf++1ryiij+xsJ/L+LD+1cV/N+CPV/wDhbVj/ANAu5/77Wj/hbVj/ANAu5/77WvKKKP7Gwn8v4sP7VxX834I9o0L4iWmu6zb6bFYTRPNuw7OCBhS39K7OvC/h7/yPOnf9tf8A0U9e6V8/muGp4esoU1ZWv+LPcyzEVK9JyqPW/wCiCiiivMPRCiiigAooooAKzfD3/It6X/16Rf8AoIrSrN8Pf8i3pf8A16Rf+gigDSooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAqtcadY3cyy3NlbzSqMB5IlYgdepFWaKACiiigAooooAK8g+LH/ACMtn/15j/0Nq9frz3xr4UvvE/iZBZS20f2ezTf5zMM7nfGMA/3TXdltSFLFRnN2Sv8AkzjzCnKphpRgrvT80eTUV3X/AAqjXf8An707/v4//wARR/wqjXf+fvTv+/j/APxFfWf2lhP50fMfUMT/ACM4Wiu6/wCFUa7/AM/enf8Afx//AIij/hVGu/8AP3p3/fx//iKP7Swn86D6hif5GcLRXdf8Ko13/n707/v4/wD8RR/wqjXf+fvTv+/j/wDxFH9pYT+dB9QxP8jOForuv+FUa7/z96d/38f/AOIo/wCFUa7/AM/enf8Afx//AIij+0sJ/Og+oYn+RnC0V3X/AAqjXf8An707/v4//wARR/wqjXf+fvTv+/j/APxFH9pYT+dB9QxP8jOForuv+FUa7/z96d/38f8A+Io/4VRrv/P3p3/fx/8A4ij+0sJ/Og+oYn+RnC0V3X/CqNd/5+9O/wC/j/8AxFH/AAqjXf8An707/v4//wARR/aWE/nQfUMT/IzhaK7r/hVGu/8AP3p3/fx//iKP+FUa7/z96d/38f8A+Io/tLCfzoPqGJ/kZwtFd1/wqjXf+fvTv+/j/wDxFH/CqNd/5+9O/wC/j/8AxFH9pYT+dB9QxP8AIzhaK7r/AIVRrv8Az96d/wB/H/8AiKP+FUa7/wA/enf9/H/+Io/tLCfzoPqGJ/kZwtFd1/wqjXf+fvTv+/j/APxFH/CqNd/5+9O/7+P/APEUf2lhP50H1DE/yM4Wiu6/4VRrv/P3p3/fx/8A4ij/AIVRrv8Az96d/wB/H/8AiKP7Swn86D6hif5GcLRXdf8ACqNd/wCfvTv+/j//ABFH/CqNd/5+9O/7+P8A/EUf2lhP50H1DE/yM4Wiu6/4VRrv/P3p3/fx/wD4ij/hVGu/8/enf9/H/wDiKP7Swn86D6hif5GcLRXdf8Ko13/n707/AL+P/wDEUf8ACqNd/wCfvTv+/j//ABFH9pYT+dB9QxP8jOForuv+FUa7/wA/enf9/H/+Io/4VRrv/P3p3/fx/wD4ij+0sJ/Og+oYn+RnC0V3X/CqNd/5+9O/7+P/APEUf8Ko13/n707/AL+P/wDEUf2lhP50H1DE/wAjMz4e/wDI86d/21/9FPXulebeFvh9q2h+JLTUbm4snhh37lidyxyjKMZUdzXpNfN5xXp1q6lTd1b9WfQZVRnSouNRWd/0QUUUV5J6YUUUUAFFFFABWb4e/wCRb0v/AK9Iv/QRWlWb4e/5FvS/+vSL/wBBFAGlRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABWZD/AMjPef8AXnB/6HLWnWZD/wAjPef9ecH/AKHLQBp0UUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVm+Hv+Rb0v/r0i/8AQRWlWb4e/wCRb0v/AK9Iv/QRQBpUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVxniPWLvSfEzfZSg8yzi3blz0eTH867OvPfGv/Iyp/wBeaf8Aob152a1J08JOcHZq35o7cvhGeJjGSutfyYn/AAmGrf34v+/dH/CYat/fi/791g0V8b/aOL/5+P7z6f6lh/5F9xvf8Jhq39+L/v3R/wAJhq39+L/v3WDRR/aOL/5+P7w+pYf+Rfcb3/CYat/fi/790f8ACYat/fi/791g0Uf2ji/+fj+8PqWH/kX3G9/wmGrf34v+/dH/AAmGrf34v+/dYNFH9o4v/n4/vD6lh/5F9xvf8Jhq39+L/v3R/wAJhq39+L/v3WDRR/aOL/5+P7w+pYf+Rfcb3/CYat/fi/790f8ACYat/fi/791g0Uf2ji/+fj+8PqWH/kX3G9/wmGrf34v+/dH/AAmGrf34v+/dYNFH9o4v/n4/vD6lh/5F9xvf8Jhq39+L/v3R/wAJhq39+L/v3WDRR/aOL/5+P7w+pYf+Rfcb3/CYat/fi/790f8ACYat/fi/791g0Uf2ji/+fj+8PqWH/kX3G9/wmGrf34v+/dH/AAmGrf34v+/dYNFH9o4v/n4/vD6lh/5F9xvf8Jhq39+L/v3R/wAJhq39+L/v3WDRR/aOL/5+P7w+pYf+Rfcb3/CYat/fi/790f8ACYat/fi/791g0Uf2ji/+fj+8PqWH/kX3G9/wmGrf34v+/dH/AAmGrf34v+/dYNFH9o4v/n4/vD6lh/5F9xvf8Jhq39+L/v3R/wAJhq39+L/v3WDRR/aOL/5+P7w+pYf+Rfcb3/CYat/fi/790f8ACYat/fi/791g0Uf2ji/+fj+8PqWH/kX3G9/wmGrf34v+/dH/AAmGrf34v+/dYNFH9o4v/n4/vD6lh/5F9xvf8Jhq39+L/v3R/wAJhq39+L/v3WDRR/aOL/5+P7w+pYf+RfcemeHr+fUtKW4uCpkLsPlGOlatYPg//kAL/wBdGrer7nAzlPDQlJ3bSPk8XFRrzjFaXCiiiuo5wooooAKKKKACs3w9/wAi3pf/AF6Rf+gitKs3w9/yLel/9ekX/oIoA0qKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACs2S51ZfEUFtHYRNpLW7PLdmYB0lBGECdwRzn/J0q8/vhMPjdY24u7oW9xokzNCJm2KwcDcq5wDjuKAPQKw9Y18WOsaZotsqNqOpCVovMzsRY13MzY5PJUAe/tXl8lhLb+AtR10arqsmoaTrcqWryXsjAKt4Ewwzh8rwS2TXR+JtMsrn4zeE/Oto3860vDJkfeKqu3P0oA7jQ7rU7vS0k1iwSxvQ7o8McvmKcMQGVvRgARnkZrRrzSa11Xxf/AMJA9pLDb31nqL21rdG9kRrTyiu392qkEHljk/MG54AxTm0t/EHxTOnXurXhtrrwzFdS/Ybx1jMvnAbojn5VO0EY6985OQD1eivNJbPVPFv/AAkAtJooL2yv2tbW6a+kR7TytpU+WFIOeWOT8wbB4AxW+wSa58Uhp9/qtzLbXXheK5nFldukLyGYKTGQcqp2g/KRnvnJyAeqUV4/4d1u8/4RPwpot5d3DwX2r3NhPdvKRI0cTSFIy3XLFVX6Aiuj1+F/B+ias9hqMghvby1VYJJSq2KSyJE5V+SoPzEHHynJAoA72vPfGv8AyMqf9eaf+hvWjo+g6rpPi03i3Nta6Tc24jk04XUk5eZckSIXUYO3ggdcZPNcr8R7m607xREyXLSCW1U7ZFXCAO/AwBxz3yawxOAq4+k8NRtzS2vtpr+hrRxlPBTWIq35Y9vPT9SGiuX/ALdvf7yf980f27e/3k/75ry/9RM07w+9/wCR3f645d2l9y/zOoorl/7dvf7yf980f27e/wB5P++aP9RM07w+9/5B/rjl3aX3L/M6iiuX/t29/vJ/3zR/bt7/AHk/75o/1EzTvD73/kH+uOXdpfcv8zqKK5f+3b3+8n/fNH9u3v8AeT/vmj/UTNO8Pvf+Qf645d2l9y/zOoorl/7dvf7yf980f27e/wB5P++aP9RM07w+9/5B/rjl3aX3L/M6iiuX/t29/vJ/3zR/bt7/AHk/75o/1EzTvD73/kH+uOXdpfcv8zqKK5f+3b3+8n/fNH9u3v8AeT/vmj/UTNO8Pvf+Qf645d2l9y/zOoorl/7dvf7yf980f27e/wB5P++aP9RM07w+9/5B/rjl3aX3L/M6iiuX/t29/vJ/3zR/bt7/AHk/75o/1EzTvD73/kH+uOXdpfcv8zqKK5f+3b3+8n/fNH9u3v8AeT/vmj/UTNO8Pvf+Qf645d2l9y/zOoorl/7dvf7yf980f27e/wB5P++aP9RM07w+9/5B/rjl3aX3L/M6iiuX/t29/vJ/3zR/bt7/AHk/75o/1EzTvD73/kH+uOXdpfcv8zqKK5f+3b3+8n/fNH9u3v8AeT/vmj/UTNO8Pvf+Qf645d2l9y/zOoorl/7dvf7yf980f27e/wB5P++aP9RM07w+9/5B/rjl3aX3L/M6iiuX/t29/vJ/3zR/bt7/AHk/75o/1EzTvD73/kH+uOXdpfcv8zqKK5f+3b3+8n/fNH9u3v8AeT/vmj/UTNO8Pvf+Qf645d2l9y/zOoorl/7dvf7yf980f27e/wB5P++aP9RM07w+9/5B/rjl3aX3L/M9r8H/APIAX/ro1b1eF2Hj3XNNtRb28kAjBJ+aIE81Z/4WZ4j/AOett/35FfV4Th7F0qEKcmrpJb/8A+dxOeYWpVlON7N9v+Ce10V4p/wszxH/AM9bb/vyKP8AhZniP/nrbf8AfkV0f2HivL7/APgGP9s4fz+7/gntdFeKf8LM8R/89bb/AL8ij/hZniP/AJ623/fkUf2HivL7/wDgB/bOH8/u/wCCe10V4p/wszxH/wA9bb/vyK9rrjxeBq4W3tLa9vI6sLjKeJv7O+gVm+Hv+Rb0v/r0i/8AQRWlWb4e/wCRb0v/AK9Iv/QRXGdZpUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFc/eeELG88UR+Imur6PUIoDbxmObCKh6gLjHU5+tdBRQByH/CuNI/4R250I3epmwubj7TKpuiWZ924/NjPLYY+4qzceCLG712w1qa/1Nr+wj8uCT7R91SMNkYwc9/WumooA5LVPhzoOq+Im1uU3sVxLtFzFb3LRxXIXgCRR97jj3FXv+EQsB4vHicT3g1AQ/ZwBN+78rrs24+7nn681v0UAclqvw50HV/ETa3Mb2G4lCi5jt7lo47kLwBIo+8Mce4q3J4OsH8TyeIUur6LUHtjaBo5sKsX90LjAAPzfWuiooA5BPhvoK+Fp/DrG8lsZZvtCmS4JkilznejdQc8/ifU1ds/BWkWvhy80OUXN7bXgIuZLydpZZcjHLnngAYxjGOK6KigDm/C/gfSfCZdrF7ueRl8tZLycytFH/cTP3VzjgdcDPQVwnxY/wCRls/+vMf+htXr9eQfFj/kZbP/AK8x/wChtXpZR/vkPn+TODNP90n8vzRwdFFFfanyAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFfTdfMlfTdfOcQf8u/n+h7+R/8vPl+oVm+Hv8AkW9L/wCvSL/0EVpVm+Hv+Rb0v/r0i/8AQRXzZ75pUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVyGu+FLHxP4mcXstzH9ns4tnksozueTOcg/3RXX1iTPfR+Jrk2drDMDZw7jLMY8fPL0wrZ/Srp1J0pKcHZoipTjUjyzV0YH/AAqjQv8An71H/v4n/wARR/wqjQv+fvUf+/if/EV0/wBo1r/oG2X/AIHN/wDGqPtGtf8AQNsv/A5v/jVdX9pYv+dnN9Qw38iOY/4VRoX/AD96j/38T/4ij/hVGhf8/eo/9/E/+Irp/tGtf9A2y/8AA5v/AI1R9o1r/oG2X/gc3/xqj+0sX/Ow+oYb+RHMf8Ko0L/n71H/AL+J/wDEUf8ACqNC/wCfvUf+/if/ABFdP9o1r/oG2X/gc3/xqj7RrX/QNsv/AAOb/wCNUf2li/52H1DDfyI5j/hVGhf8/eo/9/E/+Io/4VRoX/P3qP8A38T/AOIrp/tGtf8AQNsv/A5v/jVQ3eo6xZ2c91JpdoyQxtIwS9YkgDPH7vrR/aWL/nYfUMN/Ijnv+FUaF/z96j/38T/4ij/hVGhf8/eo/wDfxP8A4iun+0a1/wBA2y/8Dm/+NUfaNa/6Btl/4HN/8ao/tLF/zsPqGG/kRzH/AAqjQv8An71H/v4n/wARR/wqjQv+fvUf+/if/EV0/wBo1r/oG2X/AIHN/wDGqPtGtf8AQNsv/A5v/jVH9pYv+dh9Qw38iOY/4VRoX/P3qP8A38T/AOIo/wCFUaF/z96j/wB/E/8AiK6f7RrX/QNsv/A5v/jVH2jWv+gbZf8Agc3/AMao/tLF/wA7D6hhv5Ecx/wqjQv+fvUf+/if/EUf8Ko0L/n71H/v4n/xFdP9o1r/AKBtl/4HN/8AGqPtGtf9A2y/8Dm/+NUf2li/52H1DDfyI5j/AIVRoX/P3qP/AH8T/wCIo/4VRoX/AD96j/38T/4iuhg1HWLiS4RdLtAYJPLYtetgnarZH7vphh+tTfaNa/6Btl/4HN/8ao/tLF/zsPqGG/kRzH/CqNC/5+9R/wC/if8AxFH/AAqjQv8An71H/v4n/wARXT/aNa/6Btl/4HN/8ao+0a1/0DbL/wADm/8AjVH9pYv+dh9Qw38iOY/4VRoX/P3qP/fxP/iKP+FUaF/z96j/AN/E/wDiK6f7RrX/AEDbL/wOb/41R9o1r/oG2X/gc3/xqj+0sX/Ow+oYb+RHMf8ACqNC/wCfvUf+/if/ABFH/CqNC/5+9R/7+J/8RXT/AGjWv+gbZf8Agc3/AMao+0a1/wBA2y/8Dm/+NUf2li/52H1DDfyI5j/hVGhf8/eo/wDfxP8A4ij/AIVRoX/P3qP/AH8T/wCIrcvNZ1WyurC3k0q2Zr2cwRlLxiFYRvJlv3fAxGR9SKt/aNa/6Btl/wCBzf8Axqj+0sX/ADsPqGG/kRzH/CqNC/5+9R/7+J/8RR/wqjQv+fvUf+/if/EV0/2jWv8AoG2X/gc3/wAao+0a1/0DbL/wOb/41R/aWL/nYfUMN/IjmP8AhVGhf8/eo/8AfxP/AIij/hVGhf8AP3qP/fxP/iK6f7RrX/QNsv8AwOb/AONUfaNa/wCgbZf+Bzf/ABqj+0sX/Ow+oYb+RHMf8Ko0L/n71H/v4n/xFH/CqNC/5+9R/wC/if8AxFdP9o1r/oG2X/gc3/xqj7RrX/QNsv8AwOb/AONUf2li/wCdh9Qw38iOY/4VRoX/AD96j/38T/4ij/hVGhf8/eo/9/E/+Irpzc60Bn+zbL/wOb/41UVpqGsXlnBcppdoqTRrIoe9YEAjPP7vrR/aWL/nYfUMN/Ijnf8AhVGhf8/eo/8AfxP/AIij/hVGhf8AP3qP/fxP/iK6f7RrX/QNsv8AwOb/AONUfaNa/wCgbZf+Bzf/ABqj+0sX/Ow+oYb+RHMf8Ko0L/n71H/v4n/xFH/CqNC/5+9R/wC/if8AxFdP9o1r/oG2X/gc3/xqj7RrX/QNsv8AwOb/AONUf2li/wCdh9Qw38iOY/4VRoX/AD96j/38T/4ij/hVGhf8/eo/9/E/+Irp/tGtf9A2y/8AA5v/AI1R9o1r/oG2X/gc3/xqj+0sX/Ow+oYb+RHMf8Ko0L/n71H/AL+J/wDEV3VZn2jWv+gbZf8Agc3/AMaqFdR1hr2S1Gl2m+ONJC321sEMWAx+76/KfzFY1sTVr29pK9jalh6VG/s42ubNZvh7/kW9L/69Iv8A0EUn2jWv+gbZf+Bzf/Gqm0i2lstGsbWfZ50Nukb7DldwUA4OBkfhWBsXaKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAK45vE92IpNTyv2RNbGleRtH3POEBfPXd5hz6bRjGea7GudPhRDK0f2ofYDqQ1IweV83mhg+N2fu+YN3TPbNAHRUUUUAFFFFABRRRQAVxWsa5rnh3VLee6uLO7s5YLme4s4oSr26RRM4dXz8wyFQkgcuCMdK7WuaXQdannu01HVdOuLK8DJcRx6c6StGVICBzMwAGf7vc9Cc0AZ2j+K9SfU/C9vqKxMPEGnyXYCLt+zyIqPsHqu18c85Gc4OB21cxpHg2PTrzSLie9e6Oj2TWVkDGFKo20FnOTubaijIwOvHPHT0AFFFFABRRRQAVzfjTXrzRPD9/NpkSSXsNpLcbn5SFVUncw75IwB3OewNdJXLeKvAWkeKoLxrhZIr6e2MC3KzSYTg7SUDhWwSTg9aAIdY8SXtt/wAJNc2xURaBbpKYioPnt5fmuCew2bQMYwSSc9K62N1ljWRfusAw+hrmZ/BVsbbULK0uDBYajbx21zCytIxRVKHa5bIJQ7STu6A11AAAAAwB2oAKKKKACiiigAqlrGpw6Lot7qdwGMVpC8zKvVtozge56CrtUdZ0uHW9FvNMuGdYrqJomZPvLkdR7jrQBzN74uksLSGCadP7aurm1t1tTbvGsHnvtDYbBcKA/IOCVxhelbWj6lPNrGr6VcuJXsGiZJtoBdJEyNwHGQQwyMcY4qnfeEf7Vn+2ahdxyX6LAIJooNixNFJ5qttLEnLYyMjjjjJNammaSbG8v72aZZrq+kVpGVNigKoVVAyTgAE8k8k/SgDSooooAKKKKACiiigDkda8QXljrk1iJkt5WEH9npIg8u7d2IZWc9CMdAQcc/N0FKHxlf3NhJrkaxLYR60NNa3ZckxGZYPMDdd29t2Om3jGfmra1Tww2pPqiG8VbbUkjWZHh3sm0YzG24bT0IyDhufaq6+CoYybeK7ZNNbVBqjW3l5Yy7g+3dn7nmANjGe2cUAdTRRRQAUUUUAFFFFAGE2rXMHiu8spnjNlDpy3ahUIYHe4OTnnhfQViweL7u10/RdTvwrw6rp0t60KgDyCsQmCqe427gc55AIx0rfGjTnxRPq0l1C8EtmLX7N5BBwGLZL7sH7x421RtPB8MMOn2t1c/abTTrOSzto/L2ny3UJ87ZO4hF25AHUnvwAN0vW77+0dEtr50k/tbT5Lv5V2iGRPLJVfVSJe+T8vU546isLTPDjWV3p9xcXn2ltPsmsrbEWwhGKbmbk5Y+WnIwOvHPG7QAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQBzviLxOdB1HSYPsbTW91cLFdzg8WqP8qM3sXIHPoa173U7PTlBu51jyrPjBJ2r95sDnAyMnoM81zWo+EZvENlrB1ldl3dq0UCWmpzrEIwuIwwAUEhiWOVbknqOKSLTPFiXWnanIukTXyWZs72FriQRyDIIkR/LyDkHKlccjnigDoZ9c0y2CNLexBHVGDg5UK5whLDhQx4BOAe1Qr4l0dtuL1Ruu/sXKMMT8fuzkcNyODWcmka1Z65dT2v8AZ09nfxwibzmZGgdF2kogVg6kYIUsuD3NZd14R1k3twLeSwNo+uw6urySOHIXZujKhSB9zg5Oc9BQB0cHivQbkyiDVbWXyjtfy33YO8JjjqdxC4HJJA71NH4g0ma2S4ivonR5WhULku0i53IF+9uGDkYyMGuY03w14h07w1qFlG2mrd3GrSXoAmcq0Ty7ym/YCj44DBTg8jB6UtM8F+IdFvI9Qs5dMeaHUb24FvJLII5IbgqxUvsJVlKjBw2efWgDs7XxBpV9cRwWl4lw8sH2mPyQXDRZxuBAwRnjr14pbjXLGDw9NrnmM1jHA1xv2kFkAzwDg89vXIrmPEtibi50WCDVba216NzGbe3HL203yygJnIVQNwY8boh3NbvizSpNT8G6lp1mg81rciCMcAsvKr7AkAUAUtS8S32l3MEE9tB5rRxSFBnMheUIY4+eWUHJPfI4GeN23vTeXVwlvsa3h+Qyg5DSZ+ZR/u8A+5I4INMkmk1LTo20+VVWfAaUkho1PUgY++OmDjB69MHAv/ClzJr9nd2klvFa272xQlmDwpF5m5FGMEOHAOSO/XAoA3NI1Rr9722njWK7sZ/ImRWyDlQyuPZlYHHY5HOM1pVzvh6BpNb8QasAwgvLiNICwI3pHGqlx7FiwB7hQRwa6KgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAGeVH5pl8tfMK7d+OcdcZ9KfRRQAYoIyMHpRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQB/9k='),\n",
       " Document(metadata={'doc_id': '807f52fd-d7b6-4c38-8a83-df74d6a898bb', 'source': '2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries.pdf', 'type': 'table', 'paper_id': 'a5cdaa51-39b4-42fe-bc76-e19fb729c37b'}, page_content='<table><tr><td>Models</td><td>Accuracy</td></tr><tr><td>Retrieved Chunk</td><td>~Ground-truth Chunk</td></tr><tr><td>GPT-4</td><td>0.56</td><td>0.89</td></tr><tr><td>ChatGPT</td><td>0.44</td><td>0.57</td></tr><tr><td>Llama-2-70b-chat-hf</td><td>0.28</td><td>0.32</td></tr><tr><td>Mixtral-8x7B-Instruct</td><td>0.32</td><td>0.36</td></tr><tr><td>Claude-2.1</td><td>0.52</td><td>0.56</td></tr><tr><td>Google-PaLM</td><td>0.47</td><td>0.74</td></tr></table>'),\n",
       " Document(metadata={'doc_id': '31c6618c-04ed-4dfc-82d7-c28254e9a731', 'source': '2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries.pdf', 'type': 'text', 'paper_id': 'a5cdaa51-39b4-42fe-bc76-e19fb729c37b'}, page_content='Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing a multi- hop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th Inter- national Conference on Computational Linguistics, pages 6609–6625, Barcelona, Spain (Online). Inter- national Committee on Computational Linguistics.\\n\\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gi- anna Lengyel, Guillaume Bour, Guillaume Lam- ple, Lélio Renard Lavaud, Lucile Saulnier, Marie- Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2024. Mix- tral of experts.\\n\\nYichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and Mohit Bansal. 2020. HoVer: A dataset for many-hop fact extraction and claim verification. In Findings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).\\n\\nEhsan Kamalloo, Xinyu Zhang, Odunayo Ogundepo, Nandan Thakur, David Alfonso-Hermelo, Mehdi Rezagholizadeh, and Jimmy Lin. 2023. Evaluat- ing embedding apis for information retrieval. arXiv preprint arXiv:2305.06300.\\n\\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences. In Proc. of the Annual Conference of the North American Chap- ter of the Association for Computational Linguistics (NAACL).\\n\\nJerry Liu. 2022. LlamaIndex.\\n\\nYi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023. Recall: A benchmark for llms robustness against external counterfactual knowledge.\\n\\nOpenAI. 2023. GPT4 (Nov 7 version). https://chat. openai.com/chat. gpt-4-1106-preview. Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. 2023. Ares: An automated evalua- tion framework for retrieval-augmented generation systems.\\n\\nHongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. 2023. One embedder, any task: Instruction-finetuned text em- beddings.\\n\\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. Fever: a large-scale dataset for fact extraction and verification.\\n\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al- bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, An- thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di- ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar- tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly- bog, Yixin Nie, Andrew Poulton, Jeremy Reizen- stein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subrama- nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay- lor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Ro- driguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine- tuned chat models.\\n\\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or fiction: Verifying scientific claims. In Proceedings of the 2020 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 7534–7550, Online. As- sociation for Computational Linguistics.\\n\\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weakly- supervised contrastive pre-training. arXiv preprint arXiv:2212.03533.\\n\\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-pack: Packaged resources to advance general chinese embedding.\\n\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben- gio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answer- ing. In Conference on Empirical Methods in Natural Language Processing (EMNLP).\\n\\nPeitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. 2023. Retrieve anything to aug- ment large language models.\\n\\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\\n\\nJiawei Han. 2022. Towards a unified multi- dimensional evaluator for text generation. A Appendix A: GPT-4 Prompts Used for Data Generation\\n\\nWe present the prompts used for guiding GPT-4 for data generation. Table 7 shows the prompt used for claim generation, along with the corresponding top- ics and entities within these claims. Table 8, Table 9, and Table 10 respectively show the prompts used for generating multi-hop queries of the inference, comparison, and temporal types.\\n\\nB Appendix B: Dataset Examples\\n\\nIn this appendix, we present an example of each type of multi-hop query included in the MultiHop- RAG dataset. These examples are illustrated in the respective tables: Table 12 for Inference Queries, Table 13 for Comparison Queries, Table 14 for Temporal Queries, and Table 15 for Null Queries. Each query is paired with a ground-truth answer for the evaluation of generation accuracy, while multiple pieces of supporting evidence are included for assessing retrieval performance. Additionally, metadata such as the title, source, and publication time of the news articles are provided as references.\\n\\nA \"claim\" is a statement or assertion made within a text expressing a belief, opinion, or fact. Given evidence from the original context, please extract one claim and its associated topics.\\n\\nNote: The claim should not contain ambiguous references, such as ’he’,’ she,’ and’ it’, and should use complete names. If there are multiple topics, give the most dominant one. The target of the claim (one entity)is the specific individual, group, or organization that the statement or assertion within a text is directed towards or about which it is making a case. The topic of the claim should be a simple phrase representing the claim’s central argument concept. If there is no claim, please leave it blank. Please generate a claim based on the given evidence. Don’t generate the evidence yourself.\\n\\nPlease give the response following this format: Evidence: [original context] Claims: [extract claim] Claim Target: [target] Claim Topic: [topic]\\n\\nHere are examples: <examples> Now, it’s your turn. <News> <evidence>\\n\\nTable 7: Claim Generation Prompting\\n\\nA multi-hop question is a query requiring multiple inferential leaps or accessing several pieces of information from different locations or sources to arrive at an answer. The following are news articles’ metadata and claims come from the articles. All the claims from the article are related to a similar target. Your task is to generate one multi-hop inference question based on the claims. Here are some instructions:\\n\\n1. Find the Connection: The connection between claims is <target>, which is how these key pieces of information are related or how they can be combined to form a more complex idea.\\n\\n2. Formulate the Question: Create a question that cannot be answered by relying on just one of the sentences but instead requires understanding and linking the information from all of the sources. The answer is <target>.\\n\\n3. Ensure Coherence: Make sure the question flows logically from the combined information and is clear and unambiguous.\\n\\n4. Use the keywords: <key set>\\n\\n<examples> Context:\\n\\n<Context>\\n\\nTable 8: Inference Query Generation Prompting\\n\\n<Context>\\n\\nThe above are news articles’ metadata and claims come from the articles. All the claims from the articles are related to a similar target. Your task is to generate one comparison question based on all the claims from different sources. This question needs to compare some factual elements of the claims that are explicitly stated to find where they agree or differ. The correct answer to this question is expressed as a comparative adjective, a statement of alignment, a simple yes or no. To generate a comparative question from claims, you need to use the following keywords: <key set>\\n\\nThe Good Comparison Questions: <examples> Your Comparison Question:\\n\\nTable 9: Comparison Query Generation Prompting\\n\\n<Context>\\n\\nPlease create a time-sensitive comparison question using metadata and excerpts from multiple news articles. That is to compare the consistency or sequence of reports on similar topics at multiple different time points. If it is to compare the consistency, please clearly mention the news source and time in the question using <time frame>. If it is to compare sequences of reports, just clearly mention the news source and do not mention the timeline. Utilize the following keywords provided in the <key set> to construct the question. The correct answer should based on the factual excerpts and is only one word.\\n\\n<examples>\\n\\nYour time-sensitive comparison question:\\n\\nTable 10: Temporal Query Generation Prompting\\n\\nA multi-hop question is a query requiring multiple inferential leaps or accessing several pieces of information from different locations or sources to arrive at an answer. Considering you have read at least two news articles on <entity>, construct a multi-hop question that incorporates all the news sources. The source of the news should be stated in the question. Also, ensure that the answer to the question is a single word/entity. Do not answer this question directly. Just give me the question:\\n\\nTable 11: Null Query Generation Prompting\\n\\nQuery: Which platform is at the center of discussions in articles from Music Business Worldwide, Polygon, and FOX News - Health, concerning the policing of AI-driven voice replication, the debate over \"reaction\" content, and being the most used app overnight by young people? Answer: YouTube\\n\\nEvidence List:\\n\\nTitle: Sony Music’s artists aren’t involved in YouTube’s new voice-cloning AI experiment.\\n\\nSource: Music Business Worldwide\\n\\nPublished Time: 2023-11-23T18:48:48+00:00\\n\\nFact: During this period of discussion, YouTube has made a number of positive announcements regarding the biggest issue for any rightsholder regarding AI-driven voice replication of artists: their ability to police it.\\n\\nTitle: YouTube demonetizes popular content creator SSSniperwolf after doxxing accusations\\n\\nSource: Polygon\\n\\nPublished Time: 2023-10-25T18:18:06+00:00\\n\\nFact: The debate over \"reaction\" content on YouTube has been brewing for years, but a recent incident between two creators has refueled the urgency of the conversation.\\n\\nTitle: Cell phone shocker as 97% of kids use their device during school hours and beyond, says study Source: FOX News - Health\\n\\nPublished Time: 2023-10-01T09:05:26+00:00\\n\\nFact: Overnight phone use was primarily spent engaging with the same media, although YouTube appeared to be the longest-running app because videos were often left playing during the night.\\n\\nTable 12: The example of inference questions\\n\\nQuery: Did the Cnbc | World Business News Leader report on Nike’s net income and the article from\\n\\nThe Age on the 10-year Treasury yield both report a decrease in their respective financial metrics? Answer: Yes\\n\\nEvidence List:\\n\\nTitle: Nike misses revenue expectations for the first time in two years, beats on earnings and gross\\n\\nmargin\\n\\nSource: Cnbc | World Business News Leader\\n\\nPublished Time: 2023-09-28T20:31:00+00:00\\n\\nFact: The company’s reported net income for the three-month period that ended August 31 was $1.45 billion, or 94 cents per share, compared with $1.47 billion, or 93 cents per share, a year earlier.\\n\\nTitle: ASX set to open higher as Wall Street rebounds; $A rises\\n\\nSource: The Age\\n\\nPublished Time: 2023-10-04T21:01:01+00:00\\n\\nFact: The yield on the 10-year Treasury, which is the centrepiece of the bond market, pulled back from its highest level since 2007, down to 4.73 per cent from 4.80 per cent late on Tuesday.\\n\\nTable 13: The example of comparison questions\\n\\nQuery: Was the performance of the Chicago Bears’ defense reported as improved by Yardbarker after Sporting News highlighted a sack by the Bears’ defense on Joshua Dobbs during the NFL ’Monday Night Football’ game? Answer: Yes\\n\\nEvidence List:\\n\\nTitle: Bears vs. Vikings live score, updates, highlights from NFL ’Monday Night Football’ game\\n\\nSource: Sporting News\\n\\nPublished Time: 2023-11-27T23:32:04+00:00\\n\\nFact: The Bears answer right back and sack Dobbs, with Sweat and Brisker in there to take him down.\\n\\nTitle: Hottest seat on each NFC team: Buns burning for these four head coaches\\n\\nSource: Yardbarker\\n\\nPublished Time: 2023-11-30T22:29:33+00:00\\n\\nFact: In his second season as HC, the defense has improved, but positive results are hard to come by behind a lackluster offense ranked 19th in yards (323.2) and 21st in points per game (20.2).\\n\\nTable 14: The example of time-sensitive questions\\n\\nQuery: What is the first letter of the CEO’s last name in the news article from Bloomberg on TomTom, and what is the first letter of the city where the company’s headquarters is located in the news article from Reuters?\\n\\nAnswer: Insufficient information.\\n\\nTable 15: The example of negative rejection questions'),\n",
       " Document(metadata={'doc_id': 'dd44188a-4d74-401e-afdb-4eb985051b58', 'source': '2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries.pdf', 'type': 'text', 'paper_id': 'a5cdaa51-39b4-42fe-bc76-e19fb729c37b'}, page_content='Null Query: Null query is a query whose an- swer cannot be derived from the retrieved set. To create null queries, we generate multi-hop queries using entities that do not exist in the existing bridge- entities. To add complexity, we also include fic- tional news source metadata when formulating these questions, ensuring that the questions do not reference any contextually relevant content from the knowledge base. The answer to the null query should be “insufficient information” or similar.\\n\\nStep 5: Quality Assurance. Finally, we use two approaches to reassure the dataset quality. First, we manually review a subset sample of the generated multi-hop queries, their corresponding evidence sets, and the final answers. The results of the man- ual examination indicate a high degree of accuracy and data quality. Second, we utilize GPT-4 to as- sess each example in the dataset against the follow- ing criteria: 1) The generated query must utilize all provided evidence in formulating the response; 2) The query should be answerable solely based on the provided evidence; 3) The response to the generated query should be either a single word or a specific entity; 4) The query must conform to its designated query type. Table 2: Descriptive statistics of the news article knowl- edge base in MultiHop-RAG. Table 3: The distribution of query types in MultiHop- RAG.\\n\\n3.2 Descriptive Statistics\\n\\nThe MultiHop-RAG dataset contains six different types of news articles, covering 609 distinct news, with an average of 2,046 tokens. The distribution of the news categories is shown in Table 2. MultiHop- RAG contains four types of multi-hop queries and the distribution of these queries is shown in Table 3. In total, about 88% of queries in the dataset are non-null queries where answers can be retrieved and reasoned from the knowledge base. In addition, the form of queries exhibits considerable diversity. Approximately 27% of interrogative queries start with \"does,\" around 15% initiate with \"what,\" a similar proportion start \"which,\" and 14% begin with \"who,\" with the remainder incorporating a small percentage of other interrogative words such as \"when.\" Moreover, the number of evidence re- quired to answer a multi-hop query varies. Table 4 shows the distribution of evidence numbers for each query in the dataset. Around 42% of queries can be answered using two pieces of evidence, while approximately 30% and 15% of queries can be answered using three or four pieces of evidence, respectively.\\n\\n4 Benchmarking RAG system using MultiHop-RAG\\n\\nMultiHop-RAG can be used as a benchmark for var- ious RAG-related tasks. Broadly speaking, RAG- Table 4: The distribution of the number of evidence required to answer multi-hop queries in MultiHop-RAG.\\n\\nrelated tasks can be categorized as retrieval-related tasks and generation-related tasks. A retrieval- related task focuses on retrieving relevant text from the knowledge base, while a generation-related task focuses on generating high-quality responses given the retrieved text. In this section, we showcase two use cases for each task where MultiHop-RAG can be employed.\\n\\n4.1 Retrieval-related Task\\n\\nAn important design choice in an RAG system is the selection of the embedding model. An embed- ding model converts data into numerical vectors and subsequently stores these vectors in embedding databases. In this experiment, we evaluate differ- ent embedding models by examining their retrieval quality.\\n\\nExperiment Setup: We implement an RAG sys- tem using the LlamaIndex framework (Liu, 2022). We partition the documents in the MultiHop-RAG knowledge base into chunks, each consisting of 256 tokens. We then convert the chunks using an em- bedding model and save the embeddings into a vec- tor database. Similarly, in the retrieval step, we con- vert a query using the same embedding model and retrieve the top-K most relevant chunks that have the highest cosine similarity with the query embed- ding. In this experiment, we test a variety set of em- bedding models, including the ada-embeddings by OpenAI (text-embedding-ada-002, text-search-ada- query-001), voyage-02 3, llm-embedder (Zhang et al., 2023), bge-large-en-v1.5 (Xiao et al., 2023), jina-embeddings-v2-base-en (Günther et al., 2023), e5-base-v2 (Wang et al., 2022), and instructor-large (Su et al., 2023). NULL queries are excluded in this experiment because there is no matching evi- dence to the query. Additionally, we also include a Reranker module to examine the retrieval perfor- mance, using bge-reranker-large (Xiao et al., 2023). After retrieving 20 related chunks using the em-\\n\\n3https://www.voyageai.com/\\n\\nbedding model, we further select the top-K chunks using the Reranker.\\n\\nExperiment Result: Table 5 shows the retrieval result of using different embedding models. shows that there is still a significant gap in retriev- ing relevant evidence for the multi-hop queries. While Rerank can effectively improve retrieval rel- evance, the highest Hits@10 is only 0.7467 when the Reranker technique is used. Moreover, the drop in the highest Hits@4 to 0.6625 is worrisome. In practical RAG systems, the underlying LLM of- ten has a context window limit. As a result, the number of retrieved chunks is usually restricted to a small number. The low values of the retrieval metrics highlight the challenges in retrieving rele- vant pieces of evidence for multi-hop queries when using direct similarity matching between the multi- hop query and text chunks. It 4.2 Generation-related Task\\n\\nThe underlying LLMs play a crucial role in gen- erating responses in an RAG system. In this ex- periment, we evaluate the quality of generated re- sponses under two different settings. In the first setting, we employ the best-performing retrieval model, namely voyage-02 with bge-reranker-large, as indicated in Table 5, to retrieve the top-K texts and then feed them into the LLM. In the second setting, we use the ground-truth evidence associ- ated with each query as the retrieved text for the LLM. This setting represents a ceiling performance for testing the LLM’s response capabilities, as it utilizes the actual evidences.\\n\\nExperiment Setup: In the first experiment, we retrieve top-6 chunks so that the total length of the retrieved text does not exceed 2,048. All queries in MultiHop-RAG are tested in the experiment. In the second experiment, since the null queries do not have associated evidence, we exclude this type of query in the experiment. For the LLMs used in the experiment, we consider state-of-the- art commercial models, including GPT-4 (OpenAI, 2023), GPT-3.5, Claude-2 (Anthropic, 2023), and Google-PaLM (Google, 2023). We obtain answers using the provided API of the respective models. We also assess some open-source models, includ- ing Mixtral-8x7b-instruct (Jiang et al., 2024) and Llama-2-70b-chat-hf (Touvron et al., 2023).\\n\\nExperiment Results: Table 6 shows the response accuracy of different LLMs. First, we can see that the response accuracy rate using the retrieved Table 5: Retrieval performance of different embedding models. Table 6: Generation accuracy of LLMs.\\n\\nchunks is not satisfactory, with the state-of-the- art GPT-4 model achieving only 0.56 accuracy. This is expected, because the retrieval component falls short in retrieving relevant evidences from the knowledge base. Second, even when we provide the LLM with the ground-truth evidences, we can see that the response accuracy is far from being per- fect. Open source LLM such as Llama02-70B and Mixtral-8x7B only achieve an accuracy of 0.32 and 0.36 respectively. GPT-4 achieves strong reason- ing capability with an accuracy of 0.89, followed by the second-based LLM Google-PaLM with an accuracy of 0.74.\\n\\nRetrieved Chunk — lm Mixtral-8x7B mmm PT4+ comparison temporal inference 0 BY) ry @ ro) ‘Accuracy (%) Ground-truth Chunk round-truth Chun bem Mintral- 8x78 comparison mm GPr4 temporal inference 0 20 40 60 80 100 Accuracy (%)\\n\\nFigure 3: Generation accuracy for different query types.\\n\\nthe chronological order of events, which is crucial for answering temporal queries where timing is a key factor. Taken together, this experiment demon- strates that there is still room for improvement in the reasoning capabilities of LLMs, particularly those that are open-source, for multi-hop queries.\\n\\nFigure 3 shows the detailed results of different query types for GPT-4 and Mixtral-8x7B-instruct. Both models show relatively high robustness on null queries, meaning they are generally good at determining when a query cannot be answered based on the retrieved text. This is encouraging be- cause one benefit of RAG is to mitigating the LLM hallucination issue by augmenting LLM with re- trieval knowledge. However, Mixtral-8x7B model performs significantly worse than the GPT-4 in comparison and temporal queries. Upon reviewing the incorrect responses, we find that Mixtral-8x7B fails to accurately handle logical negation, leading to misinterpretation of statements and thus a low performance in the comparison queries. In addi- tion, Mixtral-8x7B often fails to correctly identify 4.3 Other Use Cases\\n\\nBeyond embedding models and LLM generation, there are other areas worth exploring. For exam- ple, query decomposition is a widely utilized tech- nique in RAG frameworks, such as LLamaIndex. This process involves breaking down the query into smaller segments; it targets a single document for retrieval and integrates the information subse- quently, thereby potentially enhancing retrieval ac- curacy. Another advanced and promising approach involves building LLM-based agents that can au- tomatically plan and execute multi-hop queries, such as AutoGPT (Gravitas, 2023). Another area of interest is the hybrid retrieval approach, which combines keyword and embedding matching tech-\\n\\nniques. We believe that there are many potential areas for enhancing RAG’s performance on multi- hop queries, and the curated dataset MultiHop- RAG can be a valuable resource to the community.\\n\\n5 Related Work RAG Evaluation: As RAG systems gain increas-\\n\\ning popularity, a variety of RAG benchmarking datasets and evaluation tools have been developed. For instance, RGB (Chen et al., 2023) and RE- CALL (Liu et al., 2023) evaluate the performance of LLMs in generating responses for RAG systems under conditions involving noisy, integrative, and counterfactual queries. However, both datasets pri- marily focus on evaluating the generation aspect of RAG systems without specifically addressing their retrieval accuracy. In addition, recent ad- vancements have been made in automated RAG evaluation tools, such as ARES (Saad-Falcon et al., 2023) and RAGAS (Es et al., 2023). These tools utilize LLMs to automatically assess the quality of RAG generation, yet they do not introduce bench- marking datasets. Our work introduces one of the first RAG benchmarking datasets, consisting of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associ- ated supporting evidence, thereby complementing existing RAG evaluations.\\n\\nRetrieval datasets: Apart from the context of RAG, several benchmarking datasets exist for in- formation retrieval evaluation. The FEVER (Fact Extraction and VERification) dataset, for instance, contains claims classified as Supported, Refuted, or NotEnoughInfo by the given Wikipedia article (Thorne et al., 2018). Similarly, the SciFact dataset comprises scientific claims paired with evidence- containing abstracts (Wadden et al., 2020). How- ever, the claims in both datasets are single-hop statements, and the supporting evidence is from one single article, in contrast to the multi-hop queries discussed in this paper. Another dataset, HoVer, involves claims that require extracting and reason- ing from multiple Wikipedia articles (Jiang et al., 2020). However, unlike our dataset, HoVer focuses solely on classifying claims as either supported or not supported by the articles without evaluating an LLM generation step. Moreover, in HoVer, the Wikipedia articles from which evidence is drawn are given for claim verification, which is signifi- cantly different from our setting, where relevant\\n\\npieces of evidence need to be extracted from a\\n\\nlarge knowledge base. Separately, (Kamalloo et al., 2023) evaluates a range of commercial embedding APIs for information retrieval, but this evaluation is not contextualized within the framework of RAG systems either.\\n\\nMulti-document QA datasets: Question- answering (QA) is a fundamental task in NLP, and several popular benchmarks, such as HotpotQA (Yang et al., 2018), MultiRC (Khashabi et al., 2018), and 2WikiMultiHopQA (Ho et al., 2020), aim to achieve QA from multiple sources of documents. This task is similar to our multi-hop query RAG task, as both involve reasoning from multiple sources of information. However, these datasets primarily focus on assessing a model’s reasoning skills, and they do not emphasize the retrieval of evidence from a knowledge base. Additionally, their primary data sources Wikipedia, significantly overlap with the training data of most existing LLMs. If we use these sources for benchmarking RAG systems, there is a potential concern that LLM responses might rely on training knowledge rather than reasoning from the retrieved knowledge base. 6 Conclusion\\n\\nIn this work, we introduce MultiHop-RAG, a novel and unique dataset designed for queries that re- quire retrieval and reasoning from multiple pieces of supporting evidence. These types of multi-hop queries represent user queries commonly encoun- tered in real-world scenarios. MultiHop-RAG con- sists of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associated supporting evidence. This paper details the creation process of MultiHop-RAG, em- ploying a hybrid approach that integrates human effort with GPT-4. Additionally, we explore two use cases of MultiHop-RAG in the benchmarking of RAG systems, thereby highlighting the potential applications of this dataset. By publicly releas- ing MultiHop-RAG, we aim to provide a valuable resource to the community, contributing to the ad- vancement and benchmarking of RAG systems.\\n\\nLimitations\\n\\nThis work has several limitations that can be im- proved in future research. First, our ground truth answers are restricted to simple responses such as “yes\", “no\", entity names, or temporal indicators like “before\" or “after\" to facilitate the use of a\\n\\nstraightforward accuracy metric for evaluating gen- eration performance. Future work could consider allowing free text as answers and employing more sophisticated metrics to assess generation quality. Second, the current dataset limits supporting ev- idence for a query to a maximum of four pieces. Future work can extend the dataset by including queries that require retrieving and reasoning from even more evidence. Lastly, while our experiments utilize a basic RAG framework using LlamaIndex, future work could involve evaluating the answering of multi-hop queries using more advanced RAG frameworks or LLM-agent frameworks. References\\n\\nAnthropic. 2023. Claude 2.1 (May version). https: //api.anthropic.com/v1/messages. Claude 2.1.\\n\\nAkari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. 2023. Retrieval-based language models and applications. In Proceedings of the 61st Annual Meet- ing of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts), pages 41–46.\\n\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff- mann, Trevor Cai, Eliza Rutherford, Katie Milli- can, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Mag- giore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 2206–2240. PMLR.\\n\\nHarrison Chase. 2022. LangChain.\\n\\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2023. Benchmarking large language models in retrieval-augmented generation.\\n\\nShahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. 2023. Ragas: Automated evalua- tion of retrieval augmented generation.\\n\\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling large language models to generate text with citations.\\n\\nGoogle. 2023. PaLM 2 (May version). https://generativelanguage.googleapis. com/v1beta2/models/. Chat-bison-002.\\n\\nSignificant Gravitas. 2023. Autogpt. https://github. com/Significant-Gravitas/AutoGPT.\\n\\nMichael Günther, Jackmin Ong, Isabelle Mohr, Alaed- dine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, and Han Xiao. 2023. Jina embeddings 2: 8192- token general-purpose text embeddings for long doc- uments.')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs_mvr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/jpeg;base64,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\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>Models</td><td>Accuracy</td></tr><tr><td>Retrieved Chunk</td><td>~Ground-truth Chunk</td></tr><tr><td>GPT-4</td><td>0.56</td><td>0.89</td></tr><tr><td>ChatGPT</td><td>0.44</td><td>0.57</td></tr><tr><td>Llama-2-70b-chat-hf</td><td>0.28</td><td>0.32</td></tr><tr><td>Mixtral-8x7B-Instruct</td><td>0.32</td><td>0.36</td></tr><tr><td>Claude-2.1</td><td>0.52</td><td>0.56</td></tr><tr><td>Google-PaLM</td><td>0.47</td><td>0.74</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:  Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing a multi- hop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th Inter- national Conference on Computational Linguistics, pages 6609–6625, Barcelona, Spain (Online). Inter- national Committee on Computational Linguistics.\n",
      "\n",
      "Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gi- anna Lengyel, Guillaume Bour, Guillaume Lam- ple, Lélio Renard Lavaud, Lucile Saulnier, Marie- Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2024. Mix- tral of experts.\n",
      "\n",
      "Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and Mohit Bansal. 2020. HoVer: A dataset for many-hop fact extraction and claim verification. In Findings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).\n",
      "\n",
      "Ehsan Kamalloo, Xinyu Zhang, Odunayo Ogundepo, Nandan Thakur, David Alfonso-Hermelo, Mehdi Rezagholizadeh, and Jimmy Lin. 2023. Evaluat- ing embedding apis for information retrieval. arXiv preprint arXiv:2305.06300.\n",
      "\n",
      "Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences. In Proc. of the Annual Conference of the North American Chap- ter of the Association for Computational Linguistics (NAACL).\n",
      "\n",
      "Jerry Liu. 2022. LlamaIndex.\n",
      "\n",
      "Yi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023. Recall: A benchmark for llms robustness against external counterfactual knowledge.\n",
      "\n",
      "OpenAI. 2023. GPT4 (Nov 7 version). https://chat. openai.com/chat. gpt-4-1106-preview. Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. 2023. Ares: An automated evalua- tion framework for retrieval-augmented generation systems.\n",
      "\n",
      "Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. 2023. One embedder, any task: Instruction-finetuned text em- beddings.\n",
      "\n",
      "James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. Fever: a large-scale dataset for fact extraction and verification.\n",
      "\n",
      "Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, An- thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di- ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar- tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly- bog, Yixin Nie, Andrew Poulton, Jeremy Reizen- stein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subrama- nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay- lor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Ro- driguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine- tuned chat models.\n",
      "\n",
      "David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or fiction: Verifying scientific claims. In Proceedings of the 2020 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 7534–7550, Online. As- sociation for Computational Linguistics.\n",
      "\n",
      "Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weakly- supervised contrastive pre-training. arXiv preprint arXiv:2212.03533.\n",
      "\n",
      "Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-pack: Packaged resources to advance general chinese embedding.\n",
      "\n",
      "Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben- gio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answer- ing. In Conference on Empirical Methods in Natural Language Processing (EMNLP).\n",
      "\n",
      "Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. 2023. Retrieve anything to aug- ment large language models.\n",
      "\n",
      "Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\n",
      "\n",
      "Jiawei Han. 2022. Towards a unified multi- dimensional evaluator for text generation. A Appendix A: GPT-4 Prompts Used for Data Generation\n",
      "\n",
      "We present the prompts used for guiding GPT-4 for data generation. Table 7 shows the prompt used for claim generation, along with the corresponding top- ics and entities within these claims. Table 8, Table 9, and Table 10 respectively show the prompts used for generating multi-hop queries of the inference, comparison, and temporal types.\n",
      "\n",
      "B Appendix B: Dataset Examples\n",
      "\n",
      "In this appendix, we present an example of each type of multi-hop query included in the MultiHop- RAG dataset. These examples are illustrated in the respective tables: Table 12 for Inference Queries, Table 13 for Comparison Queries, Table 14 for Temporal Queries, and Table 15 for Null Queries. Each query is paired with a ground-truth answer for the evaluation of generation accuracy, while multiple pieces of supporting evidence are included for assessing retrieval performance. Additionally, metadata such as the title, source, and publication time of the news articles are provided as references.\n",
      "\n",
      "A \"claim\" is a statement or assertion made within a text expressing a belief, opinion, or fact. Given evidence from the original context, please extract one claim and its associated topics.\n",
      "\n",
      "Note: The claim should not contain ambiguous references, such as ’he’,’ she,’ and’ it’, and should use complete names. If there are multiple topics, give the most dominant one. The target of the claim (one entity)is the specific individual, group, or organization that the statement or assertion within a text is directed towards or about which it is making a case. The topic of the claim should be a simple phrase representing the claim’s central argument concept. If there is no claim, please leave it blank. Please generate a claim based on the given evidence. Don’t generate the evidence yourself.\n",
      "\n",
      "Please give the response following this format: Evidence: [original context] Claims: [extract claim] Claim Target: [target] Claim Topic: [topic]\n",
      "\n",
      "Here are examples: <examples> Now, it’s your turn. <News> <evidence>\n",
      "\n",
      "Table 7: Claim Generation Prompting\n",
      "\n",
      "A multi-hop question is a query requiring multiple inferential leaps or accessing several pieces of information from different locations or sources to arrive at an answer. The following are news articles’ metadata and claims come from the articles. All the claims from the article are related to a similar target. Your task is to generate one multi-hop inference question based on the claims. Here are some instructions:\n",
      "\n",
      "1. Find the Connection: The connection between claims is <target>, which is how these key pieces of information are related or how they can be combined to form a more complex idea.\n",
      "\n",
      "2. Formulate the Question: Create a question that cannot be answered by relying on just one of the sentences but instead requires understanding and linking the information from all of the sources. The answer is <target>.\n",
      "\n",
      "3. Ensure Coherence: Make sure the question flows logically from the combined information and is clear and unambiguous.\n",
      "\n",
      "4. Use the keywords: <key set>\n",
      "\n",
      "<examples> Context:\n",
      "\n",
      "<Context>\n",
      "\n",
      "Table 8: Inference Query Generation Prompting\n",
      "\n",
      "<Context>\n",
      "\n",
      "The above are news articles’ metadata and claims come from the articles. All the claims from the articles are related to a similar target. Your task is to generate one comparison question based on all the claims from different sources. This question needs to compare some factual elements of the claims that are explicitly stated to find where they agree or differ. The correct answer to this question is expressed as a comparative adjective, a statement of alignment, a simple yes or no. To generate a comparative question from claims, you need to use the following keywords: <key set>\n",
      "\n",
      "The Good Comparison Questions: <examples> Your Comparison Question:\n",
      "\n",
      "Table 9: Comparison Query Generation Prompting\n",
      "\n",
      "<Context>\n",
      "\n",
      "Please create a time-sensitive comparison question using metadata and excerpts from multiple news articles. That is to compare the consistency or sequence of reports on similar topics at multiple different time points. If it is to compare the consistency, please clearly mention the news source and time in the question using <time frame>. If it is to compare sequences of reports, just clearly mention the news source and do not mention the timeline. Utilize the following keywords provided in the <key set> to construct the question. The correct answer should based on the factual excerpts and is only one word.\n",
      "\n",
      "<examples>\n",
      "\n",
      "Your time-sensitive comparison question:\n",
      "\n",
      "Table 10: Temporal Query Generation Prompting\n",
      "\n",
      "A multi-hop question is a query requiring multiple inferential leaps or accessing several pieces of information from different locations or sources to arrive at an answer. Considering you have read at least two news articles on <entity>, construct a multi-hop question that incorporates all the news sources. The source of the news should be stated in the question. Also, ensure that the answer to the question is a single word/entity. Do not answer this question directly. Just give me the question:\n",
      "\n",
      "Table 11: Null Query Generation Prompting\n",
      "\n",
      "Query: Which platform is at the center of discussions in articles from Music Business Worldwide, Polygon, and FOX News - Health, concerning the policing of AI-driven voice replication, the debate over \"reaction\" content, and being the most used app overnight by young people? Answer: YouTube\n",
      "\n",
      "Evidence List:\n",
      "\n",
      "Title: Sony Music’s artists aren’t involved in YouTube’s new voice-cloning AI experiment.\n",
      "\n",
      "Source: Music Business Worldwide\n",
      "\n",
      "Published Time: 2023-11-23T18:48:48+00:00\n",
      "\n",
      "Fact: During this period of discussion, YouTube has made a number of positive announcements regarding the biggest issue for any rightsholder regarding AI-driven voice replication of artists: their ability to police it.\n",
      "\n",
      "Title: YouTube demonetizes popular content creator SSSniperwolf after doxxing accusations\n",
      "\n",
      "Source: Polygon\n",
      "\n",
      "Published Time: 2023-10-25T18:18:06+00:00\n",
      "\n",
      "Fact: The debate over \"reaction\" content on YouTube has been brewing for years, but a recent incident between two creators has refueled the urgency of the conversation.\n",
      "\n",
      "Title: Cell phone shocker as 97% of kids use their device during school hours and beyond, says study Source: FOX News - Health\n",
      "\n",
      "Published Time: 2023-10-01T09:05:26+00:00\n",
      "\n",
      "Fact: Overnight phone use was primarily spent engaging with the same media, although YouTube appeared to be the longest-running app because videos were often left playing during the night.\n",
      "\n",
      "Table 12: The example of inference questions\n",
      "\n",
      "Query: Did the Cnbc | World Business News Leader report on Nike’s net income and the article from\n",
      "\n",
      "The Age on the 10-year Treasury yield both report a decrease in their respective financial metrics? Answer: Yes\n",
      "\n",
      "Evidence List:\n",
      "\n",
      "Title: Nike misses revenue expectations for the first time in two years, beats on earnings and gross\n",
      "\n",
      "margin\n",
      "\n",
      "Source: Cnbc | World Business News Leader\n",
      "\n",
      "Published Time: 2023-09-28T20:31:00+00:00\n",
      "\n",
      "Fact: The company’s reported net income for the three-month period that ended August 31 was $1.45 billion, or 94 cents per share, compared with $1.47 billion, or 93 cents per share, a year earlier.\n",
      "\n",
      "Title: ASX set to open higher as Wall Street rebounds; $A rises\n",
      "\n",
      "Source: The Age\n",
      "\n",
      "Published Time: 2023-10-04T21:01:01+00:00\n",
      "\n",
      "Fact: The yield on the 10-year Treasury, which is the centrepiece of the bond market, pulled back from its highest level since 2007, down to 4.73 per cent from 4.80 per cent late on Tuesday.\n",
      "\n",
      "Table 13: The example of comparison questions\n",
      "\n",
      "Query: Was the performance of the Chicago Bears’ defense reported as improved by Yardbarker after Sporting News highlighted a sack by the Bears’ defense on Joshua Dobbs during the NFL ’Monday Night Football’ game? Answer: Yes\n",
      "\n",
      "Evidence List:\n",
      "\n",
      "Title: Bears vs. Vikings live score, updates, highlights from NFL ’Monday Night Football’ game\n",
      "\n",
      "Source: Sporting News\n",
      "\n",
      "Published Time: 2023-11-27T23:32:04+00:00\n",
      "\n",
      "Fact: The Bears answer right back and sack Dobbs, with Sweat and Brisker in there to take him down.\n",
      "\n",
      "Title: Hottest seat on each NFC team: Buns burning for these four head coaches\n",
      "\n",
      "Source: Yardbarker\n",
      "\n",
      "Published Time: 2023-11-30T22:29:33+00:00\n",
      "\n",
      "Fact: In his second season as HC, the defense has improved, but positive results are hard to come by behind a lackluster offense ranked 19th in yards (323.2) and 21st in points per game (20.2).\n",
      "\n",
      "Table 14: The example of time-sensitive questions\n",
      "\n",
      "Query: What is the first letter of the CEO’s last name in the news article from Bloomberg on TomTom, and what is the first letter of the city where the company’s headquarters is located in the news article from Reuters?\n",
      "\n",
      "Answer: Insufficient information.\n",
      "\n",
      "Table 15: The example of negative rejection questions\n",
      "Text:  Null Query: Null query is a query whose an- swer cannot be derived from the retrieved set. To create null queries, we generate multi-hop queries using entities that do not exist in the existing bridge- entities. To add complexity, we also include fic- tional news source metadata when formulating these questions, ensuring that the questions do not reference any contextually relevant content from the knowledge base. The answer to the null query should be “insufficient information” or similar.\n",
      "\n",
      "Step 5: Quality Assurance. Finally, we use two approaches to reassure the dataset quality. First, we manually review a subset sample of the generated multi-hop queries, their corresponding evidence sets, and the final answers. The results of the man- ual examination indicate a high degree of accuracy and data quality. Second, we utilize GPT-4 to as- sess each example in the dataset against the follow- ing criteria: 1) The generated query must utilize all provided evidence in formulating the response; 2) The query should be answerable solely based on the provided evidence; 3) The response to the generated query should be either a single word or a specific entity; 4) The query must conform to its designated query type. Table 2: Descriptive statistics of the news article knowl- edge base in MultiHop-RAG. Table 3: The distribution of query types in MultiHop- RAG.\n",
      "\n",
      "3.2 Descriptive Statistics\n",
      "\n",
      "The MultiHop-RAG dataset contains six different types of news articles, covering 609 distinct news, with an average of 2,046 tokens. The distribution of the news categories is shown in Table 2. MultiHop- RAG contains four types of multi-hop queries and the distribution of these queries is shown in Table 3. In total, about 88% of queries in the dataset are non-null queries where answers can be retrieved and reasoned from the knowledge base. In addition, the form of queries exhibits considerable diversity. Approximately 27% of interrogative queries start with \"does,\" around 15% initiate with \"what,\" a similar proportion start \"which,\" and 14% begin with \"who,\" with the remainder incorporating a small percentage of other interrogative words such as \"when.\" Moreover, the number of evidence re- quired to answer a multi-hop query varies. Table 4 shows the distribution of evidence numbers for each query in the dataset. Around 42% of queries can be answered using two pieces of evidence, while approximately 30% and 15% of queries can be answered using three or four pieces of evidence, respectively.\n",
      "\n",
      "4 Benchmarking RAG system using MultiHop-RAG\n",
      "\n",
      "MultiHop-RAG can be used as a benchmark for var- ious RAG-related tasks. Broadly speaking, RAG- Table 4: The distribution of the number of evidence required to answer multi-hop queries in MultiHop-RAG.\n",
      "\n",
      "related tasks can be categorized as retrieval-related tasks and generation-related tasks. A retrieval- related task focuses on retrieving relevant text from the knowledge base, while a generation-related task focuses on generating high-quality responses given the retrieved text. In this section, we showcase two use cases for each task where MultiHop-RAG can be employed.\n",
      "\n",
      "4.1 Retrieval-related Task\n",
      "\n",
      "An important design choice in an RAG system is the selection of the embedding model. An embed- ding model converts data into numerical vectors and subsequently stores these vectors in embedding databases. In this experiment, we evaluate differ- ent embedding models by examining their retrieval quality.\n",
      "\n",
      "Experiment Setup: We implement an RAG sys- tem using the LlamaIndex framework (Liu, 2022). We partition the documents in the MultiHop-RAG knowledge base into chunks, each consisting of 256 tokens. We then convert the chunks using an em- bedding model and save the embeddings into a vec- tor database. Similarly, in the retrieval step, we con- vert a query using the same embedding model and retrieve the top-K most relevant chunks that have the highest cosine similarity with the query embed- ding. In this experiment, we test a variety set of em- bedding models, including the ada-embeddings by OpenAI (text-embedding-ada-002, text-search-ada- query-001), voyage-02 3, llm-embedder (Zhang et al., 2023), bge-large-en-v1.5 (Xiao et al., 2023), jina-embeddings-v2-base-en (Günther et al., 2023), e5-base-v2 (Wang et al., 2022), and instructor-large (Su et al., 2023). NULL queries are excluded in this experiment because there is no matching evi- dence to the query. Additionally, we also include a Reranker module to examine the retrieval perfor- mance, using bge-reranker-large (Xiao et al., 2023). After retrieving 20 related chunks using the em-\n",
      "\n",
      "3https://www.voyageai.com/\n",
      "\n",
      "bedding model, we further select the top-K chunks using the Reranker.\n",
      "\n",
      "Experiment Result: Table 5 shows the retrieval result of using different embedding models. shows that there is still a significant gap in retriev- ing relevant evidence for the multi-hop queries. While Rerank can effectively improve retrieval rel- evance, the highest Hits@10 is only 0.7467 when the Reranker technique is used. Moreover, the drop in the highest Hits@4 to 0.6625 is worrisome. In practical RAG systems, the underlying LLM of- ten has a context window limit. As a result, the number of retrieved chunks is usually restricted to a small number. The low values of the retrieval metrics highlight the challenges in retrieving rele- vant pieces of evidence for multi-hop queries when using direct similarity matching between the multi- hop query and text chunks. It 4.2 Generation-related Task\n",
      "\n",
      "The underlying LLMs play a crucial role in gen- erating responses in an RAG system. In this ex- periment, we evaluate the quality of generated re- sponses under two different settings. In the first setting, we employ the best-performing retrieval model, namely voyage-02 with bge-reranker-large, as indicated in Table 5, to retrieve the top-K texts and then feed them into the LLM. In the second setting, we use the ground-truth evidence associ- ated with each query as the retrieved text for the LLM. This setting represents a ceiling performance for testing the LLM’s response capabilities, as it utilizes the actual evidences.\n",
      "\n",
      "Experiment Setup: In the first experiment, we retrieve top-6 chunks so that the total length of the retrieved text does not exceed 2,048. All queries in MultiHop-RAG are tested in the experiment. In the second experiment, since the null queries do not have associated evidence, we exclude this type of query in the experiment. For the LLMs used in the experiment, we consider state-of-the- art commercial models, including GPT-4 (OpenAI, 2023), GPT-3.5, Claude-2 (Anthropic, 2023), and Google-PaLM (Google, 2023). We obtain answers using the provided API of the respective models. We also assess some open-source models, includ- ing Mixtral-8x7b-instruct (Jiang et al., 2024) and Llama-2-70b-chat-hf (Touvron et al., 2023).\n",
      "\n",
      "Experiment Results: Table 6 shows the response accuracy of different LLMs. First, we can see that the response accuracy rate using the retrieved Table 5: Retrieval performance of different embedding models. Table 6: Generation accuracy of LLMs.\n",
      "\n",
      "chunks is not satisfactory, with the state-of-the- art GPT-4 model achieving only 0.56 accuracy. This is expected, because the retrieval component falls short in retrieving relevant evidences from the knowledge base. Second, even when we provide the LLM with the ground-truth evidences, we can see that the response accuracy is far from being per- fect. Open source LLM such as Llama02-70B and Mixtral-8x7B only achieve an accuracy of 0.32 and 0.36 respectively. GPT-4 achieves strong reason- ing capability with an accuracy of 0.89, followed by the second-based LLM Google-PaLM with an accuracy of 0.74.\n",
      "\n",
      "Retrieved Chunk — lm Mixtral-8x7B mmm PT4+ comparison temporal inference 0 BY) ry @ ro) ‘Accuracy (%) Ground-truth Chunk round-truth Chun bem Mintral- 8x78 comparison mm GPr4 temporal inference 0 20 40 60 80 100 Accuracy (%)\n",
      "\n",
      "Figure 3: Generation accuracy for different query types.\n",
      "\n",
      "the chronological order of events, which is crucial for answering temporal queries where timing is a key factor. Taken together, this experiment demon- strates that there is still room for improvement in the reasoning capabilities of LLMs, particularly those that are open-source, for multi-hop queries.\n",
      "\n",
      "Figure 3 shows the detailed results of different query types for GPT-4 and Mixtral-8x7B-instruct. Both models show relatively high robustness on null queries, meaning they are generally good at determining when a query cannot be answered based on the retrieved text. This is encouraging be- cause one benefit of RAG is to mitigating the LLM hallucination issue by augmenting LLM with re- trieval knowledge. However, Mixtral-8x7B model performs significantly worse than the GPT-4 in comparison and temporal queries. Upon reviewing the incorrect responses, we find that Mixtral-8x7B fails to accurately handle logical negation, leading to misinterpretation of statements and thus a low performance in the comparison queries. In addi- tion, Mixtral-8x7B often fails to correctly identify 4.3 Other Use Cases\n",
      "\n",
      "Beyond embedding models and LLM generation, there are other areas worth exploring. For exam- ple, query decomposition is a widely utilized tech- nique in RAG frameworks, such as LLamaIndex. This process involves breaking down the query into smaller segments; it targets a single document for retrieval and integrates the information subse- quently, thereby potentially enhancing retrieval ac- curacy. Another advanced and promising approach involves building LLM-based agents that can au- tomatically plan and execute multi-hop queries, such as AutoGPT (Gravitas, 2023). Another area of interest is the hybrid retrieval approach, which combines keyword and embedding matching tech-\n",
      "\n",
      "niques. We believe that there are many potential areas for enhancing RAG’s performance on multi- hop queries, and the curated dataset MultiHop- RAG can be a valuable resource to the community.\n",
      "\n",
      "5 Related Work RAG Evaluation: As RAG systems gain increas-\n",
      "\n",
      "ing popularity, a variety of RAG benchmarking datasets and evaluation tools have been developed. For instance, RGB (Chen et al., 2023) and RE- CALL (Liu et al., 2023) evaluate the performance of LLMs in generating responses for RAG systems under conditions involving noisy, integrative, and counterfactual queries. However, both datasets pri- marily focus on evaluating the generation aspect of RAG systems without specifically addressing their retrieval accuracy. In addition, recent ad- vancements have been made in automated RAG evaluation tools, such as ARES (Saad-Falcon et al., 2023) and RAGAS (Es et al., 2023). These tools utilize LLMs to automatically assess the quality of RAG generation, yet they do not introduce bench- marking datasets. Our work introduces one of the first RAG benchmarking datasets, consisting of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associ- ated supporting evidence, thereby complementing existing RAG evaluations.\n",
      "\n",
      "Retrieval datasets: Apart from the context of RAG, several benchmarking datasets exist for in- formation retrieval evaluation. The FEVER (Fact Extraction and VERification) dataset, for instance, contains claims classified as Supported, Refuted, or NotEnoughInfo by the given Wikipedia article (Thorne et al., 2018). Similarly, the SciFact dataset comprises scientific claims paired with evidence- containing abstracts (Wadden et al., 2020). How- ever, the claims in both datasets are single-hop statements, and the supporting evidence is from one single article, in contrast to the multi-hop queries discussed in this paper. Another dataset, HoVer, involves claims that require extracting and reason- ing from multiple Wikipedia articles (Jiang et al., 2020). However, unlike our dataset, HoVer focuses solely on classifying claims as either supported or not supported by the articles without evaluating an LLM generation step. Moreover, in HoVer, the Wikipedia articles from which evidence is drawn are given for claim verification, which is signifi- cantly different from our setting, where relevant\n",
      "\n",
      "pieces of evidence need to be extracted from a\n",
      "\n",
      "large knowledge base. Separately, (Kamalloo et al., 2023) evaluates a range of commercial embedding APIs for information retrieval, but this evaluation is not contextualized within the framework of RAG systems either.\n",
      "\n",
      "Multi-document QA datasets: Question- answering (QA) is a fundamental task in NLP, and several popular benchmarks, such as HotpotQA (Yang et al., 2018), MultiRC (Khashabi et al., 2018), and 2WikiMultiHopQA (Ho et al., 2020), aim to achieve QA from multiple sources of documents. This task is similar to our multi-hop query RAG task, as both involve reasoning from multiple sources of information. However, these datasets primarily focus on assessing a model’s reasoning skills, and they do not emphasize the retrieval of evidence from a knowledge base. Additionally, their primary data sources Wikipedia, significantly overlap with the training data of most existing LLMs. If we use these sources for benchmarking RAG systems, there is a potential concern that LLM responses might rely on training knowledge rather than reasoning from the retrieved knowledge base. 6 Conclusion\n",
      "\n",
      "In this work, we introduce MultiHop-RAG, a novel and unique dataset designed for queries that re- quire retrieval and reasoning from multiple pieces of supporting evidence. These types of multi-hop queries represent user queries commonly encoun- tered in real-world scenarios. MultiHop-RAG con- sists of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associated supporting evidence. This paper details the creation process of MultiHop-RAG, em- ploying a hybrid approach that integrates human effort with GPT-4. Additionally, we explore two use cases of MultiHop-RAG in the benchmarking of RAG systems, thereby highlighting the potential applications of this dataset. By publicly releas- ing MultiHop-RAG, we aim to provide a valuable resource to the community, contributing to the ad- vancement and benchmarking of RAG systems.\n",
      "\n",
      "Limitations\n",
      "\n",
      "This work has several limitations that can be im- proved in future research. First, our ground truth answers are restricted to simple responses such as “yes\", “no\", entity names, or temporal indicators like “before\" or “after\" to facilitate the use of a\n",
      "\n",
      "straightforward accuracy metric for evaluating gen- eration performance. Future work could consider allowing free text as answers and employing more sophisticated metrics to assess generation quality. Second, the current dataset limits supporting ev- idence for a query to a maximum of four pieces. Future work can extend the dataset by including queries that require retrieving and reasoning from even more evidence. Lastly, while our experiments utilize a basic RAG framework using LlamaIndex, future work could involve evaluating the answering of multi-hop queries using more advanced RAG frameworks or LLM-agent frameworks. References\n",
      "\n",
      "Anthropic. 2023. Claude 2.1 (May version). https: //api.anthropic.com/v1/messages. Claude 2.1.\n",
      "\n",
      "Akari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. 2023. Retrieval-based language models and applications. In Proceedings of the 61st Annual Meet- ing of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts), pages 41–46.\n",
      "\n",
      "Sebastian Borgeaud, Arthur Mensch, Jordan Hoff- mann, Trevor Cai, Eliza Rutherford, Katie Milli- can, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Mag- giore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 2206–2240. PMLR.\n",
      "\n",
      "Harrison Chase. 2022. LangChain.\n",
      "\n",
      "Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2023. Benchmarking large language models in retrieval-augmented generation.\n",
      "\n",
      "Shahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. 2023. Ragas: Automated evalua- tion of retrieval augmented generation.\n",
      "\n",
      "Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling large language models to generate text with citations.\n",
      "\n",
      "Google. 2023. PaLM 2 (May version). https://generativelanguage.googleapis. com/v1beta2/models/. Chat-bison-002.\n",
      "\n",
      "Significant Gravitas. 2023. Autogpt. https://github. com/Significant-Gravitas/AutoGPT.\n",
      "\n",
      "Michael Günther, Jackmin Ong, Isabelle Mohr, Alaed- dine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, and Han Xiao. 2023. Jina embeddings 2: 8192- token general-purpose text embeddings for long doc- uments.\n"
     ]
    }
   ],
   "source": [
    "for d in relevant_docs_mvr:\n",
    "    if d.metadata[\"type\"] == \"image\":\n",
    "        plt_img_base64(d.page_content)\n",
    "    elif d.metadata[\"type\"] == \"table\":\n",
    "        display(HTML(d.page_content))\n",
    "    else:\n",
    "        print(\"Text: \",d.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bar charts comparing accuracy percentages of Mixtral-8x7B and GPT-4 models across categories: null, comparison, temporal, and inference. The top chart is for \"Retrieved Chunk,\" and the bottom is \"Ground-truth Chunk.\" GPT-4 generally shows higher accuracy.'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_summaries[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2. Text Only retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n"
     ]
    }
   ],
   "source": [
    "relevant_docs_txt = retriever_txt.invoke(user_query, limit=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'doc_id': 'a5cdaa51-39b4-42fe-bc76-e19fb729c37b', 'paper_id': 'a5cdaa51-39b4-42fe-bc76-e19fb729c37b', 'source': '2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries.pdf', 'type': 'text'}, page_content='Xanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara, and Akiko Aizawa. 2020. Constructing a multi- hop QA dataset for comprehensive evaluation of reasoning steps. In Proceedings of the 28th Inter- national Conference on Computational Linguistics, pages 6609–6625, Barcelona, Spain (Online). Inter- national Committee on Computational Linguistics.\\n\\nAlbert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gi- anna Lengyel, Guillaume Bour, Guillaume Lam- ple, Lélio Renard Lavaud, Lucile Saulnier, Marie- Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2024. Mix- tral of experts.\\n\\nYichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and Mohit Bansal. 2020. HoVer: A dataset for many-hop fact extraction and claim verification. In Findings of the Conference on Empirical Methods in Natural Language Processing (EMNLP).\\n\\nEhsan Kamalloo, Xinyu Zhang, Odunayo Ogundepo, Nandan Thakur, David Alfonso-Hermelo, Mehdi Rezagholizadeh, and Jimmy Lin. 2023. Evaluat- ing embedding apis for information retrieval. arXiv preprint arXiv:2305.06300.\\n\\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences. In Proc. of the Annual Conference of the North American Chap- ter of the Association for Computational Linguistics (NAACL).\\n\\nJerry Liu. 2022. LlamaIndex.\\n\\nYi Liu, Lianzhe Huang, Shicheng Li, Sishuo Chen, Hao Zhou, Fandong Meng, Jie Zhou, and Xu Sun. 2023. Recall: A benchmark for llms robustness against external counterfactual knowledge.\\n\\nOpenAI. 2023. GPT4 (Nov 7 version). https://chat. openai.com/chat. gpt-4-1106-preview. Jon Saad-Falcon, Omar Khattab, Christopher Potts, and Matei Zaharia. 2023. Ares: An automated evalua- tion framework for retrieval-augmented generation systems.\\n\\nHongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. 2023. One embedder, any task: Instruction-finetuned text em- beddings.\\n\\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. Fever: a large-scale dataset for fact extraction and verification.\\n\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al- bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, An- thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di- ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar- tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly- bog, Yixin Nie, Andrew Poulton, Jeremy Reizen- stein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subrama- nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay- lor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Ro- driguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine- tuned chat models.\\n\\nDavid Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu Wang, Madeleine van Zuylen, Arman Cohan, and Hannaneh Hajishirzi. 2020. Fact or fiction: Verifying scientific claims. In Proceedings of the 2020 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 7534–7550, Online. As- sociation for Computational Linguistics.\\n\\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2022. Text embeddings by weakly- supervised contrastive pre-training. arXiv preprint arXiv:2212.03533.\\n\\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-pack: Packaged resources to advance general chinese embedding.\\n\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben- gio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answer- ing. In Conference on Empirical Methods in Natural Language Processing (EMNLP).\\n\\nPeitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. 2023. Retrieve anything to aug- ment large language models.\\n\\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\\n\\nJiawei Han. 2022. Towards a unified multi- dimensional evaluator for text generation. A Appendix A: GPT-4 Prompts Used for Data Generation\\n\\nWe present the prompts used for guiding GPT-4 for data generation. Table 7 shows the prompt used for claim generation, along with the corresponding top- ics and entities within these claims. Table 8, Table 9, and Table 10 respectively show the prompts used for generating multi-hop queries of the inference, comparison, and temporal types.\\n\\nB Appendix B: Dataset Examples\\n\\nIn this appendix, we present an example of each type of multi-hop query included in the MultiHop- RAG dataset. These examples are illustrated in the respective tables: Table 12 for Inference Queries, Table 13 for Comparison Queries, Table 14 for Temporal Queries, and Table 15 for Null Queries. Each query is paired with a ground-truth answer for the evaluation of generation accuracy, while multiple pieces of supporting evidence are included for assessing retrieval performance. Additionally, metadata such as the title, source, and publication time of the news articles are provided as references.\\n\\nA \"claim\" is a statement or assertion made within a text expressing a belief, opinion, or fact. Given evidence from the original context, please extract one claim and its associated topics.\\n\\nNote: The claim should not contain ambiguous references, such as ’he’,’ she,’ and’ it’, and should use complete names. If there are multiple topics, give the most dominant one. The target of the claim (one entity)is the specific individual, group, or organization that the statement or assertion within a text is directed towards or about which it is making a case. The topic of the claim should be a simple phrase representing the claim’s central argument concept. If there is no claim, please leave it blank. Please generate a claim based on the given evidence. Don’t generate the evidence yourself.\\n\\nPlease give the response following this format: Evidence: [original context] Claims: [extract claim] Claim Target: [target] Claim Topic: [topic]\\n\\nHere are examples: <examples> Now, it’s your turn. <News> <evidence>\\n\\nTable 7: Claim Generation Prompting\\n\\nA multi-hop question is a query requiring multiple inferential leaps or accessing several pieces of information from different locations or sources to arrive at an answer. The following are news articles’ metadata and claims come from the articles. All the claims from the article are related to a similar target. Your task is to generate one multi-hop inference question based on the claims. Here are some instructions:\\n\\n1. Find the Connection: The connection between claims is <target>, which is how these key pieces of information are related or how they can be combined to form a more complex idea.\\n\\n2. Formulate the Question: Create a question that cannot be answered by relying on just one of the sentences but instead requires understanding and linking the information from all of the sources. The answer is <target>.\\n\\n3. Ensure Coherence: Make sure the question flows logically from the combined information and is clear and unambiguous.\\n\\n4. Use the keywords: <key set>\\n\\n<examples> Context:\\n\\n<Context>\\n\\nTable 8: Inference Query Generation Prompting\\n\\n<Context>\\n\\nThe above are news articles’ metadata and claims come from the articles. All the claims from the articles are related to a similar target. Your task is to generate one comparison question based on all the claims from different sources. This question needs to compare some factual elements of the claims that are explicitly stated to find where they agree or differ. The correct answer to this question is expressed as a comparative adjective, a statement of alignment, a simple yes or no. To generate a comparative question from claims, you need to use the following keywords: <key set>\\n\\nThe Good Comparison Questions: <examples> Your Comparison Question:\\n\\nTable 9: Comparison Query Generation Prompting\\n\\n<Context>\\n\\nPlease create a time-sensitive comparison question using metadata and excerpts from multiple news articles. That is to compare the consistency or sequence of reports on similar topics at multiple different time points. If it is to compare the consistency, please clearly mention the news source and time in the question using <time frame>. If it is to compare sequences of reports, just clearly mention the news source and do not mention the timeline. Utilize the following keywords provided in the <key set> to construct the question. The correct answer should based on the factual excerpts and is only one word.\\n\\n<examples>\\n\\nYour time-sensitive comparison question:\\n\\nTable 10: Temporal Query Generation Prompting\\n\\nA multi-hop question is a query requiring multiple inferential leaps or accessing several pieces of information from different locations or sources to arrive at an answer. Considering you have read at least two news articles on <entity>, construct a multi-hop question that incorporates all the news sources. The source of the news should be stated in the question. Also, ensure that the answer to the question is a single word/entity. Do not answer this question directly. Just give me the question:\\n\\nTable 11: Null Query Generation Prompting\\n\\nQuery: Which platform is at the center of discussions in articles from Music Business Worldwide, Polygon, and FOX News - Health, concerning the policing of AI-driven voice replication, the debate over \"reaction\" content, and being the most used app overnight by young people? Answer: YouTube\\n\\nEvidence List:\\n\\nTitle: Sony Music’s artists aren’t involved in YouTube’s new voice-cloning AI experiment.\\n\\nSource: Music Business Worldwide\\n\\nPublished Time: 2023-11-23T18:48:48+00:00\\n\\nFact: During this period of discussion, YouTube has made a number of positive announcements regarding the biggest issue for any rightsholder regarding AI-driven voice replication of artists: their ability to police it.\\n\\nTitle: YouTube demonetizes popular content creator SSSniperwolf after doxxing accusations\\n\\nSource: Polygon\\n\\nPublished Time: 2023-10-25T18:18:06+00:00\\n\\nFact: The debate over \"reaction\" content on YouTube has been brewing for years, but a recent incident between two creators has refueled the urgency of the conversation.\\n\\nTitle: Cell phone shocker as 97% of kids use their device during school hours and beyond, says study Source: FOX News - Health\\n\\nPublished Time: 2023-10-01T09:05:26+00:00\\n\\nFact: Overnight phone use was primarily spent engaging with the same media, although YouTube appeared to be the longest-running app because videos were often left playing during the night.\\n\\nTable 12: The example of inference questions\\n\\nQuery: Did the Cnbc | World Business News Leader report on Nike’s net income and the article from\\n\\nThe Age on the 10-year Treasury yield both report a decrease in their respective financial metrics? Answer: Yes\\n\\nEvidence List:\\n\\nTitle: Nike misses revenue expectations for the first time in two years, beats on earnings and gross\\n\\nmargin\\n\\nSource: Cnbc | World Business News Leader\\n\\nPublished Time: 2023-09-28T20:31:00+00:00\\n\\nFact: The company’s reported net income for the three-month period that ended August 31 was $1.45 billion, or 94 cents per share, compared with $1.47 billion, or 93 cents per share, a year earlier.\\n\\nTitle: ASX set to open higher as Wall Street rebounds; $A rises\\n\\nSource: The Age\\n\\nPublished Time: 2023-10-04T21:01:01+00:00\\n\\nFact: The yield on the 10-year Treasury, which is the centrepiece of the bond market, pulled back from its highest level since 2007, down to 4.73 per cent from 4.80 per cent late on Tuesday.\\n\\nTable 13: The example of comparison questions\\n\\nQuery: Was the performance of the Chicago Bears’ defense reported as improved by Yardbarker after Sporting News highlighted a sack by the Bears’ defense on Joshua Dobbs during the NFL ’Monday Night Football’ game? Answer: Yes\\n\\nEvidence List:\\n\\nTitle: Bears vs. Vikings live score, updates, highlights from NFL ’Monday Night Football’ game\\n\\nSource: Sporting News\\n\\nPublished Time: 2023-11-27T23:32:04+00:00\\n\\nFact: The Bears answer right back and sack Dobbs, with Sweat and Brisker in there to take him down.\\n\\nTitle: Hottest seat on each NFC team: Buns burning for these four head coaches\\n\\nSource: Yardbarker\\n\\nPublished Time: 2023-11-30T22:29:33+00:00\\n\\nFact: In his second season as HC, the defense has improved, but positive results are hard to come by behind a lackluster offense ranked 19th in yards (323.2) and 21st in points per game (20.2).\\n\\nTable 14: The example of time-sensitive questions\\n\\nQuery: What is the first letter of the CEO’s last name in the news article from Bloomberg on TomTom, and what is the first letter of the city where the company’s headquarters is located in the news article from Reuters?\\n\\nAnswer: Insufficient information.\\n\\nTable 15: The example of negative rejection questions'),\n",
       " Document(metadata={'doc_id': 'a5cdaa51-39b4-42fe-bc76-e19fb729c37b', 'paper_id': 'a5cdaa51-39b4-42fe-bc76-e19fb729c37b', 'source': '2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries.pdf', 'type': 'text'}, page_content='Null Query: Null query is a query whose an- swer cannot be derived from the retrieved set. To create null queries, we generate multi-hop queries using entities that do not exist in the existing bridge- entities. To add complexity, we also include fic- tional news source metadata when formulating these questions, ensuring that the questions do not reference any contextually relevant content from the knowledge base. The answer to the null query should be “insufficient information” or similar.\\n\\nStep 5: Quality Assurance. Finally, we use two approaches to reassure the dataset quality. First, we manually review a subset sample of the generated multi-hop queries, their corresponding evidence sets, and the final answers. The results of the man- ual examination indicate a high degree of accuracy and data quality. Second, we utilize GPT-4 to as- sess each example in the dataset against the follow- ing criteria: 1) The generated query must utilize all provided evidence in formulating the response; 2) The query should be answerable solely based on the provided evidence; 3) The response to the generated query should be either a single word or a specific entity; 4) The query must conform to its designated query type. Table 2: Descriptive statistics of the news article knowl- edge base in MultiHop-RAG. Table 3: The distribution of query types in MultiHop- RAG.\\n\\n3.2 Descriptive Statistics\\n\\nThe MultiHop-RAG dataset contains six different types of news articles, covering 609 distinct news, with an average of 2,046 tokens. The distribution of the news categories is shown in Table 2. MultiHop- RAG contains four types of multi-hop queries and the distribution of these queries is shown in Table 3. In total, about 88% of queries in the dataset are non-null queries where answers can be retrieved and reasoned from the knowledge base. In addition, the form of queries exhibits considerable diversity. Approximately 27% of interrogative queries start with \"does,\" around 15% initiate with \"what,\" a similar proportion start \"which,\" and 14% begin with \"who,\" with the remainder incorporating a small percentage of other interrogative words such as \"when.\" Moreover, the number of evidence re- quired to answer a multi-hop query varies. Table 4 shows the distribution of evidence numbers for each query in the dataset. Around 42% of queries can be answered using two pieces of evidence, while approximately 30% and 15% of queries can be answered using three or four pieces of evidence, respectively.\\n\\n4 Benchmarking RAG system using MultiHop-RAG\\n\\nMultiHop-RAG can be used as a benchmark for var- ious RAG-related tasks. Broadly speaking, RAG- Table 4: The distribution of the number of evidence required to answer multi-hop queries in MultiHop-RAG.\\n\\nrelated tasks can be categorized as retrieval-related tasks and generation-related tasks. A retrieval- related task focuses on retrieving relevant text from the knowledge base, while a generation-related task focuses on generating high-quality responses given the retrieved text. In this section, we showcase two use cases for each task where MultiHop-RAG can be employed.\\n\\n4.1 Retrieval-related Task\\n\\nAn important design choice in an RAG system is the selection of the embedding model. An embed- ding model converts data into numerical vectors and subsequently stores these vectors in embedding databases. In this experiment, we evaluate differ- ent embedding models by examining their retrieval quality.\\n\\nExperiment Setup: We implement an RAG sys- tem using the LlamaIndex framework (Liu, 2022). We partition the documents in the MultiHop-RAG knowledge base into chunks, each consisting of 256 tokens. We then convert the chunks using an em- bedding model and save the embeddings into a vec- tor database. Similarly, in the retrieval step, we con- vert a query using the same embedding model and retrieve the top-K most relevant chunks that have the highest cosine similarity with the query embed- ding. In this experiment, we test a variety set of em- bedding models, including the ada-embeddings by OpenAI (text-embedding-ada-002, text-search-ada- query-001), voyage-02 3, llm-embedder (Zhang et al., 2023), bge-large-en-v1.5 (Xiao et al., 2023), jina-embeddings-v2-base-en (Günther et al., 2023), e5-base-v2 (Wang et al., 2022), and instructor-large (Su et al., 2023). NULL queries are excluded in this experiment because there is no matching evi- dence to the query. Additionally, we also include a Reranker module to examine the retrieval perfor- mance, using bge-reranker-large (Xiao et al., 2023). After retrieving 20 related chunks using the em-\\n\\n3https://www.voyageai.com/\\n\\nbedding model, we further select the top-K chunks using the Reranker.\\n\\nExperiment Result: Table 5 shows the retrieval result of using different embedding models. shows that there is still a significant gap in retriev- ing relevant evidence for the multi-hop queries. While Rerank can effectively improve retrieval rel- evance, the highest Hits@10 is only 0.7467 when the Reranker technique is used. Moreover, the drop in the highest Hits@4 to 0.6625 is worrisome. In practical RAG systems, the underlying LLM of- ten has a context window limit. As a result, the number of retrieved chunks is usually restricted to a small number. The low values of the retrieval metrics highlight the challenges in retrieving rele- vant pieces of evidence for multi-hop queries when using direct similarity matching between the multi- hop query and text chunks. It 4.2 Generation-related Task\\n\\nThe underlying LLMs play a crucial role in gen- erating responses in an RAG system. In this ex- periment, we evaluate the quality of generated re- sponses under two different settings. In the first setting, we employ the best-performing retrieval model, namely voyage-02 with bge-reranker-large, as indicated in Table 5, to retrieve the top-K texts and then feed them into the LLM. In the second setting, we use the ground-truth evidence associ- ated with each query as the retrieved text for the LLM. This setting represents a ceiling performance for testing the LLM’s response capabilities, as it utilizes the actual evidences.\\n\\nExperiment Setup: In the first experiment, we retrieve top-6 chunks so that the total length of the retrieved text does not exceed 2,048. All queries in MultiHop-RAG are tested in the experiment. In the second experiment, since the null queries do not have associated evidence, we exclude this type of query in the experiment. For the LLMs used in the experiment, we consider state-of-the- art commercial models, including GPT-4 (OpenAI, 2023), GPT-3.5, Claude-2 (Anthropic, 2023), and Google-PaLM (Google, 2023). We obtain answers using the provided API of the respective models. We also assess some open-source models, includ- ing Mixtral-8x7b-instruct (Jiang et al., 2024) and Llama-2-70b-chat-hf (Touvron et al., 2023).\\n\\nExperiment Results: Table 6 shows the response accuracy of different LLMs. First, we can see that the response accuracy rate using the retrieved Table 5: Retrieval performance of different embedding models. Table 6: Generation accuracy of LLMs.\\n\\nchunks is not satisfactory, with the state-of-the- art GPT-4 model achieving only 0.56 accuracy. This is expected, because the retrieval component falls short in retrieving relevant evidences from the knowledge base. Second, even when we provide the LLM with the ground-truth evidences, we can see that the response accuracy is far from being per- fect. Open source LLM such as Llama02-70B and Mixtral-8x7B only achieve an accuracy of 0.32 and 0.36 respectively. GPT-4 achieves strong reason- ing capability with an accuracy of 0.89, followed by the second-based LLM Google-PaLM with an accuracy of 0.74.\\n\\nRetrieved Chunk — lm Mixtral-8x7B mmm PT4+ comparison temporal inference 0 BY) ry @ ro) ‘Accuracy (%) Ground-truth Chunk round-truth Chun bem Mintral- 8x78 comparison mm GPr4 temporal inference 0 20 40 60 80 100 Accuracy (%)\\n\\nFigure 3: Generation accuracy for different query types.\\n\\nthe chronological order of events, which is crucial for answering temporal queries where timing is a key factor. Taken together, this experiment demon- strates that there is still room for improvement in the reasoning capabilities of LLMs, particularly those that are open-source, for multi-hop queries.\\n\\nFigure 3 shows the detailed results of different query types for GPT-4 and Mixtral-8x7B-instruct. Both models show relatively high robustness on null queries, meaning they are generally good at determining when a query cannot be answered based on the retrieved text. This is encouraging be- cause one benefit of RAG is to mitigating the LLM hallucination issue by augmenting LLM with re- trieval knowledge. However, Mixtral-8x7B model performs significantly worse than the GPT-4 in comparison and temporal queries. Upon reviewing the incorrect responses, we find that Mixtral-8x7B fails to accurately handle logical negation, leading to misinterpretation of statements and thus a low performance in the comparison queries. In addi- tion, Mixtral-8x7B often fails to correctly identify 4.3 Other Use Cases\\n\\nBeyond embedding models and LLM generation, there are other areas worth exploring. For exam- ple, query decomposition is a widely utilized tech- nique in RAG frameworks, such as LLamaIndex. This process involves breaking down the query into smaller segments; it targets a single document for retrieval and integrates the information subse- quently, thereby potentially enhancing retrieval ac- curacy. Another advanced and promising approach involves building LLM-based agents that can au- tomatically plan and execute multi-hop queries, such as AutoGPT (Gravitas, 2023). Another area of interest is the hybrid retrieval approach, which combines keyword and embedding matching tech-\\n\\nniques. We believe that there are many potential areas for enhancing RAG’s performance on multi- hop queries, and the curated dataset MultiHop- RAG can be a valuable resource to the community.\\n\\n5 Related Work RAG Evaluation: As RAG systems gain increas-\\n\\ning popularity, a variety of RAG benchmarking datasets and evaluation tools have been developed. For instance, RGB (Chen et al., 2023) and RE- CALL (Liu et al., 2023) evaluate the performance of LLMs in generating responses for RAG systems under conditions involving noisy, integrative, and counterfactual queries. However, both datasets pri- marily focus on evaluating the generation aspect of RAG systems without specifically addressing their retrieval accuracy. In addition, recent ad- vancements have been made in automated RAG evaluation tools, such as ARES (Saad-Falcon et al., 2023) and RAGAS (Es et al., 2023). These tools utilize LLMs to automatically assess the quality of RAG generation, yet they do not introduce bench- marking datasets. Our work introduces one of the first RAG benchmarking datasets, consisting of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associ- ated supporting evidence, thereby complementing existing RAG evaluations.\\n\\nRetrieval datasets: Apart from the context of RAG, several benchmarking datasets exist for in- formation retrieval evaluation. The FEVER (Fact Extraction and VERification) dataset, for instance, contains claims classified as Supported, Refuted, or NotEnoughInfo by the given Wikipedia article (Thorne et al., 2018). Similarly, the SciFact dataset comprises scientific claims paired with evidence- containing abstracts (Wadden et al., 2020). How- ever, the claims in both datasets are single-hop statements, and the supporting evidence is from one single article, in contrast to the multi-hop queries discussed in this paper. Another dataset, HoVer, involves claims that require extracting and reason- ing from multiple Wikipedia articles (Jiang et al., 2020). However, unlike our dataset, HoVer focuses solely on classifying claims as either supported or not supported by the articles without evaluating an LLM generation step. Moreover, in HoVer, the Wikipedia articles from which evidence is drawn are given for claim verification, which is signifi- cantly different from our setting, where relevant\\n\\npieces of evidence need to be extracted from a\\n\\nlarge knowledge base. Separately, (Kamalloo et al., 2023) evaluates a range of commercial embedding APIs for information retrieval, but this evaluation is not contextualized within the framework of RAG systems either.\\n\\nMulti-document QA datasets: Question- answering (QA) is a fundamental task in NLP, and several popular benchmarks, such as HotpotQA (Yang et al., 2018), MultiRC (Khashabi et al., 2018), and 2WikiMultiHopQA (Ho et al., 2020), aim to achieve QA from multiple sources of documents. This task is similar to our multi-hop query RAG task, as both involve reasoning from multiple sources of information. However, these datasets primarily focus on assessing a model’s reasoning skills, and they do not emphasize the retrieval of evidence from a knowledge base. Additionally, their primary data sources Wikipedia, significantly overlap with the training data of most existing LLMs. If we use these sources for benchmarking RAG systems, there is a potential concern that LLM responses might rely on training knowledge rather than reasoning from the retrieved knowledge base. 6 Conclusion\\n\\nIn this work, we introduce MultiHop-RAG, a novel and unique dataset designed for queries that re- quire retrieval and reasoning from multiple pieces of supporting evidence. These types of multi-hop queries represent user queries commonly encoun- tered in real-world scenarios. MultiHop-RAG con- sists of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associated supporting evidence. This paper details the creation process of MultiHop-RAG, em- ploying a hybrid approach that integrates human effort with GPT-4. Additionally, we explore two use cases of MultiHop-RAG in the benchmarking of RAG systems, thereby highlighting the potential applications of this dataset. By publicly releas- ing MultiHop-RAG, we aim to provide a valuable resource to the community, contributing to the ad- vancement and benchmarking of RAG systems.\\n\\nLimitations\\n\\nThis work has several limitations that can be im- proved in future research. First, our ground truth answers are restricted to simple responses such as “yes\", “no\", entity names, or temporal indicators like “before\" or “after\" to facilitate the use of a\\n\\nstraightforward accuracy metric for evaluating gen- eration performance. Future work could consider allowing free text as answers and employing more sophisticated metrics to assess generation quality. Second, the current dataset limits supporting ev- idence for a query to a maximum of four pieces. Future work can extend the dataset by including queries that require retrieving and reasoning from even more evidence. Lastly, while our experiments utilize a basic RAG framework using LlamaIndex, future work could involve evaluating the answering of multi-hop queries using more advanced RAG frameworks or LLM-agent frameworks. References\\n\\nAnthropic. 2023. Claude 2.1 (May version). https: //api.anthropic.com/v1/messages. Claude 2.1.\\n\\nAkari Asai, Sewon Min, Zexuan Zhong, and Danqi Chen. 2023. Retrieval-based language models and applications. In Proceedings of the 61st Annual Meet- ing of the Association for Computational Linguistics (Volume 6: Tutorial Abstracts), pages 41–46.\\n\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff- mann, Trevor Cai, Eliza Rutherford, Katie Milli- can, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, Diego De Las Casas, Aurelia Guy, Jacob Menick, Roman Ring, Tom Hennigan, Saffron Huang, Loren Mag- giore, Chris Jones, Albin Cassirer, Andy Brock, Michela Paganini, Geoffrey Irving, Oriol Vinyals, Simon Osindero, Karen Simonyan, Jack Rae, Erich Elsen, and Laurent Sifre. 2022. Improving language models by retrieving from trillions of tokens. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 2206–2240. PMLR.\\n\\nHarrison Chase. 2022. LangChain.\\n\\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun. 2023. Benchmarking large language models in retrieval-augmented generation.\\n\\nShahul Es, Jithin James, Luis Espinosa-Anke, and Steven Schockaert. 2023. Ragas: Automated evalua- tion of retrieval augmented generation.\\n\\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling large language models to generate text with citations.\\n\\nGoogle. 2023. PaLM 2 (May version). https://generativelanguage.googleapis. com/v1beta2/models/. Chat-bison-002.\\n\\nSignificant Gravitas. 2023. Autogpt. https://github. com/Significant-Gravitas/AutoGPT.\\n\\nMichael Günther, Jackmin Ong, Isabelle Mohr, Alaed- dine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, and Han Xiao. 2023. Jina embeddings 2: 8192- token general-purpose text embeddings for long doc- uments.'),\n",
       " Document(metadata={'doc_id': 'a5cdaa51-39b4-42fe-bc76-e19fb729c37b', 'paper_id': 'a5cdaa51-39b4-42fe-bc76-e19fb729c37b', 'source': '2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries.pdf', 'type': 'text'}, page_content='4\\n\\n2024\\n\\n2\\n\\n0\\n\\n2\\n\\nn a J 7 2 ] L C . s c [ 1 v 1 9 3 5 1 . 1 0 4 2\\n\\n:\\n\\nv\\n\\ni\\n\\nX\\n\\nr\\n\\na\\n\\nMultiHop-RAG: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries\\n\\nYixuan Tang and Yi Yang Hong Kong University of Science and Technology {yixuantang,imyiyang}@ust.hk\\n\\nAbstract\\n\\nRetrieval-augmented generation (RAG) aug-\\n\\nments large language models (LLM) by re- trieving relevant knowledge, showing promis- ing potential in mitigating LLM hallucinations and enhancing response quality, thereby facil- itating the great adoption of LLMs in prac- tice. However, we find that existing RAG sys- tems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence. Furthermore, to our knowledge, no existing RAG benchmarking dataset focuses on multi- hop queries. In this paper, we develop a novel dataset, MultiHop-RAG, which consists of a knowledge base, a large collection of multi- hop queries, their ground-truth answers, and the associated supporting evidence. We detail the procedure of building the dataset, utiliz- ing an English news article dataset as the un- derlying RAG knowledge base. We demon- strate the benchmarking utility of MultiHop- RAG in two experiments. The first experiment compares different embedding models for re- trieving evidence for multi-hop queries. In the second experiment, we examine the capabili- ties of various state-of-the-art LLMs, includ- ing GPT-4, PaLM, and Llama2-70B, in rea- soning and answering multi-hop queries given the evidence. Both experiments reveal that ex- isting RAG methods perform unsatisfactorily in retrieving and answering multi-hop queries. We hope MultiHop-RAG will be a valuable re- source for the community in developing effec- tive RAG systems, thereby facilitating greater adoption of LLMs in practice. The MultiHop- RAG and implemented RAG system is publicly available at https://github.com/yixuantt/ MultiHop-RAG/.\\n\\nIntroduction\\n\\nThe emergence of large language models (LLMs), such as ChatGPT, has fostered a wide range of inno- vations, powering intelligent chatbots and other nat- ural language processing (NLP) applications (Ope-\\n\\nMulti-Documents Which company among \\' | Google, Apple, and Nvidia Google Chunk]| ! | reported the largest profit 4 \\' | margins in their third- nae Chunk]) (1 1 | quarter reports for 2023? q Database” || Nvidia -—[Chunk]} ) |\\n\\nFigure 1: RAG with multi-hop query.\\n\\nnAI, 2023). One promising use case is Retrieval- Augmented Generation (RAG) (Asai et al., 2023), which optimizes the output of a large language model by referencing an external knowledge base outside of the LLM training data sources before generating a response. RAG improves LLM’s re- sponse (Borgeaud et al., 2022) and also mitigates the occurrence of hallucinations, thereby enhancing the models’ credibility (Gao et al., 2023). LLM- based frameworks, such as LlamaIndex (Liu, 2022) and LangChain (Chase, 2022), specialize in sup- porting RAG pipelines.\\n\\nIn real-world Retrieval-Augmented Generation (RAG) applications, a user’s query often necessi- tates retrieving and reasoning over evidence from multiple documents, a process known as multi-hop query. For instance, consider financial analysis us- ing a database of financial reports. A financial ana- lyst might query, Which company among Google, Apple, and Nvidia reported the largest profit mar- gins in their third-quarter reports for 2023? or inquire about a specific company’s performance over time, such as How does Apple’s sales trend look over the past three years? These queries re- quire evidence from multiple documents to formu- late an answer. Due to the multifaceted nature of such queries, involving information from various sources, traditional similarity matching methods like cosine similarity between query and financial Answer\\n\\nYes\\n\\nTable 1: An example of a multi-hop query, including supporting evidence from two news articles, the paraphrased claim, the bridge-topic and bridge-entity, and the corresponding answer.\\n\\nreport chunk embeddings might not yield optimal results. We demonstrate this multi-hop retrieval process in Figure 1.\\n\\nHowever, existing RAG benchmarks, such as RGB (Chen et al., 2023) and RECALL (Liu et al., 2023), mainly evaluate a simple case where the an- swer of a query can be retrieved and solved using one single piece of evidence. None of these bench- marks assess the retrieval and reasoning capability of LLMs for complex multi-hop queries. To ad- dress this gap and make RAG benchmarking more closely resemble real-world scenarios, in this paper, we introduce MultiHop-RAG. To our knowledge, MultiHop-RAG is one of the first RAG datasets focusing specifically on multi-hop queries.\\n\\nBased on the RAG queries commonly encoun- tered in real-world scenarios, we first categorize multi-hop queries into four types: Inference query, Comparison query, Temporal query, and Null query. The first three types — Inference, Com- parison, and Temporal — require the retrieval and analysis of evidence from multiple sources, encom- passing tasks like inferring relationships, compar- ing data points, and sequencing events over time. The Null query represents a scenario where the query cannot be derived from the knowledge base. This category is crucial for assessing whether an LLM might hallucinate an answer to a multi-hop query when the retrieved text lacks relevance.\\n\\nWe construct our RAG knowledge base using a collection of news articles. Using GPT-4 as a data generator, we then take an extensive procedure to construct a diverse set of multi-hop queries, each requiring the retrieval and reasoning over multiple documents. An example of query construction is shown in Table 1. First, we begin by extracting\\n\\nfactual sentences from each news article as evi-\\n\\ndence. For example, an extracted piece of evidence from an article may state: “Back then, just like today, home prices had boomed for years before Fed officials were ultimately forced to hike interest rates aggressively in an attempt to fight inflation.” Second, we input each evidence piece into GPT-4, prompting it to rephrase the evidence into a claim. This claim is clarified with a disambiguated topic and entity. For instance, GPT-4 might rephrase the aforementioned evidence into: “Federal Reserve officials were forced to aggressively hike interest rates to combat inflation after years of booming home prices”, identifying “Interest rate hikes to combat inflation” as the topic and “Federal Re- serve” as the entity. These topics and entities act as bridges for constructing multi-hop queries, known as bridge-topic or bridge-entity. Next, we use GPT- 4 to generate specific multi-hop queries related to the same bridge-topic or bridge-entity, accompa- nied by the correct answers. Lastly, we undertake a validation step to ensure the data quality.\\n\\nWe demonstrate the benchmarking capabilities of MultiHop-RAG using two experiments, utilizing a RAG system implemented with LlamaIndex (Liu, 2022). The first experiment involves a comparison of different embedding models for retrieving rele- vant evidence for multi-hop queries. In the second experiment, we assess the reasoning and answering abilities of various state-of-the-art LLMs, including GPT-4, GPT-3.5, PaLM, Claude-2, Llama2-70B, and Mixtral-8x7B, for multi-hop queries when re- trieved text is provided. The results from both ex- periments indicate that the current RAG implemen- tations are inadequate for effectively retrieving and answering multi-hop queries. We publicly release\\n\\nthis challenging MultiHop-RAG dataset and hope it will be a valuable resource for the community in de- veloping and benchmarking RAG systems, thereby unleashing the great potential of generative AI in practice. 2 RAG with multi-Hop queries\\n\\n2.1 Retrieval-augmented Generation (RAG)\\n\\nIn an RAG application, we utilize an external cor- pus, denoted as D, which comprises multiple docu- ments and serves as the knowledge base. Each doc- ument within this corpus, represented as di ∈ D, is segmented into a set of chunks.These chunks are then transformed into vector representations using an embedding model and stored in an embedding database. Given a user query q, the system typi- cally retrieves the top-K chunks that best match the query. These chunks constitute the retrieval set for query q, represented as Rq = {r1, r2, ..., rK}. The retrieved chunks, combined with the query and an optional prompt, are then fed into an LLM to generate a final answer, following the format: LLM(q, Rq, prompt) → answer.\\n\\n2.2 Multi-Hop Query\\n\\nWe define a multi-hop query as one that requires retrieving and reasoning over multiple pieces of supporting evidence to provide an answer. In other words, for a multi-hop query q, the chunks in the retrieval set Rq collectively provide an answer to q. For example, the query \"Which company among Google, Apple, and Nvidia reported the largest profit margins in their third-quarter reports for 2023?\" requires 1) retrieving relevant pieces of evidence related to profit margins from the reports of the three companies; 2) generating an answer by comparing and reasoning from the multiple pieces of retrieved evidence. This differs from a single- hop query such as \"What is Google’s profit margin in the third-quarter reports for 2023,\" where the answer can be directly derived from a single piece of evidence.\\n\\nBased on the queries commonly used in real- world RAG systems, we identify four types of multi-hop queries. For each type, we present a hypothetical query within the context of a financial RAG system, where the knowledge base consists of a collection of annual reports.\\n\\nInference query: For such a query q, the answer is deduced through reasoning from the retrieval set Rq. An example of an inference query might\\n\\nbe: Which report discusses the supply chain risk of Apple, the 2019 annual report or the 2020 annual report?\\n\\nComparison query: For such a query q, the an- swer requires a comparison of evidence within the retrieval set Rq. For instance, a comparison query might ask: Did Netflix or Google report higher revenue for the year 2023?\"\\n\\nTemporal query: For such a query q, the answer requires an analysis of the temporal information of the retrieved chunks. For example, a temporal query may ask: Did Apple introduce the AirTag tracking device before or after the launch of the 5th generation iPad Pro?\\n\\nNull query: For such as query q, the answer cannot be derived from the retrieved set Rq. We include the null query to assess the generation quality, es- pecially regarding the issue of hallucination. For a null query, even though a retrieved set is provided, an LLM should produce a null response instead of hallucinating an answer. For example, assum- ing ABCD is a non-existent company, a null query might ask: What are the sales of company ABCD as reported in its 2022 and 2023 annual reports? 2.3 Evaluation Metrics\\n\\nAn RAG system handling multi-hop queries can be assessed from two key aspects: retrieval evaluation and generation evaluation.\\n\\nRetrieval Evaluation: Evidently, the quality of the retrieval set Rq determines the final genera- tion quality. We compare the retrieved set with the ground truth evidence associated with each query, except for the null queries, as they have no evidence to derive from. Assuming the top- K chunks are retrieved, i.e., |Rq| = K, we use retrieval evaluation metrics including Mean Aver- age Precision at K (MAP@K), Mean Reciprocal Rank at K (MRR@K), and Hit Rate at K (Hit@K). MAP@K measures the average top-K retrieval pre- cision across all queries. MRR@K calculates the average of the reciprocal ranks of the first relevant chunk for each query, considering the top-K re- trieved set. Hit@K metric measures the fraction of evidence that appears in the top-K retrieved set.\\n\\nResponse Evaluation: Since the multi-hop query requires reasoning over multiple pieces of retrieved chunks, we can also evaluate the reason- ing capability of the LLM by comparing the LLM response with the ground truth answer of the query.\\n\\n| Download Dataset |! Dataset Collection | ---6 —————_ | | Select News | Extraction | Select Sentences | | Claim Generation . . ooo Claim Generation | --0| [pridge-Entity Generation | | Bridge-Topic Generation OO = Query and Answer all Inference || Comparison} | Generation ~ | Null Temporal ji | Manually Review | Quality Assurance +--Q = | | GPT-4 Review I\\n\\nFigure 2: MultiHop-RAG Construction Pipeline.\\n\\n3 A Benchmarking Dataset: MultiHop-RAG\\n\\nIn this section, we provide detailed information on the construction of the MultiHop-RAG dataset. Specifically, we describe the process of creating a set of multi-hop queries, along with the correspond- ing ground truth evidence sets and answers derived from a collection of news articles. 3.1 MultiHop-RAG Construction\\n\\nStep 1: Dataset Collection. We download a news dataset using the mediastack API 1, a REST API in- terface delivering worldwide news data. The news data source comprises various English-language websites covering a range of news categories: en- tertainment, business, sports, technology, health, and science. To mimic real-world RAG scenarios, where the knowledge base data, such as an enter- prise’s internal data, may differ from the LLMs’ training data, we select news articles published from September 26, 2023, to December 26, 2023. This timeframe extends beyond the knowledge cut- off of some widely-used LLMs, including Chat- GPT and LLaMA, as of the time of writing. This selection also helps in teasing out the possibility of the underlying LLM having been exposed to these news articles. We only keep articles with a token length greater than or equal to 1,024. Every\\n\\n1https://mediastack.com/\\n\\nnews article is paired with metadata, including the title, publish date, author, category, URL, and news source.\\n\\nStep 2: Evidence Extraction. For each article, we extract factual or opinion sentences using a trained language model 2. These factual sentences are later used as evidence for answering multi-hop queries. We retain only those news articles containing ev- idence that may have overlapping keywords with other news articles. This allows us to later create multi-hop queries where the answer’s evidences are drawn from multiple sources.\\n\\nStep 3: Claim, Bridge-Entity, Bridge-Topic Gen- eration. Our goal is to use GPT-4 to automatically generate high-quality multi-hop queries using the evidence set. However, the raw evidence obtained from Step 2 is not ideal for query generation due to inconsistency in linguistic structure. For exam- ple, some pieces of evidence use pronouns to refer to subjects and lack the actual entity in the text. To address this, we employ GPT-4 to paraphrase the evidence, which we refer to as claims, given the original evidence and its context. To ensure consistency between the generated claim and the evidence, we further perform fact-checking using the UniEval (Zhong et al., 2022) framework to ver- ify the alignment between the evidence and claim. Appendix A presents the prompt used for GPT-4 for claim generation.\\n\\nBridge-Entity and Bridge-Topic: The shared en- tity or topic across pieces of evidence is referred to as the bridge-entity or bridge-topic. These bridge- entities or bridge-topics can be used to link dif- ferent pieces of evidence from which a multi-hop query’s answer is derived. For example, in a claim such as “Google reports its third-quarter results for 2023, showcasing a detailed overview of its finan- cial performance, including revenue growth, profit margins”, the term profit margin can be viewed as a bridge-topic and the term Google can be viewed as a bridge-entity that links the different pieces of evidence. We prompt GPT-4 to identify the bridge- entity and bridge-topic for each claim. Appendix A also presents the prompt used for GPT-4 for bridge generation.\\n\\nStep 4: Query and Answer Generation. In this step, we leverage the bridge-entity or bridge-topic to generate multi-hop queries. Specifically, we first group the claims having the same bridge-entity or\\n\\n2https://huggingface.co/lighteternal/fact-or-opinion-xlmr- el\\n\\nbridge-topic into a claim set. We restrict the claim set to have at least two claims but no more than four claims. For each type of query, we feed the claim set to GPT-4 and prompt it with an instruction to generate a query with information from each claim. Below, we explain the specifications for different multi-hop query types. In the construction of each query, we also include the source of the news article where the supporting evidence is associated with to mimic real-world RAG scenarios. Appendix A presents the prompts used for GPT-4 for query generation.\\n\\nInference Query: These queries are formulated by synthesizing the various characterizations of the bridge-entity across multiple claims, with the final answer being the identification of the entity itself.\\n\\nComparison Query: These queries are struc- tured to compare the similarities and differences related to the bridge entity or topic. The resultant answer to such queries is typically a definitive “yes” or “no”, based on the comparison.\\n\\nTemporal Query: These queries explore the temporal ordering of events across different points in time. The answer to such queries is typically a “yes” or “no” or a single temporal indicator word like “before” or “after”.')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Reranking and Document Selection (Leave this to the MultiModal Retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Augmented Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to write the prompt. It will basically instruct the LLM to generate result based on the {question} and the {context}.\n",
    "\n",
    "The context is inputted from the retrieved documents from p previous step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "QA_RAG = \"SIMPLE_QUESTION_ANSWER_RAG\"\n",
    "\n",
    "MM_QA_RAG = \"MULTIMODAL_QUESTION_ANSWER_RAG\"\n",
    "\n",
    "prompt_type = {\n",
    "    \"QA_RAG\" : \"SIMPLE_QUESTION_ANSWER_RAG\",\n",
    "    \"MM_QA_RAG\" : \"MULTIMODAL_QUESTION_ANSWER_RAG\",\n",
    "}\n",
    "\n",
    "simple_rag_template = \"\"\"\n",
    "Answer the question based on the context below. \n",
    "If you can't answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "multimodal_rag_template = \"\"\"\n",
    "To define the new Prompt.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "def initPrompt(type) -> ChatPromptTemplate:\n",
    "    #default\n",
    "    prompt = ChatPromptTemplate.from_template(simple_rag_template)\n",
    "    if type == prompt_type[\"QA_RAG\"]: \n",
    "        prompt = ChatPromptTemplate.from_template(simple_rag_template)\n",
    "    if type == prompt_type[\"MM_QA_RAG\"]: \n",
    "        prompt = ChatPromptTemplate.from_template(multimodal_rag_template)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now send the augmented prompt to instruct a LLM generating response to user's query. The response is finally parsed for readable. \n",
    "In this experiment, we use OpenAI model GPT3.5-Turbo. \n",
    "\n",
    "Note: There are many options for LLMs selection, from public to private, from simple to advance. Privacy, performance and quality should be considered to trade off. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. QA Generation \n",
    "Using LLM to generation response to augmented query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG chain: MM openai embedding + Gemini Flash LLM\n",
    "chain_openai_gemini = multi_modal_rag_chain(retriever=retriever_mv_openai,model=model_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG chain: MM Google embedding + Gemini Flash LLM\n",
    "chain_google_gemini = multi_modal_rag_chain(retriever=retriever_mv_google,model=model_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG chain: MM Google embedding + Gemini Flash LLM\n",
    "chain_hugg_gemini = multi_modal_rag_chain(retriever=retriever_mv_hugg,model=model_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "response1 = chain_openai_gemini.invoke(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2 = chain_google_gemini.invoke(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "response3 = chain_hugg_gemini.invoke(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The table shows that GPT-4 has an accuracy of 0.56 for retrieved chunks and 0.89 for ground-truth chunks, while Mixtral-8x7B-Instruct has an accuracy of 0.32 for retrieved chunks and 0.36 for ground-truth chunks. Therefore, GPT-4 outperforms Mixtral on both retrieved and ground-truth chunks. \\n\\nHowever, it's important to note that this is just one dataset and more research is needed to generalize the findings. Also, the provided text mentions that existing RAG methods are inadequate for effectively retrieving and answering multi-hop queries. This suggests that both models may have limitations in handling complex queries, and further improvements are needed.  \\n\\nHere are some research advice based on the information provided:\\n\\n1. **Focus on Multi-hop Queries:** The research highlights the importance of developing RAG systems capable of handling multi-hop queries. It is crucial to design models that can effectively retrieve and reason over multiple pieces of evidence.\\n2. **Benchmarking and Dataset Development:** The MultiHop-RAG dataset provides a valuable resource for benchmarking RAG systems, especially for multi-hop queries. More research on developing and evaluating RAG models on this dataset is encouraged.\\n3. **Explore Different Retrieval Techniques:** The research suggests that existing retrieval methods may not be optimal for multi-hop queries. Exploring alternative approaches like graph-based retrieval or cross-attention mechanisms for retrieving relevant evidence could be beneficial.\\n4. **Improve Reasoning Capabilities:** Enhancing the reasoning abilities of LLMs for handling multi-hop queries is critical. This could involve incorporating techniques like knowledge graph reasoning, graph neural networks, or attention mechanisms to better extract and synthesize information from multiple sources.\\n5. **Address Hallucination:** The null query category in MultiHop-RAG is essential for evaluating the hallucination problem in RAG systems. Further research on mitigating hallucination in LLMs, particularly in the context of multi-hop queries, is highly recommended.\\n6. **Compare Different RAG Systems:** The research compares different RAG systems, including GPT-4, PaLM, and Llama2-70B. It is important to conduct more extensive comparisons of various RAG systems on diverse datasets to identify the strengths and weaknesses of each approach.\\n\\nBy focusing on these research directions, we can advance the development of robust and effective RAG systems capable of handling complex multi-hop queries, leading to more reliable and informative AI-powered applications.\""
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The research shows that GPT-4 outperforms Mixtral-8x7B in most areas, especially in comparison and temporal queries. While both models are relatively good at identifying null queries, Mixtral-8x7B struggles with logical negation and chronological order, leading to lower accuracy in comparison and temporal queries. GPT-4 demonstrates strong reasoning capabilities and accuracy, while Mixtral-8x7B's performance is significantly lower. \\n\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The chart shows that GPT-4 performs significantly better than Mixtral-8x7B in temporal and comparison queries. This is likely due to Mixtral-8x7B's struggles with logical negation and identifying the chronological order of events. However, both models perform well on null queries, demonstrating their ability to determine when a query cannot be answered based on the retrieved text. Overall, GPT-4 shows stronger reasoning capabilities for multi-hop queries than Mixtral-8x7B. \\n\\nThe table confirms this finding, with GPT-4 achieving an accuracy of 0.89 on ground-truth chunks compared to Mixtral-8x7B's accuracy of 0.36. This further emphasizes GPT-4's superior performance in handling multi-hop queries, particularly in terms of reasoning and generating accurate responses. \\n\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG chain: MM Google embedding + gpt-4o\n",
    "chain_google_gpt = multi_modal_rag_chain(retriever=retriever_mv_google,model=model_gpt4o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided data and charts, here's a summary of the performance comparison between GPT-4 and Mixtral-8x7B:\\n\\n1. **Overall Accuracy**:\\n   - GPT-4 shows significantly higher accuracy compared to Mixtral-8x7B in both retrieved and ground-truth chunks.\\n   - For retrieved chunks, GPT-4 achieves an accuracy of 56%, while Mixtral-8x7B achieves 32%.\\n   - For ground-truth chunks, GPT-4 achieves an accuracy of 89%, while Mixtral-8x7B achieves 36%.\\n\\n2. **Query Type Performance**:\\n   - **Inference Queries**: GPT-4 performs much better than Mixtral-8x7B, especially in inference tasks, indicating stronger reasoning capabilities.\\n   - **Comparison and Temporal Queries**: GPT-4 outperforms Mixtral-8x7B, particularly in handling logical negation and understanding temporal sequences.\\n   - **Null Queries**: Both models perform well, indicating they can recognize when a query cannot be answered with the given information.\\n\\n3. **Strengths and Weaknesses**:\\n   - GPT-4 demonstrates strong reasoning and inference capabilities, making it more suitable for complex multi-hop queries.\\n   -\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response4 = chain_google_gpt.invoke(user_query)\n",
    "response4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG chain: MM Google embedding + gpt-4o\n",
    "chain_google_llama = multi_modal_rag_chain(retriever=retriever_mv_google,model=model_llama3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response5 = chain_google_llama.invoke(user_query)\n",
    "response5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Retrieve Topic and Relevant Articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_catalog = pd.DataFrame()\n",
    "with open('document_catalog.pickle', 'rb') as pkl_file:\n",
    "        doc_catalog = pickle.load(pkl_file)\n",
    "articles = list(set([a.metadata[\"paper_id\"] for a in relevant_docs_mvr]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a5cdaa51-39b4-42fe-bc76-e19fb729c37b']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = list(set([doc_catalog[\"topic\"].loc[i] for i, docid in enumerate(list(doc_catalog[\"docid\"])) if docid in articles]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MultiHop-RAG dataset, Retrieval-Augmented Generation (RAG), multi-hop queries, knowledge base, evidence retrieval, answer generation, benchmarking, LLM evaluation, RAG system challenges']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "model = SentenceTransformer('nq-distilbert-base-v1')\n",
    "def topic_match(topic, topics):\n",
    "    encoded_topics = []\n",
    "    for t in topics:\n",
    "        encoded_topics.append(model.encode(t))\n",
    "    encoded_topic = model.encode(topic)\n",
    "    for t in encoded_topics:\n",
    "        if util.pytorch_cos_sim(t, encoded_topic)[0][0].item() > 0.5:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_articles = [doc_catalog[\"filename\"].loc[i][:-4] for i, docid, topic in zip(range(len(doc_catalog.index)),list(doc_catalog[\"docid\"]),list(doc_catalog[\"topic\"])) if docid not in articles and topic_match(topic, topics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2407.21059v1.Modular_RAG__Transforming_RAG_Systems_into_LEGO_like_Reconfigurable_Frameworks',\n",
       " '2408.02545v1.RAG_Foundry__A_Framework_for_Enhancing_LLMs_for_Retrieval_Augmented_Generation',\n",
       " '2410.20299v1.EACO_RAG__Edge_Assisted_and_Collaborative_RAG_with_Adaptive_Knowledge_Update']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Retrieve Article Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_summary = []\n",
    "topic_articles_summary = []\n",
    "if articles:\n",
    "    articles_summary = [doc_catalog[\"summary\"].loc[i] for i, docid in enumerate(list(doc_catalog[\"docid\"])) if docid in articles]\n",
    "if topic_articles:\n",
    "    topic_articles_summary = [doc_catalog[\"summary\"].loc[i] for i, filename in enumerate(list(doc_catalog[\"filename\"])) if filename[:-4] in topic_articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Generate the final response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Query: What is the pperformance of GPT-4 vs Mixtral?\n",
      "The answer: Based on the provided data and charts, here's a summary of the performance comparison between GPT-4 and Mixtral-8x7B:\n",
      "\n",
      "1. **Overall Accuracy**:\n",
      "   - GPT-4 shows significantly higher accuracy compared to Mixtral-8x7B in both retrieved and ground-truth chunks.\n",
      "   - For retrieved chunks, GPT-4 achieves an accuracy of 56%, while Mixtral-8x7B achieves 32%.\n",
      "   - For ground-truth chunks, GPT-4 achieves an accuracy of 89%, while Mixtral-8x7B achieves 36%.\n",
      "\n",
      "2. **Query Type Performance**:\n",
      "   - **Inference Queries**: GPT-4 performs much better than Mixtral-8x7B, especially in inference tasks, indicating stronger reasoning capabilities.\n",
      "   - **Comparison and Temporal Queries**: GPT-4 outperforms Mixtral-8x7B, particularly in handling logical negation and understanding temporal sequences.\n",
      "   - **Null Queries**: Both models perform well, indicating they can recognize when a query cannot be answered with the given information.\n",
      "\n",
      "3. **Strengths and Weaknesses**:\n",
      "   - GPT-4 demonstrates strong reasoning and inference capabilities, making it more suitable for complex multi-hop queries.\n",
      "   -\n",
      "\n",
      "You can find the details of the answer from the following articles\n",
      "\n",
      "Article 1: 2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries\n",
      "Article Summary:\n",
      "This paper introduces MultiHop-RAG, a novel dataset designed for benchmarking Retrieval-Augmented Generation (RAG) systems on multi-hop queries. The dataset contains a knowledge base of news articles, multi-hop queries requiring reasoning over multiple pieces of evidence, their ground-truth answers, and the associated supporting evidence. The authors demonstrate the dataset's utility by evaluating different embedding models and state-of-the-art LLMs, revealing the challenges in retrieving and answering multi-hop queries and highlighting the need for further research in developing effective RAG systems.\n",
      "\n",
      "You seem interested in the topics: MultiHop-RAG dataset, Retrieval-Augmented Generation (RAG), multi-hop queries, knowledge base, evidence retrieval, answer generation, benchmarking, LLM evaluation, RAG system challenges \n",
      "You may be interested in other articles in those topics below:\n",
      "\n",
      "Article 1: 2407.21059v1.Modular_RAG__Transforming_RAG_Systems_into_LEGO_like_Reconfigurable_Frameworks\n",
      "Article Summary:\n",
      "This paper introduces Modular RAG, a new framework for Retrieval-Augmented Generation (RAG) systems that tackles the increasing complexity of these systems. Modular RAG decomposes RAG systems into independent modules and operators, allowing for flexible configuration and customization. The paper identifies common RAG flow patterns and analyzes existing methods within the Modular RAG framework, demonstrating its compatibility with new developments in RAG technology.\n",
      "\n",
      "Article 2: 2408.02545v1.RAG_Foundry__A_Framework_for_Enhancing_LLMs_for_Retrieval_Augmented_Generation\n",
      "Article Summary:\n",
      "This paper introduces RAG Foundry, an open-source framework for enhancing large language models (LLMs) for retrieval-augmented generation (RAG) tasks.  The framework provides a unified workflow for data creation, training, inference, and evaluation, enabling researchers and practitioners to easily experiment with various RAG techniques and configurations. The authors demonstrate the effectiveness of RAG Foundry by augmenting and fine-tuning Llama-3 and Phi-3 models, achieving consistent improvements across three knowledge-intensive datasets.\n",
      "\n",
      "Article 3: 2410.20299v1.EACO_RAG__Edge_Assisted_and_Collaborative_RAG_with_Adaptive_Knowledge_Update\n",
      "Article Summary:\n",
      "This paper proposes EACO-RAG, an edge-assisted and collaborative retrieval-augmented generation (RAG) system that tackles the scalability challenges of traditional RAG systems. By distributing knowledge across edge nodes, EACO-RAG reduces communication overhead and delay, while adaptive knowledge updates based on user behavior enhance response accuracy. The system uses a multi-armed bandit framework to dynamically select between edge and cloud resources, minimizing costs while maintaining performance.\n"
     ]
    }
   ],
   "source": [
    "print(\"Your Query:\", user_query)\n",
    "print(\"The answer:\", response4)\n",
    "if articles:\n",
    "    print(\"\\nYou can find the details of the answer from the following articles\")\n",
    "    for i in range(len(articles)):\n",
    "        print(\"\\nArticle \"+str(i+1)+\": \"+ doc_catalog[doc_catalog[\"docid\"]==articles[i]][\"filename\"].loc[0][:-4])\n",
    "        print(\"Article Summary:\\n\"+articles_summary[i])\n",
    "if topic_articles:\n",
    "    print(\"\\nYou seem interested in the topics:\", \", \".join(topics),\"\\nYou may be interested in other articles in those topics below:\")\n",
    "    for i in range(len(topic_articles)):\n",
    "        print(\"\\nArticle \"+str(i+1)+\": \"+topic_articles[i])\n",
    "        print(\"Article Summary:\\n\"+topic_articles_summary[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Retrieval and Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_catalog = pd.DataFrame()\n",
    "with open('document_catalog.pickle', 'rb') as pkl_file:\n",
    "    doc_catalog = pickle.load(pkl_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer(retriever, model, query):\n",
    "    chain = multi_modal_rag_chain(retriever=retriever,model=model)\n",
    "    response = chain.invoke(query)\n",
    "    articles = list(set([a.metadata[\"paper_id\"] for a in relevant_docs_mvr]))\n",
    "    topics = list(set([doc_catalog[\"topic\"].loc[i] for i, docid in enumerate(list(doc_catalog[\"docid\"])) if docid in articles]))\n",
    "    topic_articles = [doc_catalog[\"filename\"].loc[i][:-4] for i, docid, topic in zip(range(len(doc_catalog.index)),list(doc_catalog[\"docid\"]),list(doc_catalog[\"topic\"])) if docid not in articles and topic_match(topic, topics)]\n",
    "    articles_summary = []\n",
    "    topic_articles_summary = []\n",
    "    if articles:\n",
    "        articles_summary = [doc_catalog[\"summary\"].loc[i] for i, docid in enumerate(list(doc_catalog[\"docid\"])) if docid in articles]\n",
    "    if topic_articles:\n",
    "        topic_articles_summary = [doc_catalog[\"summary\"].loc[i] for i, filename in enumerate(list(doc_catalog[\"filename\"])) if filename[:-4] in topic_articles]\n",
    "    print(\"Your Query:\", query)\n",
    "    print(\"The answer:\", response)\n",
    "    if articles:\n",
    "        print(\"\\nYou can find the details of the answer from the following articles\")\n",
    "        for i in range(len(articles)):\n",
    "            print(\"\\nArticle \"+str(i+1)+\": \"+ doc_catalog[doc_catalog[\"docid\"]==articles[i]][\"filename\"].loc[0][:-4])\n",
    "            print(\"Article Summary:\\n\"+articles_summary[i])\n",
    "    if topic_articles:\n",
    "        print(\"\\nYou seem interested in the topics:\", \", \".join(topics),\"\\nYou may be interested in other articles in those topics below:\")\n",
    "        for i in range(len(topic_articles)):\n",
    "            print(\"\\nArticle \"+str(i+1)+\": \"+topic_articles[i])\n",
    "            print(\"Article Summary:\\n\"+topic_articles_summary[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Query: What is the pperformance of GPT-4 vs Mixtral?\n",
      "The answer: Based on the provided data and charts, here's a summary of the performance comparison between GPT-4 and Mixtral-8x7B:\n",
      "\n",
      "1. **Overall Accuracy**:\n",
      "   - GPT-4 shows significantly higher accuracy compared to Mixtral-8x7B in both retrieved and ground-truth chunks.\n",
      "   - For retrieved chunks, GPT-4 achieves an accuracy of 56%, while Mixtral-8x7B achieves 32%.\n",
      "   - For ground-truth chunks, GPT-4 achieves an accuracy of 89%, while Mixtral-8x7B achieves 36%.\n",
      "\n",
      "2. **Query Type Performance**:\n",
      "   - **Inference Queries**: GPT-4 performs much better than Mixtral-8x7B, especially in inference tasks, indicating stronger reasoning capabilities.\n",
      "   - **Comparison and Temporal Queries**: GPT-4 outperforms Mixtral-8x7B, particularly in handling logical negation and understanding temporal sequences.\n",
      "   - **Null Queries**: Both models perform well, indicating they can recognize when a query cannot be answered with the given information.\n",
      "\n",
      "3. **Strengths and Weaknesses**:\n",
      "   - GPT-4 demonstrates strong reasoning and inference capabilities, making it more suitable for complex multi-hop queries.\n",
      "   -\n",
      "\n",
      "You can find the details of the answer from the following articles\n",
      "\n",
      "Article 1: 2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries\n",
      "Article Summary:\n",
      "This paper introduces MultiHop-RAG, a novel dataset designed for benchmarking Retrieval-Augmented Generation (RAG) systems on multi-hop queries. The dataset contains a knowledge base of news articles, multi-hop queries requiring reasoning over multiple pieces of evidence, their ground-truth answers, and the associated supporting evidence. The authors demonstrate the dataset's utility by evaluating different embedding models and state-of-the-art LLMs, revealing the challenges in retrieving and answering multi-hop queries and highlighting the need for further research in developing effective RAG systems.\n",
      "\n",
      "You seem interested in the topics: MultiHop-RAG dataset, Retrieval-Augmented Generation (RAG), multi-hop queries, knowledge base, evidence retrieval, answer generation, benchmarking, LLM evaluation, RAG system challenges \n",
      "You may be interested in other articles in those topics below:\n",
      "\n",
      "Article 1: 2407.21059v1.Modular_RAG__Transforming_RAG_Systems_into_LEGO_like_Reconfigurable_Frameworks\n",
      "Article Summary:\n",
      "This paper introduces Modular RAG, a new framework for Retrieval-Augmented Generation (RAG) systems that tackles the increasing complexity of these systems. Modular RAG decomposes RAG systems into independent modules and operators, allowing for flexible configuration and customization. The paper identifies common RAG flow patterns and analyzes existing methods within the Modular RAG framework, demonstrating its compatibility with new developments in RAG technology.\n",
      "\n",
      "Article 2: 2408.02545v1.RAG_Foundry__A_Framework_for_Enhancing_LLMs_for_Retrieval_Augmented_Generation\n",
      "Article Summary:\n",
      "This paper introduces RAG Foundry, an open-source framework for enhancing large language models (LLMs) for retrieval-augmented generation (RAG) tasks.  The framework provides a unified workflow for data creation, training, inference, and evaluation, enabling researchers and practitioners to easily experiment with various RAG techniques and configurations. The authors demonstrate the effectiveness of RAG Foundry by augmenting and fine-tuning Llama-3 and Phi-3 models, achieving consistent improvements across three knowledge-intensive datasets.\n",
      "\n",
      "Article 3: 2410.20299v1.EACO_RAG__Edge_Assisted_and_Collaborative_RAG_with_Adaptive_Knowledge_Update\n",
      "Article Summary:\n",
      "This paper proposes EACO-RAG, an edge-assisted and collaborative retrieval-augmented generation (RAG) system that tackles the scalability challenges of traditional RAG systems. By distributing knowledge across edge nodes, EACO-RAG reduces communication overhead and delay, while adaptive knowledge updates based on user behavior enhance response accuracy. The system uses a multi-armed bandit framework to dynamically select between edge and cloud resources, minimizing costs while maintaining performance.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is RAG\"\n",
    "answer(retriever_mv_google,model_google,query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III. Research Assistant Use Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration of Research Assistant for: \n",
    "- Answer queries\n",
    "- Relevant papers: from the query and from the topic\n",
    "- Summary of the recommanded papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Query: Why Rag is popular?\n",
      "The answer: The provided text focuses on the technical aspects of RAG (Retrieval-Augmented Generation) systems and their evolution, not directly addressing the popularity of RAG itself. \n",
      "\n",
      "However, we can infer some reasons for RAG's popularity based on the text:\n",
      "\n",
      "* **Enhanced Capabilities of LLMs:** RAG significantly improves the performance of Large Language Models (LLMs) on knowledge-intensive tasks by providing them access to external knowledge bases, reducing hallucinations and improving accuracy. This is a crucial factor driving RAG's adoption.\n",
      "* **Flexibility and Adaptability:** The Modular RAG framework, as described in the text, allows for highly reconfigurable systems. This flexibility is attractive to developers as it allows them to adapt RAG to various applications and integrate different components based on specific needs. \n",
      "* **Addressing the Limitations of Naive RAG:** The text highlights the limitations of the initial \"Naive RAG\" paradigm. The evolution to \"Advanced RAG\" and now Modular RAG demonstrates the ongoing effort to overcome these limitations and improve the overall effectiveness of RAG systems.\n",
      "* **Growing Demand for Knowledge-Intensive Applications:** RAG is becoming increasingly popular due to the growing demand for knowledge-intensive applications in various fields, such as question answering, recommendation systems, customer service, and personal assistants.\n",
      "\n",
      "In summary, RAG's popularity is driven by its ability to significantly enhance the capabilities of LLMs, its flexibility and adaptability, the ongoing efforts to address its limitations, and the growing demand for knowledge-intensive applications. \n",
      "\n",
      "\n",
      "You can find the details of the answer from the following articles\n",
      "\n",
      "Article 1: 2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries\n",
      "Article Summary:\n",
      "This paper introduces MultiHop-RAG, a novel dataset designed for benchmarking Retrieval-Augmented Generation (RAG) systems on multi-hop queries. The dataset contains a knowledge base of news articles, multi-hop queries requiring reasoning over multiple pieces of evidence, their ground-truth answers, and the associated supporting evidence. The authors demonstrate the dataset's utility by evaluating different embedding models and state-of-the-art LLMs, revealing the challenges in retrieving and answering multi-hop queries and highlighting the need for further research in developing effective RAG systems.\n",
      "\n",
      "You seem interested in the topics: MultiHop-RAG dataset, Retrieval-Augmented Generation (RAG), multi-hop queries, knowledge base, evidence retrieval, answer generation, benchmarking, LLM evaluation, RAG system challenges \n",
      "You may be interested in other articles in those topics below:\n",
      "\n",
      "Article 1: 2407.21059v1.Modular_RAG__Transforming_RAG_Systems_into_LEGO_like_Reconfigurable_Frameworks\n",
      "Article Summary:\n",
      "This paper introduces Modular RAG, a new framework for Retrieval-Augmented Generation (RAG) systems that tackles the increasing complexity of these systems. Modular RAG decomposes RAG systems into independent modules and operators, allowing for flexible configuration and customization. The paper identifies common RAG flow patterns and analyzes existing methods within the Modular RAG framework, demonstrating its compatibility with new developments in RAG technology.\n",
      "\n",
      "Article 2: 2408.02545v1.RAG_Foundry__A_Framework_for_Enhancing_LLMs_for_Retrieval_Augmented_Generation\n",
      "Article Summary:\n",
      "This paper introduces RAG Foundry, an open-source framework for enhancing large language models (LLMs) for retrieval-augmented generation (RAG) tasks.  The framework provides a unified workflow for data creation, training, inference, and evaluation, enabling researchers and practitioners to easily experiment with various RAG techniques and configurations. The authors demonstrate the effectiveness of RAG Foundry by augmenting and fine-tuning Llama-3 and Phi-3 models, achieving consistent improvements across three knowledge-intensive datasets.\n",
      "\n",
      "Article 3: 2410.20299v1.EACO_RAG__Edge_Assisted_and_Collaborative_RAG_with_Adaptive_Knowledge_Update\n",
      "Article Summary:\n",
      "This paper proposes EACO-RAG, an edge-assisted and collaborative retrieval-augmented generation (RAG) system that tackles the scalability challenges of traditional RAG systems. By distributing knowledge across edge nodes, EACO-RAG reduces communication overhead and delay, while adaptive knowledge updates based on user behavior enhance response accuracy. The system uses a multi-armed bandit framework to dynamically select between edge and cloud resources, minimizing costs while maintaining performance.\n",
      "Your Query: what are available framework to enhance LLM?\n",
      "The answer: The provided text focuses on various techniques for enhancing LLMs within the context of Retrieval-Augmented Generation (RAG).  Here's a breakdown of the available frameworks and their potential for enhancing LLMs:\n",
      "\n",
      "**1. Retriever Selection:**\n",
      "\n",
      "* **Sparse Retrievers (TF-IDF, BM25):** Efficient for large datasets but may struggle with complex semantics.\n",
      "* **Dense Retrievers (BERT-based, ColBERT, BGE, GTE):**  Offer richer semantic representations but have higher computational costs.\n",
      "* **Hybrid Retrievers:** Combine the strengths of both sparse and dense methods for improved effectiveness.\n",
      "\n",
      "**2. Retriever Fine-Tuning:**\n",
      "\n",
      "* **Supervised Fine-Tuning (SFT):**  Uses contrastive learning with labeled domain data to improve retrieval efficiency.\n",
      "* **LM-Supervised Retriever (LSR):**  Leverages outputs generated by the LLM as supervisory signals for fine-tuning.\n",
      "* **Adapters:**  Add a trainable module to the retriever, allowing for fine-tuning without affecting the base model (useful for large LLMs).\n",
      "\n",
      "**3. Post-Retrieval Techniques:**\n",
      "\n",
      "* **Reranking:** Reorders retrieved chunks based on relevance and diversity using rule-based or model-based approaches.\n",
      "* **Compression:**  Reduces the size of the retrieved context to improve LLM focus and reduce noise.\n",
      "* **Selection:** Removes irrelevant chunks from the retrieved context.\n",
      "\n",
      "**4. Generation:**\n",
      "\n",
      "* **Generator Fine-Tuning:**\n",
      "    * **Instruct Tuning:**  Provides the LLM with additional knowledge through fine-tuning on domain-specific datasets.\n",
      "    * **Reinforcement Learning:** Aligns LLM outputs with human or retriever preferences.\n",
      "* **Verification:**\n",
      "    * **Knowledge-Base Verification:**  Validates LLM responses using external knowledge sources.\n",
      "    * **Model-Based Verification:**  Uses a smaller LLM to assess the accuracy of the generated answer.\n",
      "\n",
      "**5. Orchestration:**\n",
      "\n",
      "* **Routing:**  Dynamically selects the appropriate RAG pipeline based on the query's characteristics.\n",
      "* **Scheduling:**  Manages the RAG process based on various criteria, such as output quality, confidence levels, and whether further retrieval is necessary.\n",
      "* **Fusion:**  Combines information from multiple retrieval pipelines to create a more comprehensive response.\n",
      "\n",
      "**Frameworks for Enhancing LLMs:**\n",
      "\n",
      "* **RAG FOUNDRY:**  An open-source framework that integrates data creation, training, inference, and evaluation for RAG-based LLM development.  It facilitates rapid experimentation with different RAG techniques.\n",
      "* **ITER-RETGEN:**  An iterative framework that performs multiple rounds of retrieval and generation.\n",
      "* **ToC:**  A recursive retrieval framework that progressively deepens the analysis of complex queries.\n",
      "* **FLARE:**  An adaptive retrieval framework that uses prompt engineering to control the retrieval process.\n",
      "* **SELF-RAG:**  A framework that fine-tunes the LLM to generate specific tokens that trigger retrieval or generation actions.\n",
      "* **DR-RAG:**  Employs a two-stage retrieval strategy and classifier selection mechanism for improved accuracy in multi-hop question-answering.\n",
      "* **PlanRAG:**  Introduces a preliminary planning stage for handling complex decision-making problems.\n",
      "* **Multi-Head RAG:**  Uses multi-head attention layers to retrieve multifaceted documents for complex queries.\n",
      "\n",
      "These frameworks provide a variety of options for enhancing LLMs within the context of RAG.  The best framework choice depends on the specific task, resources, and desired level of customization. \n",
      "\n",
      "\n",
      "You can find the details of the answer from the following articles\n",
      "\n",
      "Article 1: 2401.15391v1.MultiHop_RAG__Benchmarking_Retrieval_Augmented_Generation_for_Multi_Hop_Queries\n",
      "Article Summary:\n",
      "This paper introduces MultiHop-RAG, a novel dataset designed for benchmarking Retrieval-Augmented Generation (RAG) systems on multi-hop queries. The dataset contains a knowledge base of news articles, multi-hop queries requiring reasoning over multiple pieces of evidence, their ground-truth answers, and the associated supporting evidence. The authors demonstrate the dataset's utility by evaluating different embedding models and state-of-the-art LLMs, revealing the challenges in retrieving and answering multi-hop queries and highlighting the need for further research in developing effective RAG systems.\n",
      "\n",
      "You seem interested in the topics: MultiHop-RAG dataset, Retrieval-Augmented Generation (RAG), multi-hop queries, knowledge base, evidence retrieval, answer generation, benchmarking, LLM evaluation, RAG system challenges \n",
      "You may be interested in other articles in those topics below:\n",
      "\n",
      "Article 1: 2407.21059v1.Modular_RAG__Transforming_RAG_Systems_into_LEGO_like_Reconfigurable_Frameworks\n",
      "Article Summary:\n",
      "This paper introduces Modular RAG, a new framework for Retrieval-Augmented Generation (RAG) systems that tackles the increasing complexity of these systems. Modular RAG decomposes RAG systems into independent modules and operators, allowing for flexible configuration and customization. The paper identifies common RAG flow patterns and analyzes existing methods within the Modular RAG framework, demonstrating its compatibility with new developments in RAG technology.\n",
      "\n",
      "Article 2: 2408.02545v1.RAG_Foundry__A_Framework_for_Enhancing_LLMs_for_Retrieval_Augmented_Generation\n",
      "Article Summary:\n",
      "This paper introduces RAG Foundry, an open-source framework for enhancing large language models (LLMs) for retrieval-augmented generation (RAG) tasks.  The framework provides a unified workflow for data creation, training, inference, and evaluation, enabling researchers and practitioners to easily experiment with various RAG techniques and configurations. The authors demonstrate the effectiveness of RAG Foundry by augmenting and fine-tuning Llama-3 and Phi-3 models, achieving consistent improvements across three knowledge-intensive datasets.\n",
      "\n",
      "Article 3: 2410.20299v1.EACO_RAG__Edge_Assisted_and_Collaborative_RAG_with_Adaptive_Knowledge_Update\n",
      "Article Summary:\n",
      "This paper proposes EACO-RAG, an edge-assisted and collaborative retrieval-augmented generation (RAG) system that tackles the scalability challenges of traditional RAG systems. By distributing knowledge across edge nodes, EACO-RAG reduces communication overhead and delay, while adaptive knowledge updates based on user behavior enhance response accuracy. The system uses a multi-armed bandit framework to dynamically select between edge and cloud resources, minimizing costs while maintaining performance.\n",
      "Thank for chatting with me\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    query = input(\"What can I help you? \\n\")\n",
    "    if query:\n",
    "        answer(retriever_mv_google,model_google,query)\n",
    "    else:\n",
    "        print(\"Thank for chatting with me\")\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
